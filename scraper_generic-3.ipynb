{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import datetime\n",
    "from textblob import TextBlob\n",
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "import re\n",
    "from requests import get\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [art_content,art_content_html,art_extract_datetime,art_lang,art_title,art_url,src_name,src_type,src_url,src_img,art_auth]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BigScraper:\n",
    "    cols = ['art_content', 'art_content_html', 'art_published_datetime', 'art_lang', 'art_title',\n",
    "            'art_url', 'src_name', 'src_type', 'src_url', 'art_img', 'art_auth', 'art_tag']\n",
    "\n",
    "    def __init__(self):\n",
    "        self.df = pd.DataFrame(columns=BigScraper.cols)\n",
    "\n",
    "    def add_row(self, row_scrap):\n",
    "        if type(row_scrap) == list:\n",
    "            self.df.loc[len(self.df)] = row_scrap\n",
    "        elif type(row_scrap) == dict:\n",
    "            self.df = self.df.append(row_scrap, ignore_index=True)\n",
    "\n",
    "    def get_base_url(url):\n",
    "        for val in re.finditer(\"(\\w)+://[^/]+/\", url):\n",
    "            return val.group(0)\n",
    "\n",
    "    #Jason    \n",
    "        \n",
    "    def scrap_changethework(url):\n",
    "        response = requests.get(url)\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        art_html = soup.find('div', {'style': 'text-align: justify;'})\n",
    "        art_content = art_html.get_text().strip().replace('\\xa0', '')\n",
    "        if soup.find('meta', {'property': 'article:modified_time'})['content'] == None:\n",
    "            if soup.find('meta', {'property': 'article:published_time'})['content'] == None:\n",
    "                art_extract_datetime = datetime.date.today()\n",
    "            else:\n",
    "                art_extract_datetime = soup.find(\n",
    "                    'meta', {'property': 'article:published_time'})['content']\n",
    "                art_extract_datetime = datetime.datetime.strptime(\n",
    "                    art_extract_datetime, '%Y-%m-%dT%H:%M:%S%z').date()\n",
    "        else:\n",
    "            art_extract_datetime = soup.find(\n",
    "                'meta', {'property': 'article:modified_time'})['content']\n",
    "            art_extract_datetime = datetime.datetime.strptime(\n",
    "                art_extract_datetime, '%Y-%m-%dT%H:%M:%S%z').date()\n",
    "        art_lang = TextBlob(art_content).detect_language()\n",
    "        art_title = soup.find('meta', {'property': 'og:title'})['content']\n",
    "        art_url = soup.find('meta', {'property': 'og:url'})['content']\n",
    "        src_name = soup.find('meta', {'property': 'og:site_name'})['content']\n",
    "        src_type = 'xpath_source'\n",
    "        src_url = BigScraper.get_base_url(art_url)\n",
    "        src_img = soup.find('meta', {'property': 'og:image'})['content']\n",
    "        art_auth = [el.get_text().strip() for el in soup.find_all(\n",
    "            'span', class_='elementor-post-author')]\n",
    "        art_tag = np.nan\n",
    "        return [art_content, art_html, art_extract_datetime, art_lang, art_title, art_url, src_name, src_type, src_url, src_img, art_auth, art_tag]\n",
    "\n",
    "    #Marianne\n",
    "    \n",
    "    def scrap_fncrr(url):\n",
    "        '''Documentation\n",
    "        Parameters:\n",
    "            url: url of the scraped page\n",
    "        Out:\n",
    "            row: dict of values\n",
    "        '''\n",
    "        response = requests.get(url)\n",
    "        html_soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        # content, content_html\n",
    "        try:\n",
    "            content = html_soup.find(\"div\", {'class': \"contenu_c\"})\n",
    "            content_html = content\n",
    "            content = content.text\n",
    "        except:\n",
    "            content_html = np.nan\n",
    "            content = np.nan\n",
    "        # date\n",
    "        if html_soup.find(\"time\", {'class': \"updated\"}) != None:\n",
    "            date = html_soup.find(\"time\", {'class': \"updated\"})\n",
    "        else:\n",
    "            date = html_soup.find(\"time\", {'class': 'entry-date published'})\n",
    "        try:\n",
    "            date = date['datetime']\n",
    "            date = datetime.datetime.strptime(\n",
    "                date, '%Y-%m-%dT%H:%M:%S%z').date()\n",
    "        except:\n",
    "            # if no date is specified, put scraping date\n",
    "            date = datetime.date.today()\n",
    "        # tag, title\n",
    "        presentation = html_soup.find(\"div\", {'class': \"prensentation\"})\n",
    "        tag = np.nan  # tags are not always interesting\n",
    "        title = presentation.find(\"h1\")\n",
    "        title = title.text\n",
    "        # Remplissage du dataframe\n",
    "        row = {'art_content': content,\n",
    "               'art_content_html': content_html,\n",
    "               'art_published_datetime': date,\n",
    "               'art_lang': 'fr',\n",
    "               'art_title': title,\n",
    "               'art_url': url,\n",
    "               'src_name': 'fnccr',\n",
    "               'src_type': 'xpath_source',\n",
    "               'src_url': BigScraper.get_base_url(url),\n",
    "               'art_img': np.nan,  # No images\n",
    "               'art_auth': np.nan,  # No author specified\n",
    "               'art_tag': tag}\n",
    "        return row\n",
    "    \n",
    "    def scrap_cnil(url):\n",
    "        \n",
    "        '''Documentation\n",
    "\n",
    "        Parameters:\n",
    "            url: url of the scraped page\n",
    "\n",
    "        Out:\n",
    "            new_row: data to put in dataframe\n",
    "        '''\n",
    "        req = get(url)\n",
    "        html_soup = BeautifulSoup(req.text, 'html.parser')\n",
    "        #content, content_html\n",
    "        try:\n",
    "            content_html = html_soup.find(\"div\",{'class':\"field-item even\"})\n",
    "            content = content_html.text\n",
    "        except:\n",
    "            #maybe find a way to take into account multiple article structures instead\n",
    "            content_html = np.nan\n",
    "            content = np.nan\n",
    "        #date\n",
    "        date = html_soup.find(\"div\",{'class':\"ctn-gen-auteur\"}).text\n",
    "        \n",
    "        if date is None:\n",
    "            date = datetime.date.today()\n",
    "        else:\n",
    "            print(date)\n",
    "            trans_month = {'01':['janvier'], \n",
    "            '02':['février'],\n",
    "            '03':['mars'],\n",
    "            '04':['avril'],\n",
    "            '05':['mai'],\n",
    "            '06':['juin'],\n",
    "            '07':['juillet'],\n",
    "            '08':['août'],\n",
    "            '09':['septembre'],\n",
    "            '10':['octobre'],\n",
    "            '11':['novembre'],\n",
    "            '12':['décembre']}\n",
    "            \n",
    "            date_tab = date.split(\" \")\n",
    "            day = date_tab[0]\n",
    "            month = date_tab[1]\n",
    "            for m in trans_month:\n",
    "                if month.lower() in trans_month[m]:\n",
    "                    month = m\n",
    "            year = date_tab[2]\n",
    "            date = datetime.date(int(year), int(month), int(day))\n",
    "           \n",
    "    \n",
    "    \n",
    "        #title\n",
    "        zone_title = html_soup.find(\"div\",{'class':\"ctn-gen-titre\"})\n",
    "        title = zone_title.find(\"h1\")\n",
    "        title = title.text\n",
    "        #img\n",
    "        try:\n",
    "            zone_img = html_soup.find(\"div\",{'class':\"ctn-gen-visuel\"})\n",
    "            img = zone_img.find(\"img\")['src']\n",
    "        except:\n",
    "            img = \"no_data\"\n",
    "        #tag\n",
    "        zone_tag = html_soup.find(\"div\",{'class':\"mots cles\"})\n",
    "        try:\n",
    "            tags_li_list = zone_tag.find_all(\"li\")\n",
    "            tags_list = []\n",
    "            for tag in tags_li_list:\n",
    "                tags_list.append(tag.text[1:]) #[1:] to remove \"#\"\n",
    "        except:\n",
    "            tags_list = \"no_data\"\n",
    "        # add data to dataframe \n",
    "        new_row = {'art_content': content ,\n",
    "                   'art_content_html': content_html ,\n",
    "                   'art_published_datetime': date ,\n",
    "                   'art_lang': 'fr' , \n",
    "                   'art_title' : title , \n",
    "                   'art_url' : url ,\n",
    "                   'src_name' :'cnil'  ,\n",
    "                   'src_type' : 'xpath_source' ,\n",
    "                   'src_url' : 'https://www.cnil.fr/',\n",
    "                   'src_img' : img ,\n",
    "                   'art_auth': \"no_data\", # No author specified\n",
    "                   'art_tag': tags_list}\n",
    "        print(date)\n",
    "        return new_row\n",
    "    \n",
    "    def scrap_jdn(url):\n",
    "        req = get(url)\n",
    "        html_soup = BeautifulSoup(req.text, 'html.parser')\n",
    "        # content_html, content (maybe clean a little the content)\n",
    "        try:\n",
    "            content_html = html_soup.find(\"div\",{'id':\"jArticleInside\"})\n",
    "            content = content_html.text\n",
    "        except:\n",
    "            content_html = np.nan\n",
    "            content = np.nan\n",
    "        #date\n",
    "        try:\n",
    "            date = html_soup.find(\"time\",{'itemprop':\"publishDate\"})['datetime']\n",
    "            format_end = date[-5:]\n",
    "            date = datetime.datetime.strptime(date,'%Y-%m-%dT%H:%M:%S+'+format_end)\n",
    "            date = datetime.date.strftime('%Y-%m-%d')\n",
    "            #possibly change where the date is extracted\n",
    "            #see <script type=\"application/ld+json\"> \n",
    "        except:\n",
    "            date = datetime.datetime.today().strftime('%Y-%m-%d')\n",
    "        #title\n",
    "        try:\n",
    "            zone_title = html_soup.find(\"div\",{'id':\"jStickySize\"})\n",
    "            title = zone_title.find(\"h1\")\n",
    "            title = title.text\n",
    "        except:\n",
    "            title = \"no_data\"\n",
    "        #img\n",
    "        try:\n",
    "            zone_img = content_html.find(\"p\",{'class':\"app_entry_lead\"})\n",
    "            img = zone_img.find(\"img\")['src']\n",
    "        except:\n",
    "            img = \"no_data\"\n",
    "        #author\n",
    "        try:\n",
    "            link_author = html_soup.find(\"a\",{'rel':\"author\"})\n",
    "            author = link_author.text\n",
    "        except:\n",
    "            author = \"no_data\"\n",
    "        #tags\n",
    "        head = html_soup.find(\"head\")\n",
    "        scripts_list = head.find_all(\"script\")\n",
    "        script = str(scripts_list[1])\n",
    "        pattern = re.compile(\"keywords: \\[(\\\"(\\w|\\-|\\d)*\\\",?)*\\]\")\n",
    "        match =  re.search(pattern, script)\n",
    "        list_tag_str = match.group(0)\n",
    "        list_tag_str = list_tag_str[11:-1]\n",
    "        list_tag_str = list_tag_str.replace(\"-\",\" \")\n",
    "        list_tag = list_tag_str.split(\",\")\n",
    "        #data \n",
    "        new_row = {'art_content': content ,\n",
    "                   'art_content_html': content_html ,\n",
    "                   'art_published_datetime': date ,\n",
    "                   'art_lang': 'fr' , \n",
    "                   'art_title' : title , \n",
    "                   'art_url' : url ,\n",
    "                   'src_name' :'journal du net'  ,\n",
    "                   'src_type' : 'xpath_source' ,\n",
    "                   'src_url' : 'https://www.journaldunet.com/',\n",
    "                   'art_img' : img ,\n",
    "                   'art_auth': author, # No author specified\n",
    "                   'art_tag': list_tag}\n",
    "        return new_row\n",
    "    \n",
    "    \n",
    "\n",
    "    def scrap_zdnet(url):\n",
    "        '''Documentation\n",
    "\n",
    "        Parameters:\n",
    "            url: url of the scraped page\n",
    "\n",
    "        Out:\n",
    "            new_row: data to put in dataframe\n",
    "        '''\n",
    "        req = get(url)\n",
    "        html_soup = BeautifulSoup(req.text, 'html.parser')\n",
    "        # content, html_content\n",
    "        content_html = html_soup.find(\"div\",{'class':\"storyBody\"})\n",
    "        content = content_html.text\n",
    "        # date, author\n",
    "        zone_infos = html_soup.find(\"div\",{'class':\"byline\"})\n",
    "        zone_infos = zone_infos.find(\"p\",{'class':\"meta\"})\n",
    "        ## author\n",
    "        try:\n",
    "            zone_author = zone_infos.find(\"span\")\n",
    "            author = zone_author.find(\"span\").text\n",
    "        except:\n",
    "            author = \"no_data\"\n",
    "        ## date\n",
    "        date = zone_infos.find(\"time\")['datetime']\n",
    "        format_end = date[-5:]\n",
    "        date = datetime.datetime.strptime(date,'%Y-%m-%dT%H:%M:%S+'+format_end)\n",
    "        date = date.strftime('%Y-%m-%d')\n",
    "        # title\n",
    "        title = html_soup.find(\"h1\").text\n",
    "        # img\n",
    "        try:\n",
    "            img = content_html.find(\"img\")['src']\n",
    "        except:\n",
    "            img = \"no_data\"\n",
    "        #tags\n",
    "        zone_tags = html_soup.find(\"p\",{'class':\"relatedTopics\"})\n",
    "        list_tags_links = zone_tags.find_all(\"a\")\n",
    "        list_tags = []\n",
    "        for link in list_tags_links:\n",
    "            list_tags.append(link.text)\n",
    "        # data to add in dataframe \n",
    "        new_row = {'art_content': content_html ,\n",
    "                   'art_content_html': content ,\n",
    "                   'art_published_datetime': date ,\n",
    "                   'art_lang': 'fr' , \n",
    "                   'art_title' : title , \n",
    "                   'art_url' : url ,\n",
    "                   'src_name' :'zdnet',\n",
    "                   'src_type' : 'xpath_source',\n",
    "                   'src_url' : 'https://www.zdnet.fr/',\n",
    "                   'src_img' : img ,\n",
    "                   'art_auth': author ,\n",
    "                   'art_tag': list_tags}\n",
    "        return new_row\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    #Louis\n",
    "    \n",
    "    def scrap_sabbar(url):\n",
    "        response = requests.get(url)\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        # Récupération du contenu de la page web (les paragraphes, avec et sans les balises html)\n",
    "        art_content_html = soup.find('div', class_=\"entry-content\")\n",
    "        art_content = art_content_html.get_text().replace(\"\\xa0\", \"\").strip()\n",
    "        # Extraction de la date de l'article\n",
    "        art_extract_datetime = json.loads(soup.find(\n",
    "            'script', class_='yoast-schema-graph yoast-schema-graph--main').get_text())['@graph'][1]['dateModified']\n",
    "        art_extract_datetime = datetime.datetime.strptime(\n",
    "            art_extract_datetime, '%Y-%m-%dT%H:%M:%S%z').date()\n",
    "        # Langue de l'article\n",
    "        art_lang = soup.find('meta', property=\"og:locale\").get('content')\n",
    "        # Titre\n",
    "        art_title = soup.find('meta', property=\"og:title\").get('content')\n",
    "        # Url\n",
    "        art_url = soup.find('link', rel='canonical').get('href')\n",
    "        # Nom de la source\n",
    "        src_name = \"Sabbar\"\n",
    "        # Type de la source\n",
    "        src_type = \"xpath_source\"\n",
    "        # url source\n",
    "        src_url = BigScraper.get_base_url(art_url)\n",
    "        # Image(s)\n",
    "        src_img = np.nan\n",
    "        # Auteur de l'article\n",
    "        art_auth = np.nan\n",
    "        # Tag de l'auteur\n",
    "        art_tag = np.nan\n",
    "\n",
    "        return [art_content, art_content_html, art_extract_datetime, art_lang, art_title, art_url, \\\n",
    "                src_name, src_type, src_url, src_img, art_auth, art_tag]\n",
    "    \n",
    "    def scrap_lebigdata(url):\n",
    "        response = requests.get(url)\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        art_content_html = soup.find(\"article\")\n",
    "        art_content = art_content_html.get_text().replace('\\xa0', '')\n",
    "        if soup.find('meta', property='article:modified_time') is not None:\n",
    "            art_published_datetime = soup.find(\n",
    "                'meta', property='article:modified_time').get('content')\n",
    "            art_published_datetime = datetime.datetime.strptime(\n",
    "                art_published_datetime, '%Y-%m-%dT%H:%M:%S%z').date()\n",
    "        elif soup.find('meta', property='article:published_time') is not None:\n",
    "            art_published_datetime = soup.find(\n",
    "                'meta', property='article:published_time').get('content')\n",
    "            art_published_datetime = datetime.datetime.strptime(\n",
    "                art_published_datetime, '%Y-%m-%dT%H:%M:%S%z').date()\n",
    "        else:\n",
    "            art_published_datetime = datetime.date.today()\n",
    "        art_lang = soup.find('meta', property='og:locale').get('content')\n",
    "        art_title = soup.find('meta', property=\"og:title\").get('content')\n",
    "        art_url = soup.find('meta', property=\"og:url\").get('content')\n",
    "        src_name = soup.find('meta', property=\"og:site_name\").get('content')\n",
    "        src_type = \"xpath_source\"\n",
    "        src_url = BigScraper.get_base_url(art_url)\n",
    "        art_img = soup.find('meta', property=\"og:image\").get('content')\n",
    "        art_auth = soup.find(\n",
    "            'meta', attrs={'name': \"twitter:data1\"}).get('content')\n",
    "        art_tag = np.nan\n",
    "        return [art_content, art_content_html, art_published_datetime, art_lang, art_title, art_url, \\\n",
    "                src_name, src_type, src_url, art_img, art_auth, art_tag]\n",
    "    \n",
    "    def scrap_cadre(url):\n",
    "        \n",
    "        response = requests.get(url)\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        art_content_html = soup.find('div', class_ = 'td-post-content').find_all('p')\n",
    "        art_content = \"\".join([x.text for x in art_content_html])\n",
    "\n",
    "        #Extraction de la date de l'article\n",
    "        art_extract_datetime = soup.find('meta', property = \"article:modified_time\").get('content')\n",
    "\n",
    "        #Langue de l'article\n",
    "        art_lang = soup.find('meta', property = \"og:locale\").get('content')\n",
    "\n",
    "        #Titre\n",
    "        art_title = soup.find('meta', property = \"og:title\").get('content')\n",
    "\n",
    "        #Url\n",
    "        art_url = soup.find('link', rel = 'canonical').get('href')\n",
    "\n",
    "        #Nom de la source\n",
    "        src_name = soup.find('meta', property = \"og:site_name\").get('content')\n",
    "\n",
    "        #Type de la source\n",
    "        src_type = \"xpath_source\"\n",
    "\n",
    "        #url source\n",
    "        src_url = soup.find('form', class_ = 'td-search-form').get('action')\n",
    "\n",
    "        #Image(s)\n",
    "        src_img = soup.find('meta', property = \"og:image\").get('content')\n",
    "\n",
    "        #Auteur de l'article\n",
    "        art_auth = soup.find('div', class_ = \"td-post-author-name\").text\n",
    "\n",
    "        #Tag de l'auteur\n",
    "        art_tag = np.nan\n",
    "\n",
    "        return [art_content, art_content_html, art_extract_datetime, art_lang, art_title,\\\n",
    "            art_url, src_name, src_type, src_url, src_img, art_auth, art_tag]\n",
    "    \n",
    "    \n",
    "    def scrap_sap(url):\n",
    "        \n",
    "        response = requests.get(url)\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "        art_content_html = soup.find_all('div', class_=\"parContent\")\n",
    "        art_content = \" \".join([x.text for x in art_content_html])\n",
    "\n",
    "        date = soup.find('video', class_='video-js vjs-default-skin vjs-big-play-centered vjs-fluid').get('data-publishingdate')\n",
    "        art_published_datetime = datetime.datetime.strptime(date, '%Y-%m-%dT%H:%M:%S%z').date()\n",
    "\n",
    "        art_lang = soup.find('meta', attrs={'name':\"language\"}).get('content')\n",
    "\n",
    "        art_title = soup.find('meta', property = \"og:title\").get('content')\n",
    "\n",
    "        art_url = soup.find('meta', property = \"og:url\").get('content')\n",
    "\n",
    "        src_name = soup.find('meta', property = \"og:site_name\").get('content')\n",
    "\n",
    "        src_type = \"xpath_source\"\n",
    "\n",
    "        src_url = soup.find('meta', property = \"og:site_name\").get('content') + \".com\"\n",
    "\n",
    "        src_img = soup.find('meta', property = \"og:image\").get('content') \n",
    "\n",
    "        art_auth = \"no_data\"\n",
    "\n",
    "        art_tag = soup.find('meta', attrs={'name':\"keywords\"}).get('content')\n",
    "\n",
    "        return [art_content, art_content_html, art_published_datetime, art_lang, art_title, art_url, \\\n",
    "                src_name, src_type, src_url, src_img, art_auth, art_tag]\n",
    "    \n",
    "    def scrap_datagouv(url):\n",
    "    \n",
    "    \n",
    "        response = requests.get(url)\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        if soup.find_all('section', class_=\"content noncertified\") == [] :\n",
    "            art_content_html = soup.find_all('section', class_=\"content certified\")\n",
    "        else :\n",
    "            art_content_html = soup.find_all('section', class_=\"content noncertified\")\n",
    "\n",
    "        art_content = \" \".join([x.text for x in art_content_html])\n",
    "\n",
    "        art_extract_datetime = datetime.date.today()\n",
    "\n",
    "        art_lang = soup.find('html').get('lang')\n",
    "\n",
    "        art_title = soup.find('meta', property = \"og:title\").get('content')\n",
    "\n",
    "        art_url = soup.find('link', rel = 'canonical').get('href')\n",
    "\n",
    "        src_type = \"xpath_source\"\n",
    "\n",
    "        for val in re.finditer(\"(\\w)+://[^/]+/\", url):\n",
    "            src_url = val.group(0)\n",
    "\n",
    "        src_name = src_url.replace('https://','').replace('/','')\n",
    "\n",
    "        src_img = soup.find('meta', property = \"og:image\").get('content')\n",
    "\n",
    "        art_auth = soup.find('link', rel='author').get('href')\n",
    "\n",
    "        art_tag = []\n",
    "        tags = soup.find_all('a', class_ = \"label label-default\")\n",
    "        for x in tags:\n",
    "            art_tag.append(x.get('title'))\n",
    "\n",
    "        return [art_content, art_content_html, art_extract_datetime, art_lang, art_title, art_url, \\\n",
    "                src_name, src_type, src_url, src_img, art_auth, art_tag] \n",
    "    \n",
    "    def scrap_blockchain(url):\n",
    "    \n",
    "    \n",
    "        response = requests.get(url)\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        art_content_html = soup.find_all('div', class_=\"site-content\")\n",
    "        art_content = \" \".join([x.text for x in art_content_html])\n",
    "\n",
    "        date = soup.find('meta', property='article:modified_time').get('content')\n",
    "        art_extract_datetime = datetime.datetime.strptime(date, '%Y-%m-%dT%H:%M:%S%z').date()    \n",
    "\n",
    "        art_lang = soup.find('meta', attrs={'property':\"og:locale\"}).get('content')\n",
    "\n",
    "        art_title = soup.find('meta', property = \"og:title\").get('content')\n",
    "\n",
    "        art_url = soup.find('meta', property = \"og:url\").get('content')\n",
    "\n",
    "        src_type = \"xpath_source\"\n",
    "\n",
    "        for val in re.finditer(\"(\\w)+://[^/]+/\", url):\n",
    "            src_url = val.group(0)\n",
    "\n",
    "        src_name = src_url.replace('https://','').replace('/','')\n",
    "\n",
    "        src_img = soup.find('meta', property = \"og:image\").get('content')\n",
    "\n",
    "        art_auth = \"no_data\"\n",
    "\n",
    "        if soup.find('meta', attrs={'name':\"keywords\"}) :\n",
    "            art_tag =soup.find('meta', attrs={'name':\"keywords\"})   \n",
    "        else :\n",
    "            art_tag = \"no_data\"\n",
    "\n",
    "        return [art_content, art_content_html, art_extract_datetime, art_lang, art_title, art_url, \\\n",
    "                src_name, src_type, src_url, src_img, art_auth, art_tag]\n",
    "    \n",
    "\n",
    "\n",
    "    #Michael\n",
    "\n",
    "    def scrap_theinnovation(url):\n",
    "        response = requests.get(url)\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        art_content_html = soup.find(\"div\", {\"class\": \"entry-content\"})\n",
    "        art_content = art_content_html.text.replace('\\xa0', '')\n",
    "        if soup.find(\"meta\", {\"property\": \"article:modified_time\"}) != None:\n",
    "            art_extract_datetime = soup.find(\n",
    "                \"meta\", {\"property\": \"article:modified_time\"})[\"content\"]\n",
    "            art_extract_datetime = datetime.datetime.strptime(\n",
    "                art_extract_datetime, '%Y-%m-%dT%H:%M:%S%z').date()\n",
    "        elif soup.find(\"meta\", {\"property\": \"article:published_time\"}) != None:\n",
    "            art_extract_datetime = soup.find(\n",
    "                \"meta\", {\"property\": \"article:published_time\"})[\"content\"]\n",
    "            art_extract_datetime = datetime.datetime.strptime(\n",
    "                art_extract_datetime, '%Y-%m-%dT%H:%M:%S%z').date()\n",
    "        else:\n",
    "            art_extract_datetime = datetime.date.today()\n",
    "        art_lang = soup.find(\"meta\", {\"property\": \"og:locale\"})[\"content\"]\n",
    "        art_title = soup.find(\"meta\", {\"property\": \"og:title\"})[\"content\"]\n",
    "        art_url = soup.find(\"meta\", {\"property\": \"og:url\"})[\"content\"]\n",
    "        src_name = soup.find(\"meta\", {\"property\": \"og:site_name\"})[\"content\"]\n",
    "        src_type = \"xpath_source\"  # default value\n",
    "        src_url = BigScraper.get_base_url(art_url)\n",
    "        src_img = soup.find(\"meta\", {\"property\": \"og:image\"})[\"content\"]\n",
    "        art_auth = soup.find(\"a\", {\"rel\": \"author\"}).text\n",
    "        art_tag = soup.find(\"meta\", {\"name\": \"keywords\"})[\"content\"].split(',')\n",
    "        return [art_content, art_content_html, art_extract_datetime, art_lang, art_title, art_url,\n",
    "                src_name, src_type, src_url, src_img, art_auth, art_tag]\n",
    "    \n",
    "    def scrap_myrhline(url):\n",
    "        response = requests.get(url)\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        art_content_html = soup.find(\"div\", {\"class\": \"post-detail-wrap\"})\n",
    "        if art_content_html is None:\n",
    "            art_content_html = \"no_data\"\n",
    "\n",
    "        art_content = art_content_html.get_text().replace('\\xa0', '').strip()\n",
    "        if art_content is None:\n",
    "            art_content = \"no_data\"\n",
    "\n",
    "        if soup.find(\"meta\", {\"property\": \"article:modified_time\"}) is not None:\n",
    "            art_published_datetime = soup.find(\"meta\", {\"property\": \"article:modified_time\"})[\"content\"]\n",
    "            art_published_datetime = datetime.datetime.strptime(art_published_datetime, '%Y-%m-%dT%H:%M:%S%z').date()\n",
    "\n",
    "        elif soup.find(\"meta\", {\"property\": \"article:published_time\"}) is not None:\n",
    "            art_published_datetime = soup.find(\"meta\", {\"property\": \"article:published_time\"})[\"content\"]\n",
    "            art_published_datetime = datetime.datetime.strptime(art_published_datetime, '%Y-%m-%dT%H:%M:%S%z').date()\n",
    "\n",
    "        else:\n",
    "            art_published_datetime = datetime.date.today()\n",
    "\n",
    "        if art_content is not None:\n",
    "            art_lang = TextBlob(art_content).detect_language()\n",
    "        elif soup.find(\"meta\", {\"property\": \"og:locale\"}) is not None:\n",
    "            art_lang = soup.find(\"meta\", {\"property\": \"og:locale\"})[\"content\"]   \n",
    "        else:\n",
    "            art_lang = \"no_data\"\n",
    "\n",
    "        if soup.find(\"meta\", {\"property\": \"og:title\"}) is not None:\n",
    "            art_title = soup.find(\"meta\", {\"property\": \"og:title\"})[\"content\"]\n",
    "        elif soup.find(\"title\") is not None:\n",
    "            art_title = soup.find(\"title\").text\n",
    "        else:\n",
    "            art_title = \"no_data\"\n",
    "\n",
    "        if soup.find(\"meta\", {\"property\": \"og:url\"}) is not None:\n",
    "            art_url = soup.find(\"meta\", {\"property\": \"og:url\"})[\"content\"]\n",
    "        else:\n",
    "            art_url = url\n",
    "\n",
    "        if soup.find(\"meta\", {\"property\": \"og:site_name\"}) is not None:\n",
    "            src_name = soup.find(\"meta\", {\"property\": \"og:site_name\"})[\"content\"]\n",
    "        else:\n",
    "            src_name = \"no_data\"\n",
    "\n",
    "        src_type = \"xpath_source\" #default value  \n",
    "\n",
    "        src_url = BigScraper.get_base_url(art_url)\n",
    "\n",
    "        if soup.find(\"meta\", {\"property\": \"og:image\"}) is not None:\n",
    "            art_img = soup.find(\"meta\", {\"property\": \"og:image\"})[\"content\"]\n",
    "        else:\n",
    "            art_img = \"no_data\"\n",
    "\n",
    "        if soup.find(\"meta\", {\"name\": \"author\"}) is not None:   \n",
    "            art_auth = soup.find(\"meta\", {\"name\": \"author\"})[\"content\"]\n",
    "        else:\n",
    "            art_auth = \"no_data\"\n",
    "\n",
    "        if soup.find_all(\"a\", {\"rel\": \"tag\"}) is not None:\n",
    "            art_tag = [tag.text for tag in soup.find_all(\"a\", {\"rel\": \"tag\"})]\n",
    "        else:\n",
    "            art_tag = \"no_data\"\n",
    "\n",
    "\n",
    "        return [art_content, art_content_html, art_published_datetime, art_lang, art_title, art_url,\\\n",
    "            src_name, src_type, src_url, art_img, art_auth, art_tag]\n",
    "    \n",
    "    def scrap_usinedigitale(url):\n",
    "        response = requests.get(url)\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        #retrieval of the html content\n",
    "        art_content_html = soup.find(\"article\", {\"class\":\"contenuArticle\"})\n",
    "        if art_content_html is None:\n",
    "            art_content_html = \"no_data\"\n",
    "\n",
    "        #retrieval of the article content    \n",
    "        art_content = art_content_html.text\n",
    "        if art_content is None:\n",
    "            art_content = \"no_data\"   \n",
    "\n",
    "        #retrieval of the publication/modification date    \n",
    "        if soup.find(\"time\", {\"class\": \"dateEtiquette3\"}) is None:\n",
    "            art_published_datetime = datetime.datetime.now()\n",
    "        else:    \n",
    "            art_published_datetime = soup.find(\"time\", {\"class\": \"dateEtiquette3\"})[\"datetime\"]\n",
    "            art_published_datetime = datetime.datetime.strptime(art_published_datetime, '%Y-%m-%dT%H:%M').date()\n",
    "\n",
    "        #retrieval of the language\n",
    "        if art_content is not None:\n",
    "            art_lang = TextBlob(art_content).detect_language()    \n",
    "        elif soup.find(\"meta\", {\"property\": \"og:locale\"}) is not None:\n",
    "            art_lang = soup.find(\"meta\", {\"property\": \"og:locale\"})[\"content\"]\n",
    "        else:\n",
    "            art_lang = \"no_data\"\n",
    "\n",
    "        #retrieval of the title\n",
    "        if soup.find(\"meta\", {\"property\": \"og:title\"}) is not None:\n",
    "            art_title = soup.find(\"meta\", {\"property\": \"og:title\"})[\"content\"]\n",
    "        elif soup.find(\"title\") is not None:\n",
    "            art_title = soup.find(\"title\").text\n",
    "        else:\n",
    "            art_title = \"no_data\"\n",
    "\n",
    "        #retrieval of the article url   \n",
    "        art_url = soup.find(\"meta\", {\"property\": \"og:url\"})\n",
    "\n",
    "        if art_url is None:\n",
    "            art_url = url\n",
    "        else:    \n",
    "            art_url = art_url[\"content\"]\n",
    "\n",
    "        #retrieval of the website name\n",
    "        if soup.find(\"meta\", {\"name\": \"ipd:siteName\"}) is not None:\n",
    "            src_name = soup.find(\"meta\", {\"name\": \"ipd:siteName\"})[\"content\"]\n",
    "        elif soup.find(\"meta\", {\"property\": \"og:site_name\"}) is not None:\n",
    "            src_name = soup.find(\"meta\", {\"property\": \"og:site_name\"})[\"content\"]\n",
    "        else:\n",
    "            src_name = \"no_data\"\n",
    "\n",
    "        #retrieval of the source type\n",
    "        src_type = \"xpath_source\" #default value\n",
    "\n",
    "        #retrieval of the source url\n",
    "        src_url = BigScraper.get_base_url(art_url)\n",
    "        if src_url is None:\n",
    "            src_url = \"no_data\"\n",
    "\n",
    "        #retrieval of the article image\n",
    "        art_img = soup.find(\"meta\", {\"property\": \"og:image\"})\n",
    "        if art_img is None:\n",
    "            art_img = \"no_data\" \n",
    "        else:\n",
    "            art_img = art_img[\"content\"]\n",
    "\n",
    "        #retrieval of the article author\n",
    "        art_auth = soup.find(\"a\", {\"class\": \"nomAuteur\"})\n",
    "        if art_auth is None:   \n",
    "            art_auth = \"no_data\"\n",
    "        else:\n",
    "            art_auth = art_auth.text\n",
    "\n",
    "        #retrieval of the article tags\n",
    "        if soup.find_all(\"a\", {\"rel\": \"tag\"}) is not None:\n",
    "            art_tag = [tag.text for tag in soup.find_all(\"a\", {\"rel\": \"tag\"}) ]\n",
    "        else:\n",
    "            art_tag = \"no_data\"\n",
    "\n",
    "\n",
    "        return [art_content, art_content_html, art_published_datetime, art_lang, art_title,\\\n",
    "            art_url, src_name, src_type, src_url, art_img, art_auth, art_tag]\n",
    "\n",
    "    #Rémy\n",
    "    \n",
    "    def scrap_lemondeinformatique(url):\n",
    "        response = requests.get(url)\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        art_content_html = soup.find('div', class_='article-body')\n",
    "        art_content = art_content_html.get_text().replace('\\xa0', '').strip()\n",
    "        if soup.find(\"meta\", {\"itemprop\": \"datePublished\"}) is not None:\n",
    "            art_extract_datetime = soup.find(\n",
    "                \"meta\", {\"itemprop\": \"datePublished\"})['content']\n",
    "            art_extract_datetime = datetime.datetime.strptime(\n",
    "                art_extract_datetime, '%Y-%m-%dT%H:%M:%S%z').date()\n",
    "        else:\n",
    "            art_extract_datetime = datetime.date.today()\n",
    "        art_langue = TextBlob(get_Title(art_content)).detect_language()\n",
    "        art_title = soup.find(\"meta\", {\"property\": \"og:title\"})[\"content\"]\n",
    "        art_url = soup.find(\"meta\", {\"property\": \"og:url\"})[\"content\"]\n",
    "        src_name = soup.find(\"meta\", {\"property\": \"og:site_name\"})[\"content\"]\n",
    "        src_type = 'xpath_source'\n",
    "        src_url = BigScraper.get_base_url(art_url)\n",
    "        src_img = soup.find(\"meta\", {\"property\": \"og:image\"})[\"content\"]\n",
    "        art_auth = soup.find(\n",
    "            \"div\", class_=\"author-infos\").find(\"b\", {\"itemprop\": \"name\"}).get_text()\n",
    "        art_tag = [el.get_text()\n",
    "                   for el in soup.find_all(\"a\", {\"rel\": \"category tag\"})]\n",
    "        return [art_content, art_content_html, art_extract_datetime, art_lang, art_title, art_url, src_name, src_type, src_url, src_img, art_auth, art_tag]\n",
    "\n",
    "    def scrap_erudit(url : str) -> tuple :\n",
    "        \"\"\"\n",
    "        function which from a url returns all the data collected with the functions above in a tuple\n",
    "        \"\"\"\n",
    "        req = get(url)\n",
    "        html_soup = BeautifulSoup(req.text, 'html.parser')\n",
    "        #Retrieval of the content of the article with the html tags\n",
    "        art_content_html = html_soup.find(\"section\",{\"id\":\"s1n1\"})\n",
    "        #Removal of the html tags and replacement of '\\xa0' by ''\n",
    "        art_content = art_content_html.text.replace('\\xa0','')\n",
    "        #Retrieval of the date and conversion to the datetime format\n",
    "        art_published_datetime = datetime.datetime.strptime(html_soup.find(\"meta\",{\"name\":\"citation_online_date\"})['content'], \"%Y/%m/%d\").date()\n",
    "        #Analysis of the language of the text with the TextBlob library\n",
    "        art_lang = TextBlob(art_content).detect_language()\n",
    "        #Retrieval of the title in meta property, replacing '\\xa0' by ''\n",
    "        art_title = html_soup.find(\"meta\",{\"property\":\"og:title\"})['content'].replace('\\xa0','')\n",
    "        #Retrieval of the url in meta property\n",
    "        art_url = html_soup.find(\"meta\",{\"property\":\"og:url\"})['content']\n",
    "        #Retrieval of the website's name in meta property\n",
    "        src_name = html_soup.find(\"meta\",{\"property\":\"og:site_name\"})['content']\n",
    "        src_type = 'xpath_source' #default value \n",
    "        src_url = 'https://www.erudit.org/fr/'\n",
    "        #Concatenation of the base url of the website and the end of the url of the image representing the article\n",
    "        art_img = 'https://www.erudit.org'+html_soup.find(\"meta\",{\"property\":\"og:image\"})['content']\n",
    "        #Retrieval of a list of the author(s) of the article\n",
    "        art_auth = [el.text.replace('\\n      ',' ') for el in html_soup.find_all(\"span\",{\"class\":\"nompers\"})]\n",
    "        #No tags found on this website\n",
    "        art_tag = 'no_data'\n",
    "        return [art_content,art_content_html,art_published_datetime,art_lang,art_title,art_url,src_name,\\\n",
    "                src_type,src_url,art_img,art_auth,art_tag]\n",
    "    \n",
    "    def scrap_citmar(url):\n",
    "        \n",
    "        req = get(url)\n",
    "        html_soup = BeautifulSoup(req.text, 'html.parser')\n",
    "\n",
    "        paragraphe = html_soup.find_all('p')\n",
    "        art_content_html=\" \".join([str(x) for x in paragraphe])\n",
    "        art_content=\" \".join([x.text for x in paragraphe])\n",
    "\n",
    "        art_published_datetime = html_soup.find(\"time\",{\"class\":\"entry-date published\"})['content']\n",
    "\n",
    "        language = TextBlob(art_content)\n",
    "        art_lang = language.detect_language()\n",
    "\n",
    "        art_title = html_soup.find(\"h1\",{\"class\":\"hestia-title entry-title\"}).text\n",
    "\n",
    "        art_url = url\n",
    "\n",
    "        src_name = 'citoyen-ne-s-de-marseille'\n",
    "\n",
    "        src_type = 'xpath_source'\n",
    "\n",
    "        src_url = 'https://citoyen-ne-s-de-marseille.fr/'\n",
    "\n",
    "        art_img = 'no_data'\n",
    "\n",
    "        art_auth = html_soup.find(\"strong\",{\"class\":\"fn\"}).text\n",
    "\n",
    "        art_tag = 'no_data'\n",
    "\n",
    "        return [art_content,art_content_html,art_published_datetime,art_lang,art_title,art_url,\\\n",
    "                src_name,src_type,src_url,art_img,art_auth,art_tag]\n",
    "\n",
    "    def scrap_digitrec(url):\n",
    "        \n",
    "        response = requests.get(url)\n",
    "        html_soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        paragraphs = html_soup.find_all('p')\n",
    "        art_content_html=\" \".join([str(x) for x in paragraphs])\n",
    "        art_content=\" \".join([x.text for x in paragraphs])\n",
    "\n",
    "        Datetemp = html_soup.find(\"ul\",{\"class\":\"list-inline infos\"}).text\n",
    "        art_extract_datetime = Datetemp.split(\"\\n\")[4]\n",
    "\n",
    "        a = TextBlob(art_content)\n",
    "        art_lang = a.detect_language()\n",
    "\n",
    "        art_title = html_soup.find(\"meta\",{\"property\":\"og:title\"})['content']\n",
    "\n",
    "        art_url = url\n",
    "\n",
    "        for val in re.finditer(\"(\\w)+://[^/]+/\", url):\n",
    "            src_url = val.group(0)\n",
    "\n",
    "        src_name = src_url.replace('https://','').replace('/','')\n",
    "\n",
    "        src_type = 'xpath_source'\n",
    "\n",
    "        src_img = html_soup.find(\"meta\",{\"property\":\"og:image\"})['content']\n",
    "\n",
    "        authortemp1 = html_soup.find(\"ul\",{\"class\":\"list-inline infos\"}).text\n",
    "        authortemp2 = authortemp1.split(\"\\n\")[5]\n",
    "        author = authortemp2.split(\" \")[1:]\n",
    "        art_auth = str(author[0]+\" \"+author[1])\n",
    "\n",
    "        art_tag = 'no_data'\n",
    "\n",
    "        return [art_content,art_content_html,art_extract_datetime,art_lang,art_title,art_url,\\\n",
    "                src_name,src_type,src_url,src_img,art_auth,art_tag]\n",
    "    \n",
    "    def scrap_hellofuture(url : str) -> tuple : #Scraping Rémy HelloFuture Orange\n",
    "  \n",
    "        req = get(url)\n",
    "        html_soup = BeautifulSoup(req.text, 'html.parser')\n",
    "\n",
    "        paragraphe = html_soup.find_all('p')\n",
    "        art_content_html=\" \".join([str(x) for x in paragraphe])\n",
    "        art_content=\" \".join([x.text for x in paragraphe])   \n",
    "\n",
    "        Datetemp = html_soup.find(\"div\",{\"class\":\"article__content--author\"})\n",
    "        art_published_datetime = Datetemp.find(\"time\")[\"datetime\"]    \n",
    "\n",
    "        a = TextBlob(art_content)\n",
    "        art_lang = a.detect_language()\n",
    "\n",
    "        art_title = html_soup.find(\"h1\",{\"class\":\"h1\"}).text\n",
    "\n",
    "        art_url = url\n",
    "\n",
    "        src_name = 'hello_future_orange'\n",
    "\n",
    "        src_type = 'xpath_source'\n",
    "\n",
    "        for val in re.finditer(\"(\\w)+://[^/]+/\", url):\n",
    "            src_url = val.group(0)    \n",
    "\n",
    "        if html_soup.find(\"div\",{\"class\":\"article__media\"}) :\n",
    "            art_img = html_soup.find(\"div\",{\"class\":\"article__media\"}).find(\"img\")[\"src\"]\n",
    "        else :\n",
    "            art_img = 'no_data'\n",
    "\n",
    "        art_auth = 'nodata'\n",
    "\n",
    "        art_tag = html_soup.find(\"div\",{\"class\":\"article__tag\"}).find('img')['alt']\n",
    "\n",
    "        return [art_content,art_content_html,art_published_datetime,art_lang,art_title,art_url,\\\n",
    "        src_name,src_type,src_url,art_img,art_auth,art_tag]\n",
    "    \n",
    "    def scrap_silicon(url : str) -> tuple : #Scraping Rémy silicon.fr\n",
    "  \n",
    "        req = get(url)\n",
    "        html_soup = BeautifulSoup(req.text, 'html.parser')\n",
    "\n",
    "        #Retrieval of the content of the article with the html tags\n",
    "        art_content_html = html_soup.find(\"section\",{\"class\":\"article-content\"})\n",
    "\n",
    "        #Removal of the html tags and replacement of '\\xa0' by ''\n",
    "        art_content = art_content_html.text.replace('\\xa0',' ')   \n",
    "\n",
    "        #Retrieval of the date and conversion to the datetime format\n",
    "        art_published_datetime = datetime.datetime.strptime(html_soup.find(\"meta\",{\"itemprop\":\"datePublished\"})['content'],\\\n",
    "                                                   \"%Y-%m-%dT%H:%M:%S%z\").date() \n",
    "    \n",
    "        #Analysis of the language of the text with the TextBlob library\n",
    "        art_lang = TextBlob(art_content).detect_language()\n",
    "\n",
    "        #Retrieval of the title in meta property, replacing '\\xa0' by ''\n",
    "        art_title = html_soup.find(\"meta\",{\"property\":\"og:title\"})['content'].replace('\\xa0',' ')\n",
    "\n",
    "        art_url = url\n",
    "\n",
    "        #Retrieval of the website's name in meta property\n",
    "        src_name = html_soup.find(\"meta\",{\"property\":\"og:site_name\"})['content']\n",
    "\n",
    "        src_type = 'xpath_source'\n",
    "\n",
    "        for val in re.finditer(\"(\\w)+://[^/]+/\", url):\n",
    "            src_url = val.group(0)    \n",
    "\n",
    "        #Retrieval of the image representing the article\n",
    "        #Because this website the image can be found at two different places in the html code we use a if/else condition\n",
    "        if html_soup.find(\"picture\",{\"class\":\"img\"}) is not None:\n",
    "            art_img = html_soup.find(\"picture\",{\"class\":\"img\"}).find(\"source\")[\"srcset\"]\n",
    "        else:\n",
    "            art_img = html_soup.find(\"meta\",{\"itemprop\":\"image\"})[\"content\"]\n",
    "\n",
    "        #Retrieval of the author of the article\n",
    "        art_auth = html_soup.find(\"meta\",{\"itemprop\":\"author\"})['content']\n",
    "\n",
    "        #Retrieval of the tag(s) of the article in meta property, if there are no tags we return 'no_data'\n",
    "        art_tag = [el['content'] for el in html_soup.find_all(\"meta\",{\"property\":\"article:tag\"})]\n",
    "        if art_tag == []:\n",
    "            art_tag = 'no_data'\n",
    "\n",
    "        return [art_content,art_content_html,art_published_datetime,art_lang,art_title,art_url,\\\n",
    "            src_name,src_type,src_url,art_img,art_auth,art_tag]\n",
    "    \n",
    "    #Sibel\n",
    "    \n",
    "    def scrap_riskinsight(url):\n",
    "        \"\"\"\n",
    "        This function add different information of the webpage/article to the dataFrame\n",
    "        \"\"\"\n",
    "        response = requests.get(url)\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        art_content_html = soup.find('article')\n",
    "        art_content = art_content_html.get_text()\n",
    "        src_type = 'xpath_source'\n",
    "        art_url = soup.find('meta', {'property': 'og:url'})['content']\n",
    "        src_url = BigScraper.get_base_url(art_url)\n",
    "        src_name = soup.find('meta', {'property': 'og:site_name'})['content']\n",
    "        if soup.find(\"meta\", {\"name\": \"twitter:data1\"}) is not None:\n",
    "            art_auth = soup.find(\"meta\", {\"name\": \"twitter:data1\"})['content']\n",
    "        else:\n",
    "            art_auth = 'no_data'\n",
    "        if soup.find(\"meta\", {\"property\": \"article:modified_time\"}) is not None:\n",
    "            date = soup.find(\"meta\", {\"property\": \"article:modified_time\"})[\n",
    "                \"content\"]\n",
    "            art_published_datetime = datetime.datetime.strptime(\n",
    "                date, '%Y-%m-%dT%H:%M:%S%z').date()\n",
    "        elif soup.find(\"meta\", {\"property\": \"article:published_time\"}) is not None:\n",
    "            date = soup.find(\"meta\", {\"property\": \"article:published_time\"})[\n",
    "                \"content\"]\n",
    "            art_published_datetime = datetime.datetime.strptime(\n",
    "                date, '%Y-%m-%dT%H:%M:%S%z').date()\n",
    "        else:\n",
    "            art_published_datetime = datetime.date.today()\n",
    "        art_title = soup.title.get_text()\n",
    "        art_lang = TextBlob(art_content).detect_language()\n",
    "        if soup.find_all(\"a\", {'class': \"tag--link\"}) is not None:\n",
    "            art_tag = [el.get_text()\n",
    "                       for el in soup.find_all(\"a\", {'class': \"tag--link\"})]\n",
    "        else:\n",
    "            art_tag = 'no_data'\n",
    "        if soup.find(\"meta\", {\"property\": \"og:image\"}) is not None:\n",
    "            art_img = soup.find(\n",
    "                \"meta\", {\"property\": \"og:image\"})[\"content\"]\n",
    "        else:\n",
    "            art_img = 'no_data'\n",
    "        return [art_content, art_content_html, art_published_datetime, art_lang, art_title, art_url, src_name, src_type, src_url, art_img, art_auth, art_tag]\n",
    "\n",
    "    def scrap_parlonsrh(url):\n",
    "        response = requests.get(url)\n",
    "        html_soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        #content\n",
    "        paragraphe = html_soup.find_all('p')\n",
    "        content = \" \".join([x.text for x in paragraphe])\n",
    "        #content_html\n",
    "        content_html = \" \".join([str(x) for x in paragraphe])\n",
    "        #time\n",
    "        time = html_soup.find(\"span\", {\"class\":\"date updated value-title\"})[\"title\"]\n",
    "        if time is None or time==[ ]:\n",
    "            time = 'no data'\n",
    "        else:\n",
    "            trans_month = {'01':['janvier'], \n",
    "                 '02':['février'],\n",
    "                 '03':['mars'],\n",
    "                 '04':['avril'],\n",
    "                 '05':['mai'],\n",
    "                 '06':['juin'],\n",
    "                 '07':['juillet'],\n",
    "                 '08':['août'],\n",
    "                 '09':['septembre'],\n",
    "                 '10':['octobre'],\n",
    "                 '11':['novembre'],\n",
    "                 '12':['décembre']}\n",
    "            date_tab = time.split(\" \")\n",
    "            day = date_tab[0]\n",
    "            month = date_tab[1]\n",
    "            for m in trans_month:\n",
    "                if month.lower() in trans_month[m]:\n",
    "                    month = m\n",
    "            year = date_tab[2]\n",
    "            time = datetime.date(int(year), int(month), int(day))\n",
    "        #title\n",
    "        html_title = html_soup.title\n",
    "        title = html_title.get_text()\n",
    "        #img\n",
    "        img = html_soup.find('meta', {'property':'og:image'})['content']\n",
    "        if img is None:\n",
    "            img = 'no_data'\n",
    "        #author\n",
    "        author = html_soup.find(\"span\",{\"class\":\"fn\"}).get_text()\n",
    "        if author[11:29]=='La Team Parlons RH':\n",
    "            author = author[11:29] \n",
    "        else:\n",
    "            author = author[11:34]\n",
    "        #tag\n",
    "        html_tag = html_soup.find_all(\"meta\",{'property':\"article:tag\"})\n",
    "        tags = []\n",
    "        for i in html_tag:\n",
    "            tag_i = i['content']\n",
    "            tags.append(tag_i)\n",
    "        if tags is None or tags==[ ]:\n",
    "            tags = 'no data'\n",
    "        new_row = {'art_content': content ,\n",
    "                   'art_content_html': content_html ,\n",
    "                   'art_published_datetime': time ,\n",
    "                   'art_lang': 'fr' , \n",
    "                   'art_title' : title , \n",
    "                   'art_url' : url ,\n",
    "                   'src_name' : 'parlonsrh' ,\n",
    "                   'src_type' : 'xpath_source' ,\n",
    "                   'src_url' : 'https://www.parlonsrh.com/' ,\n",
    "                   'src_img' : img ,\n",
    "                   'art_auth': author,\n",
    "                   'art_tag': tags}\n",
    "        return new_row\n",
    "    \n",
    "    def scrap_inserm(url : 'str'):\n",
    "        \"\"\"Documentation\n",
    "        function which from a url creates a BeautifulSoup object, then extract different informations about the article and the \n",
    "        source. Then researches informations about the article and the website. It finally returns all this data as a list\n",
    "\n",
    "        Parameters:\n",
    "            url(str): The url that we will scrap \n",
    "\n",
    "        Out:\n",
    "            new_row: it contains some propreties of the article and the sources \n",
    "\n",
    "        \"\"\"\n",
    "        response = requests.get(url)\n",
    "        html_soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        src_type = 'xpath_source'\n",
    "        src_url = 'https://www.inserm.fr/'\n",
    "        src_name = 'Inserm'\n",
    "        art_url = url\n",
    "        art_published_datetime = html_soup.find('time').get_text()\n",
    "        art_title = html_soup.title.get_text()\n",
    "        art_img = 'no_data'\n",
    "        html_tag = html_soup.find(\"a\",{'rel':\"category\"})\n",
    "        if html_tag is None:\n",
    "            art_tag = 'no_data'\n",
    "        else:\n",
    "            art_tag = html_tag.get_text()\n",
    "        paragraphe = html_soup.find_all('p')\n",
    "        art_content_html = \" \".join([str(x) for x in paragraphe])\n",
    "        paragraphe = html_soup.find_all('p')\n",
    "        art_content = \" \".join([x.text for x in paragraphe])\n",
    "        a = TextBlob(art_title)\n",
    "        art_lang = a.detect_language()\n",
    "        art_auth = \"no_data\"\n",
    "        new_row = {'art_content': art_content ,\n",
    "                   'art_content_html': art_content_html ,\n",
    "                   'art_published_datetime': art_published_datetime ,\n",
    "                   'art_lang': art_lang , \n",
    "                   'art_title' : art_title , \n",
    "                   'art_url' : art_url ,\n",
    "                   'src_name' : src_name ,\n",
    "                   'src_type' : src_type ,\n",
    "                   'src_url' : src_url ,\n",
    "                   'src_img' : art_img ,\n",
    "                   'art_auth': art_auth ,\n",
    "                   'art_tag': art_tag }\n",
    "\n",
    "        return new_row\n",
    "    \n",
    "    def scrap_lemonde(url : 'str'):\n",
    "        \"\"\"Documentation\n",
    "        function which from a url creates a BeautifulSoup object, then extract different informations about the article and the \n",
    "        source. Then researches informations about the article and the website. It finally returns all this data as a dict\n",
    "\n",
    "        Parameters:\n",
    "            url(str): The url that we will scrap \n",
    "\n",
    "        Out:\n",
    "            new_row: it contains some propreties of the article and the sources \n",
    "\n",
    "        \"\"\"\n",
    "        response = requests.get(url)\n",
    "        html_soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        src_type = 'xpath_source'\n",
    "        src_url = 'https://www.lemonde.fr/'\n",
    "        src_name = \"Le Monde\"\n",
    "\n",
    "        # find the article URL (art_url) \n",
    "        if html_soup.find(\"meta\", {\"property\": \"og:url\"}) is not None:\n",
    "            art_url = html_soup.find(\"meta\", {\"property\": \"og:url\"})['content']\n",
    "        else:\n",
    "            art_url = url\n",
    "\n",
    "        # find the article Title (art_title)  \n",
    "        art_title = html_soup.title.get_text().replace('\\xa0','')\n",
    "\n",
    "\n",
    "        # find the article Author (art_auth)  \n",
    "        if html_soup.find(\"meta\",{\"property\":\"og:article:author\"}) is not None:\n",
    "            art_auth = html_soup.find(\"meta\",{\"property\":\"og:article:author\"})['content']\n",
    "            if art_auth==[]: art_auth = 'no-data' \n",
    "        else:\n",
    "            art_auth = 'no_data'\n",
    "\n",
    "\n",
    "        # find the date of publication of the article (art_published_datetime) (format: datetime)  \n",
    "        if html_soup.find(\"meta\", {\"property\":\"og:article:published_time\"}) is not None:\n",
    "            art_published = html_soup.find(\"meta\", {\"property\":\"og:article:published_time\"})['content'][:10]      # take the date and remove the hour\n",
    "            art_published_datetime = datetime.datetime.strptime(art_published,'%Y-%m-%d').date()  # put at the format datetime      \n",
    "        # if there is no date, we replace None with the date of today\n",
    "        else:\n",
    "            art_published_datetime = datetime.datetime.today().date()     # \n",
    "\n",
    "\n",
    "        #src_img  \n",
    "        if html_soup.find(\"figure\", {\"class\":\"article__media\"}) is not None:\n",
    "            art_img = html_soup.find(\"figure\", {\"class\":\"article__media\"}).find('img')['src']\n",
    "        else:\n",
    "            art_img = 'no_data'\n",
    "\n",
    "\n",
    "        #art_tag  \n",
    "        #art_tag = json.loads(html_soup.find('script', type = 'application/ld+json')['@type'])\n",
    "        art_tag = \"no_data\"\n",
    "\n",
    "\n",
    "        #art_content_html  and  art_content\n",
    "        try:\n",
    "            art_content_html_corps = html_soup.find(\"article\", {\"class\": \"article__content old__article-content-single\"})\n",
    "            if html_soup.find(\"p\", {\"class\": \"article__desc\"}) is not None:\n",
    "                art_content_html_intro = html_soup.find(\"p\", {\"class\": \"article__desc\"}) #prend une sous balise en trop \n",
    "                art_content_html = [art_content_html_intro,art_content_html_corps]  \n",
    "                art_content = (art_content_html_intro.get_text() + art_content_html_corps.get_text()).replace('\\xa0','')\n",
    "            else:\n",
    "                art_content_html = art_content_html_corps\n",
    "                art_content = art_content_html_corps.get_text().replace('\\xa0','')\n",
    "                # REPLACE NE MARCHE PAS\n",
    "        except:\n",
    "            #problems with https://www.lemonde.fr/transition-ecologique/article/2020/08/03/les-villes-et-leurs-jumeaux-numeriques_6048030_179.html\n",
    "            art_content_html = np.nan\n",
    "            art_content = np.nan\n",
    "\n",
    "        #art_lang   \n",
    "        art_lang = 'fr'\n",
    "\n",
    "        #return art_content\n",
    "        new_row = {'art_content': art_content ,\n",
    "                   'art_content_html': art_content_html ,\n",
    "                   'art_published_datetime': art_published_datetime ,\n",
    "                   'art_lang': art_lang , \n",
    "                   'art_title' : art_title , \n",
    "                   'art_url' : art_url ,\n",
    "                   'src_name' : src_name ,\n",
    "                   'src_type' : src_type ,\n",
    "                   'src_url' : src_url ,\n",
    "                   'src_img' : art_img ,\n",
    "                   'art_auth': art_auth ,\n",
    "                   'art_tag': art_tag }\n",
    "        return new_row\n",
    "\n",
    " \n",
    "    def assign_scraper(url):\n",
    "        #Jason\n",
    "        if 'https://changethework.com/' in url:\n",
    "            return BigScraper.scrap_changethework(url)\n",
    "        #Marianne\n",
    "        elif 'https://www.fnccr.asso.fr/article/' in url:\n",
    "            return BigScraper.scrap_fncrr(url)\n",
    "        elif 'https://www.cnil.fr/' in url:\n",
    "            return BigScraper.scrap_cnil(url)\n",
    "        elif 'https://www.journaldunet.com/' in url:\n",
    "            return BigScraper.scrap_jdn(url)\n",
    "        elif 'https://www.zdnet.fr/' in url:\n",
    "            return BigScraper.scrap_zdnet(url)\n",
    "        #Louis\n",
    "        elif 'http://sabbar.fr/' in url:\n",
    "            return BigScraper.scrap_sabbar(url)\n",
    "        elif 'https://www.lebigdata.fr/' in url:\n",
    "            return BigScraper.scrap_lebigdata(url)\n",
    "        elif 'https://www.cadre-dirigeant-magazine.com/' in url:\n",
    "            return BigScraper.scrap_cadre(url)\n",
    "        elif 'https://www.sap.com/' in url:\n",
    "            return BigScraper.scrap_sap(url)\n",
    "        elif 'https://www.data.gouv.fr/' in url:\n",
    "            return BigScraper.scrap_datagouv(url)\n",
    "        elif 'https://blockchainfrance.net' in url:\n",
    "            return BigScraper.scrap_blockchain(url)\n",
    "        #Michael\n",
    "        elif 'https://www.theinnovation.eu/' in url:\n",
    "            return BigScraper.scrap_theinnovation(url)\n",
    "        elif 'https://www.myrhline.com/' in url:\n",
    "            return BigScraper.scrap_myrhline(url)\n",
    "        elif 'https://www.usine-digitale.fr/' in url:\n",
    "            return BigScraper.scrap_usinedigitale(url)\n",
    "        #Rémy\n",
    "        elif 'https://www.lemondeinformatique.fr/' in url:\n",
    "            return BigScraper.scrap_lemondeinformatique(url)\n",
    "        elif 'https://www.erudit.org/fr/' in url:\n",
    "            return BigScraper.scrap_erudit(url)\n",
    "        elif 'https://citoyen-ne-s-de-marseille.fr/' in url:\n",
    "            return BigScraper.scrap_citmar(url)\n",
    "        elif 'https://www.digitalrecruiters.com/' in url:\n",
    "            return BigScraper.scrap_digitrec(url)\n",
    "        elif 'https://hellofuture.orange.com/' in url:\n",
    "            return BigScraper.scrap_hellofuture(url)\n",
    "        elif 'https://www.silicon.fr/' in url:\n",
    "            return BigScraper.scrap_silicon(url)\n",
    "        #Sibel\n",
    "        elif 'https://www.riskinsight-wavestone.com/' in url:\n",
    "            return BigScraper.scrap_riskinsight(url)\n",
    "        elif 'https://www.parlonsrh.com/' in url:\n",
    "            return BigScraper.scrap_parlonsrh(url)\n",
    "        elif 'https://www.inserm.fr/' in url:\n",
    "            return BigScraper.scrap_inserm(url)\n",
    "        elif 'https://www.lemonde.fr/' in url:\n",
    "            return BigScraper.scrap_lemonde(url)\n",
    "\n",
    "\n",
    "        return None\n",
    "\n",
    "    def scrap(self, url):\n",
    "        row = BigScraper.assign_scraper(url)\n",
    "        self.add_row(row)\n",
    "        return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#     cols = ['art_content', 'art_content_html', 'art_published_datetime', 'art_lang', 'art_title',\n",
    "#             'art_url', 'src_name', 'src_type', 'src_url', 'art_img', 'art_auth', 'art_tag']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "BG = BigScraper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "row = BG.scrap('https://changethework.com/chatbot-rh-recrutement/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BG.df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row = BG.scrap(\"https://www.fnccr.asso.fr/article/big-data-territorial-publication-de-letude-de-la-fnccr/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BG.df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row = BG.scrap(\"https://www.cnil.fr/fr/video-le-youtubeur-cookie-connecte-repond-vos-questions-sur-larrivee-du-rgpd\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BG.df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row = BG.scrap(\"https://www.journaldunet.com/solutions/dsi/1496655-la-cybersecurite-est-une-question-informationnelle/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BG.df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row = BG.scrap('http://sabbar.fr/management/le-management-strategique-et-le-management-operationnel/#:~:text=Le%20management%20op%C3%A9rationnel%20correspond%20aux,pour%20atteindre%20les%20objectifs%20fix%C3%A9s.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BG.df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row = BG.scrap(\"https://www.cadre-dirigeant-magazine.com/manager/la-recherche-operationnelle-un-formidable-outil-daide-a-la-decision/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BG.df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row = BG.scrap(\"https://www.sap.com/france/products/erp-financial-management/grc.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BG.df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row = BG.scrap(\"https://www.data.gouv.fr/fr/datasets/repertoire-national-des-elus-1/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BG.df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row = BG.scrap(\"https://blockchainfrance.net/decouvrir-la-blockchain/c-est-quoi-la-blockchain/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BG.df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row = BG.scrap(\"https://www.erudit.org/fr/revues/ateliers/2019-v14-n2-ateliers05462/1071130ar/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BG.df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row = BG.scrap(\"https://citoyen-ne-s-de-marseille.fr/encore-un-immeuble-prive-finance-par-la-metropole/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BG.df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row = BG.scrap(\"https://www.digitalrecruiters.com/blog/comment-la-crise-ravive-quete-de-sens-au-travail.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BG.df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row = BG.scrap('https://www.theinnovation.eu/comment-tuer-linnovation-avec-lanalyse-financiere/45')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BG.df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row = BG.scrap('https://www.parlonsrh.com/raisons-utiliser-lintelligence-artificielle-dans-gestion-gpec/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BG.df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row = BG.scrap(\"https://hellofuture.orange.com/fr/hologramme-quatre-exemples-dune-technologie-revolutionnaire/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BG.df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row = BG.scrap(\"https://www.lebigdata.fr/base-de-donnees\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BG.df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row = BG.scrap(\"https://www.silicon.fr/emploi-it-recrutements-2021-356166.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BG.df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row = BG.scrap(\"https://www.inserm.fr/actualites-et-evenements/actualites/ondes-electromagnetiques-faut-il-craindre-5g\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "BG.df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row = BG.scrap(\"https://www.zdnet.fr/actualites/la-data-est-notre-or-noir-mais-quel-est-son-moteur-39881697.htm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "BG.df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "row = BG.scrap(\"https://www.usine-digitale.fr/article/le-specialiste-du-jeu-video-niantic-acquiert-la-plateforme-de-jeu-social-mayhem.N1046614\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>art_content</th>\n",
       "      <th>art_content_html</th>\n",
       "      <th>art_published_datetime</th>\n",
       "      <th>art_lang</th>\n",
       "      <th>art_title</th>\n",
       "      <th>art_url</th>\n",
       "      <th>src_name</th>\n",
       "      <th>src_type</th>\n",
       "      <th>src_url</th>\n",
       "      <th>art_img</th>\n",
       "      <th>art_auth</th>\n",
       "      <th>art_tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\\n\\n\\nLe spécialiste du jeu vidéo Niantic acqu...</td>\n",
       "      <td>[\\n, [\\n, [], \\n, [Le spécialiste du jeu vidéo...</td>\n",
       "      <td>2021-01-07</td>\n",
       "      <td>fr</td>\n",
       "      <td>Le spécialiste du jeu vidéo Niantic acquiert l...</td>\n",
       "      <td>https://www.usine-digitale.fr/article/le-speci...</td>\n",
       "      <td>L'Usine Digitale</td>\n",
       "      <td>xpath_source</td>\n",
       "      <td>https://www.usine-digitale.fr/</td>\n",
       "      <td>https://www.usine-digitale.fr/mediatheque/4/4/...</td>\n",
       "      <td>Aude Chardenon</td>\n",
       "      <td>[Jeux Video, Entertainment, Loisirs numériques]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         art_content  \\\n",
       "0  \\n\\n\\nLe spécialiste du jeu vidéo Niantic acqu...   \n",
       "\n",
       "                                    art_content_html art_published_datetime  \\\n",
       "0  [\\n, [\\n, [], \\n, [Le spécialiste du jeu vidéo...             2021-01-07   \n",
       "\n",
       "  art_lang                                          art_title  \\\n",
       "0       fr  Le spécialiste du jeu vidéo Niantic acquiert l...   \n",
       "\n",
       "                                             art_url          src_name  \\\n",
       "0  https://www.usine-digitale.fr/article/le-speci...  L'Usine Digitale   \n",
       "\n",
       "       src_type                         src_url  \\\n",
       "0  xpath_source  https://www.usine-digitale.fr/   \n",
       "\n",
       "                                             art_img        art_auth  \\\n",
       "0  https://www.usine-digitale.fr/mediatheque/4/4/...  Aude Chardenon   \n",
       "\n",
       "                                           art_tag  \n",
       "0  [Jeux Video, Entertainment, Loisirs numériques]  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BG.df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "row = BG.scrap(\"https://www.myrhline.com/actualite-rh/de-la-gpec-et-au-workforce-planning-les-5-evolutions-a-connaitre.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>art_content</th>\n",
       "      <th>art_content_html</th>\n",
       "      <th>art_published_datetime</th>\n",
       "      <th>art_lang</th>\n",
       "      <th>art_title</th>\n",
       "      <th>art_url</th>\n",
       "      <th>src_name</th>\n",
       "      <th>src_type</th>\n",
       "      <th>src_url</th>\n",
       "      <th>art_img</th>\n",
       "      <th>art_auth</th>\n",
       "      <th>art_tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\\n\\n\\nLe spécialiste du jeu vidéo Niantic acqu...</td>\n",
       "      <td>[\\n, [\\n, [], \\n, [Le spécialiste du jeu vidéo...</td>\n",
       "      <td>2021-01-07</td>\n",
       "      <td>fr</td>\n",
       "      <td>Le spécialiste du jeu vidéo Niantic acquiert l...</td>\n",
       "      <td>https://www.usine-digitale.fr/article/le-speci...</td>\n",
       "      <td>L'Usine Digitale</td>\n",
       "      <td>xpath_source</td>\n",
       "      <td>https://www.usine-digitale.fr/</td>\n",
       "      <td>https://www.usine-digitale.fr/mediatheque/4/4/...</td>\n",
       "      <td>Aude Chardenon</td>\n",
       "      <td>[Jeux Video, Entertainment, Loisirs numériques]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Historiquement, la GPEC est un exercice mené d...</td>\n",
       "      <td>[[ , [&lt;i aria-hidden=\"true\" class=\"fa fa-share...</td>\n",
       "      <td>2020-10-27</td>\n",
       "      <td>fr</td>\n",
       "      <td>De la GPEC et au Workforce Planning : Les 5 év...</td>\n",
       "      <td>https://www.myrhline.com/actualite-rh/de-la-gp...</td>\n",
       "      <td>myrhline.com | Actualité RH et tendances des R...</td>\n",
       "      <td>xpath_source</td>\n",
       "      <td>https://www.myrhline.com/</td>\n",
       "      <td>https://www.myrhline.com/wp-content/uploads/20...</td>\n",
       "      <td>Christophe PATTE</td>\n",
       "      <td>[Article sponsorisé, Digitalisation RH, SIRH, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         art_content  \\\n",
       "0  \\n\\n\\nLe spécialiste du jeu vidéo Niantic acqu...   \n",
       "1  Historiquement, la GPEC est un exercice mené d...   \n",
       "\n",
       "                                    art_content_html art_published_datetime  \\\n",
       "0  [\\n, [\\n, [], \\n, [Le spécialiste du jeu vidéo...             2021-01-07   \n",
       "1  [[ , [<i aria-hidden=\"true\" class=\"fa fa-share...             2020-10-27   \n",
       "\n",
       "  art_lang                                          art_title  \\\n",
       "0       fr  Le spécialiste du jeu vidéo Niantic acquiert l...   \n",
       "1       fr  De la GPEC et au Workforce Planning : Les 5 év...   \n",
       "\n",
       "                                             art_url  \\\n",
       "0  https://www.usine-digitale.fr/article/le-speci...   \n",
       "1  https://www.myrhline.com/actualite-rh/de-la-gp...   \n",
       "\n",
       "                                            src_name      src_type  \\\n",
       "0                                   L'Usine Digitale  xpath_source   \n",
       "1  myrhline.com | Actualité RH et tendances des R...  xpath_source   \n",
       "\n",
       "                          src_url  \\\n",
       "0  https://www.usine-digitale.fr/   \n",
       "1       https://www.myrhline.com/   \n",
       "\n",
       "                                             art_img          art_auth  \\\n",
       "0  https://www.usine-digitale.fr/mediatheque/4/4/...    Aude Chardenon   \n",
       "1  https://www.myrhline.com/wp-content/uploads/20...  Christophe PATTE   \n",
       "\n",
       "                                             art_tag  \n",
       "0    [Jeux Video, Entertainment, Loisirs numériques]  \n",
       "1  [Article sponsorisé, Digitalisation RH, SIRH, ...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BG.df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "row = BG.scrap(\"https://www.lemonde.fr/pixels/article/2020/09/16/donnees-de-sante-nouveau-recours-contre-le-health-data-hub-devant-le-conseil-d-etat_6052464_4408996.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>art_content</th>\n",
       "      <th>art_content_html</th>\n",
       "      <th>art_published_datetime</th>\n",
       "      <th>art_lang</th>\n",
       "      <th>art_title</th>\n",
       "      <th>art_url</th>\n",
       "      <th>src_name</th>\n",
       "      <th>src_type</th>\n",
       "      <th>src_url</th>\n",
       "      <th>art_img</th>\n",
       "      <th>art_auth</th>\n",
       "      <th>art_tag</th>\n",
       "      <th>src_img</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Les chatbots, ou agents relationnels, sont dep...</td>\n",
       "      <td>[\\n, [[Les , &lt;a href=\"https://changethework.co...</td>\n",
       "      <td>2017-11-28</td>\n",
       "      <td>fr</td>\n",
       "      <td>Chatbot RH : l'assistant fun du recrutement</td>\n",
       "      <td>https://changethework.com/chatbot-rh-recrutement/</td>\n",
       "      <td>Change the work</td>\n",
       "      <td>xpath_source</td>\n",
       "      <td>https://changethework.com/</td>\n",
       "      <td>https://changethework.com/wp-content/uploads/2...</td>\n",
       "      <td>[Léo Bernard, Aurélien Leleux]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Le Health Data Hub, lancé en décembre dern...</td>\n",
       "      <td>[[    Le Health Data Hub, lancé en décembre de...</td>\n",
       "      <td>2020-09-16</td>\n",
       "      <td>fr</td>\n",
       "      <td>Données de santé: nouveau recours contre le He...</td>\n",
       "      <td>https://www.lemonde.fr/pixels/article/2020/09/...</td>\n",
       "      <td>Le Monde</td>\n",
       "      <td>xpath_source</td>\n",
       "      <td>https://www.lemonde.fr/</td>\n",
       "      <td>NaN</td>\n",
       "      <td>no_data</td>\n",
       "      <td>no_data</td>\n",
       "      <td>no_data</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         art_content  \\\n",
       "0  Les chatbots, ou agents relationnels, sont dep...   \n",
       "1      Le Health Data Hub, lancé en décembre dern...   \n",
       "\n",
       "                                    art_content_html art_published_datetime  \\\n",
       "0  [\\n, [[Les , <a href=\"https://changethework.co...             2017-11-28   \n",
       "1  [[    Le Health Data Hub, lancé en décembre de...             2020-09-16   \n",
       "\n",
       "  art_lang                                          art_title  \\\n",
       "0       fr        Chatbot RH : l'assistant fun du recrutement   \n",
       "1       fr  Données de santé: nouveau recours contre le He...   \n",
       "\n",
       "                                             art_url         src_name  \\\n",
       "0  https://changethework.com/chatbot-rh-recrutement/  Change the work   \n",
       "1  https://www.lemonde.fr/pixels/article/2020/09/...         Le Monde   \n",
       "\n",
       "       src_type                     src_url  \\\n",
       "0  xpath_source  https://changethework.com/   \n",
       "1  xpath_source     https://www.lemonde.fr/   \n",
       "\n",
       "                                             art_img  \\\n",
       "0  https://changethework.com/wp-content/uploads/2...   \n",
       "1                                                NaN   \n",
       "\n",
       "                         art_auth  art_tag  src_img  \n",
       "0  [Léo Bernard, Aurélien Leleux]      NaN      NaN  \n",
       "1                         no_data  no_data  no_data  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BG.df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
