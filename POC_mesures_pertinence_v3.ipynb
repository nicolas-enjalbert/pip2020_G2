{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled7.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ETIb_7bvnhdb"
      },
      "source": [
        "# POC mesures de pertinence\r\n",
        "Projet Interpromo 2020, groupe 2,\r\n",
        "Corentin Prat-Marca"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3iQvTPXXnzzZ"
      },
      "source": [
        "Définition de score de pertinences pour chaque sources/sites afin de les classer et définir une priorité pour le crawl et le scrap"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ubl2-Z5haKdH",
        "outputId": "b295acd6-69aa-4838-cbe0-1e6e89c48286"
      },
      "source": [
        "import nltk   #the importations for cleandesc\r\n",
        "from nltk.corpus import stopwords\r\n",
        "from nltk.stem import LancasterStemmer\r\n",
        "from nltk.stem import SnowballStemmer \r\n",
        "nltk.download('stopwords')\r\n",
        "stop_words=nltk.corpus.stopwords.words('french') #we're working on french articles\r\n",
        "import pandas as pd\r\n",
        "\r\n",
        "\r\n",
        "def cleandesc(desc : str): #function to remove stopwords and suffixes\r\n",
        "  \"\"\"Doucumentation\r\n",
        "  @Sonia Bezombes\r\n",
        "  \"\"\"\r\n",
        "  sent = desc\r\n",
        "  sent = \"\".join([x.lower() if x.isalpha()  else \" \" for x in sent])\r\n",
        "  Porter=SnowballStemmer('french')\r\n",
        "  sent = \" \".join([Porter.stem(x) if x.lower() not in stop_words  else \"\" for x in sent.split()])\r\n",
        "  sent = \" \".join(sent.split())\r\n",
        "  return sent\r\n",
        "\r\n",
        "def calcul_score(texte : str,lexique : list): #function to count the number of words of a list in a text\r\n",
        "  \"\"\"Documentation\r\n",
        "  Parameters:\r\n",
        "    texte : the text we're working on\r\n",
        "    lexique : the list of words we're looking for\r\n",
        "  Out :\r\n",
        "    somme : the number of lexique's words we found in texte\r\n",
        "  \"\"\"\r\n",
        "  somme : int=0\r\n",
        "  for mot in lexique:\r\n",
        "    somme += texte.count(' '+mot+' ')\r\n",
        "  return somme\r\n",
        "\r\n",
        "\r\n",
        "#before using pertinence 4, we have to clean the columns we're gonna work on like that :\r\n",
        "#df_scrapped_articles['column_name']= [cleandesc(x.column_name) for x in df_scrapped_articles.itertuples()]\r\n",
        "#cleaning art_content all the time, just once\r\n",
        "#df_scrapped_articles['art_content']= [cleandesc(x.art_content) for x in df_work.itertuples()]\r\n",
        "\r\n",
        "def pertinence_4(df : pd.DataFrame , list_columns : list, lexiqueI : list, lexiqueG : list):\r\n",
        "  \"\"\"Documentation\r\n",
        "  Parameters:\r\n",
        "    df : a data frame of scrapped articles. It must contains a \"art_content\" columns to determine its size\r\n",
        "    list_columns : the list of columns we want to count the words of the two lexiques on. \r\n",
        "    lexiqueI : the innovation lexique, it has to be a list\r\n",
        "    lexiqueG : the gestion lexique, it has to be a list\r\n",
        "  Out :\r\n",
        "    df : the same data but with the scores and the step to have it\r\n",
        "  \"\"\"\r\n",
        "  list_nom_colonne_I : list = [] #the list of the names of the innovation's score columns \r\n",
        "  list_nom_colonne_G : list = [] #the list of the names of the gestion's score columns \r\n",
        "  cleanedG : list = [cleandesc(mot) for mot in lexiqueG] #cleaning the gestion's lexique\r\n",
        "  cleanedI : list = [cleandesc(mot) for mot in lexiqueI] #cleaning the gestion's lexique\r\n",
        "\r\n",
        "  for columns in list_columns:\r\n",
        "\r\n",
        "    nomI : str ='score_innovation_'+columns #the name of the new columns : score_innovation_column_name or score_gestion_column_name\r\n",
        "    nomG : str ='score_gestion_'+columns\r\n",
        "    df[nomI]=[calcul_score(df.loc[i][columns], cleanedI) for i in range(len(df))] #creating the new columns using calcul_score, counting the number of words\r\n",
        "    df[nomG]=[calcul_score(df.loc[i][columns], cleanedG) for i in range(len(df))]\r\n",
        "    df['score_'+columns]=[df.loc[i][nomI]+df.loc[i][nomG] for i in range(len(df))] #new column, wich is the sum of gestion and innovation scores\r\n",
        "    list_nom_colonne_I.append(nomI)\r\n",
        "    list_nom_colonne_G.append(nomG)\r\n",
        "  \r\n",
        "  df['taille_content'] = [len(df['art_content'][i]) for i in range(len(df))] #length of the cleaned content of the article\r\n",
        "  df['score_I'] = df[list_nom_colonne_I].sum(axis = 1) #sum of all the innovation's scores\r\n",
        "  df['score_G'] = df[list_nom_colonne_G].sum(axis = 1) #sum of all the gestion's scores\r\n",
        "  df['score_abs'] = abs(df['score_I']-df['score_G']) #absolute value of the difference of the number of gestion's words and innovation's words\r\n",
        "  df['detct_0'] = [0 if (x.score_I == 0) or (x.score_G == 0) else 1 for x in df.itertuples()] #finding if there is 0 gestion's words or 0 innovation's words, to penalise the articles which don't speak of innovation or gestion\r\n",
        "  df['score_total'] = [(1.09*(x.score_I+x.score_G)-abs(x.score_I-x.score_G))*100/x.taille_content if x.detct_0 == 0 else (1.09*(x.score_I+x.score_G)-abs(x.score_I-x.score_G))*1000/x.taille_content for x in df.itertuples()]\r\n",
        "  \r\n",
        "  return df"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XR9SbuETaJ5z"
      },
      "source": [
        "On cherche à attribuer un score à des articles scrappés en regardant combien on trouve de mots de chaque lexique. On choisit sur quels parties on se base (nom de l'article, contenu, etc), et on calcule un score dépendant du nombre de mots des lexiques (plus il y a de mots mieux c'est, si un article comporte 0 mots d'un lexique on pénalise) de la différence des nombres de mots de chaque lexique (on veut à peu près autant de mots de chaque lexique), et de la taille de l'article."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1MV02FFUaLhU"
      },
      "source": [
        "import re\r\n",
        "def url_site(url_art : str):\r\n",
        "  \"\"\"Documentation\r\n",
        "  @Flavien Caminade et Corentin Prat-Marca\r\n",
        "  Parameters:\r\n",
        "    url_art : the url of an article\r\n",
        "  Out :\r\n",
        "    url_site : the url of a site obtained with the article's url\r\n",
        "    \"\"\"\r\n",
        "  site : list  =url_art.split(\"://\")\r\n",
        "  #keep everything on the right side of \"https://\"\"\r\n",
        "  if site[0]==\"https\" or site[0]==\"http\":\r\n",
        "      url_site : str = site[1]\r\n",
        "  else:\r\n",
        "      url_site : str = site[0]\r\n",
        "  #delete everything after the first \"/\"\r\n",
        "  tab : list = url_site.split(\"/\")\r\n",
        "  url_site = tab[0]\r\n",
        "  return url_site\r\n",
        "\r\n",
        "def pertinence_4_source(df_pertinence_4 : pd.DataFrame):\r\n",
        "  \"\"\"Documentation\r\n",
        "  Parameters:\r\n",
        "    df_pertinence_4 : a data frame of scrapped articles with their score\r\n",
        "  Out :\r\n",
        "    df : a data frame with the score for each sites\r\n",
        "  \"\"\"\r\n",
        "  dico : dict ={} #dictionnary : keys=sources url, values=sum of the scores of each articles\r\n",
        "  for articles in df_calcul_global.itertuples():\r\n",
        "\r\n",
        "    if url_site(articles.art_url) in dico: #if the source il already in the dictionnary\r\n",
        "      dico[url_site(articles.art_url)][0] += (articles.score_total)**2 #using square so higher score have more values\r\n",
        "      dico[url_site(articles.art_url)][1] += 1 #increasing the number of articles for the source\r\n",
        "    else :\r\n",
        "      dico[url_site(articles.art_url)] = [(articles.score_total)**2, 1]\r\n",
        "\r\n",
        "  df : pd.DataFrame = pd.DataFrame.from_dict(dico, orient = 'index', columns = ['score_total', 'nb_articles'])\r\n",
        "  df['score_moyen'] = df.apply(lambda row: row.score_total / row.nb_articles, axis = 1) #mean of the scores of the artciles for each sources\r\n",
        "  \r\n",
        "  return df\r\n",
        "#pertinence_4_source(df_pertinence_4)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p8HthiBDaL7v"
      },
      "source": [
        "On utilise les scores des articles pour obtenir un score pour chaque source en faisant la moyenne. Pour cela on utilise une fonction qui à partir d'un url d'article trouve l'url du site source. Du coup, on se retrouve avec linkedin.com=/=fr.linkedin, mais 2 sites différents avec le même nom (site.fr, www.site.com, etc) sont bien différenciés. Ca nous permet aussi de faire la différence entre la version française et anglaise d'un site."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4iI0al5XWAz_"
      },
      "source": [
        "def pertinence_6(df : pd.DataFrame): #df avec proba innovation et proba gestion\r\n",
        "  \"\"\"Documentation\r\n",
        "  Parameters:\r\n",
        "    df : a data frame of scrapped articles with the probability for each to contain a \"gestion\" word or an \"innovation\" word (group 5 work)\r\n",
        "  Out :\r\n",
        "    df : the same data frame with 4 more columns each containing a score based on the probability\r\n",
        "    \"\"\"\r\n",
        "  df['pertinence_10'] = df.apply(lambda row: (10*(row.probaInnovation + row.probaGestion) - abs(row.probaInnovation-row.probaGestion))/20, axis = 1) #(10*(motsGestion+motsInnovation)-abs(motsGestion-motsInnovation)) / 20 (pour un score entre 0 et 1)\r\n",
        "  df['pertinence_5'] = df.apply(lambda row: (5*(row.probaInnovation + row.probaGestion) - abs(row.probaInnovation-row.probaGestion))/10, axis = 1) #(5*(motsGestion+motsInnovation)-abs(motsGestion-motsInnovation)) / 10 (pour un score entre 0 et 1)\r\n",
        "  df['pertinence_2'] = df.apply(lambda row: (2*(row.probaInnovation + row.probaGestion) - abs(row.probaInnovation-row.probaGestion))/4, axis = 1) #(2*(motsGestion+motsInnovation)-abs(motsGestion-motsInnovation)) / 4 (pour un score entre 0 et 1)\r\n",
        "  df['pertinence_F1'] = df.apply(lambda row: 2*(row.probaInnovation * row.probaGestion) / (row.probaInnovation+row.probaGestion), axis = 1) #F1 score adapté\r\n",
        "  \r\n",
        "  return df"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jop2-2wZWZyb"
      },
      "source": [
        "Cette fonction permet de créer un score de pertinance à partir d'un df d'articles scrappés dont on a les probabilités d'apparitions d'un mot de gestions ou d'innovation (travail du groupe 5).\r\n",
        "Elle permet d'évaluer les articles pour avoir une idée de la pertinence. Les scores vont de 0 à 1, et prennent en compte la somme des probabilités (plus on a de mots des lexiques, mieux c'est) et la différence des probabilités (les sites avec un nombre à peu près egal de mots de chaques lexiques sont plus intéressants)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fnPnsjY7Wbpo"
      },
      "source": [
        "def pertinence_6_source(df_pertinence_6 : pd.DataFrame):\r\n",
        "  \"\"\"Documentation\r\n",
        "  Parameters: \r\n",
        "    df_pertinence_6 : a data frame of scrapped articles with their score, obtained with the function pertinence_6\r\n",
        "  Out :\r\n",
        "    df : a data frame of the means of each score for each source\r\n",
        "    \"\"\"\r\n",
        "  dico_pertinence : dict = {} #key : a source, values : a list [score_10,score_5,score_2,score_F1,number of articles]\r\n",
        "  for articles in df_pertinence_6.itertuples():\r\n",
        "\r\n",
        "    if url_site(articles.art_url) in dico_pertinence: #if the source arlready is in the dictionnary\r\n",
        "      dico_pertinence[url_site(articles.art_url)][0] += articles.pertinence_10**2 #using square function so that articles with a higher score are more important\r\n",
        "      dico_pertinence[url_site(articles.art_url)][1] += articles.pertinence_5**2\r\n",
        "      dico_pertinence[url_site(articles.art_url)][2] += articles.pertinence_2**2\r\n",
        "      dico_pertinence[url_site(articles.art_url)][3] += articles.pertinence_F1**2\r\n",
        "      dico_pertinence[url_site(articles.art_url)][4] += 1 #increasing the number or articles\r\n",
        "    else :\r\n",
        "      dico_pertinence[url_site(articles.art_url)]=[articles.pertinence_10**2,articles.pertinence_5**2,articles.pertinence_2**2,articles.pertinence_F1**2,1] #adding a new source\r\n",
        "  \r\n",
        "  df : pd.DataFrame = pd.DataFrame.from_dict(dico_pertinence, orient = 'index', columns = ['pertinence_10','pertinence_5','pertinence_2','pertinence_F1', 'nb_articles']) #turning the dictionnary into a data frame\r\n",
        "  df['pertinence_10_moyenne'] = df.apply(lambda row: row.pertinence_10 / row.nb_articles, axis = 1)\r\n",
        "  df['pertinence_5_moyenne'] = df.apply(lambda row: row.pertinence_5 / row.nb_articles, axis = 1)\r\n",
        "  df['pertinence_2_moyenne'] = df.apply(lambda row: row.pertinence_2 / row.nb_articles, axis = 1)\r\n",
        "  df['pertinence_F1_moyenne'] = df.apply(lambda row: row.pertinence_F1 / row.nb_articles, axis = 1)\r\n",
        "  \r\n",
        "  return df"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M--lIncOZs27"
      },
      "source": [
        "On cherche à obtenir la pertinence des sites, donc on fait la moyenne de la pertinence de chaque articles des sites obtenue avec la fonction précédente. On utilise la fonction carré pour que les articles avec un score faible soient moins importants dans le score total que ceux avec un score élevé."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nLeRTUP4oEvU"
      },
      "source": [
        "En résumé, on a 2 manières d'obtenir des scores de pertinence qu'il peut être intéressant de combiner. Par contre elles sont très dépendantes des lexiques et les synonymes ne sont pas forcément pris en compte."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B4AINGxHol40"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}