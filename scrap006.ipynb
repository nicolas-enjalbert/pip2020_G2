{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install textblob \n",
    "#language recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime #today's date\n",
    "from textblob import TextBlob \n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import time #delay for scrapping multiple articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://www.myrhline.com/actualite-rh/de-la-gpec-et-au-workforce-planning-les-5-evolutions-a-connaitre.html\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>art_content</th>\n",
       "      <th>art_content_html</th>\n",
       "      <th>art_extract_datetime</th>\n",
       "      <th>art_lang</th>\n",
       "      <th>art_title</th>\n",
       "      <th>art_url</th>\n",
       "      <th>src_name</th>\n",
       "      <th>src_type</th>\n",
       "      <th>src_url</th>\n",
       "      <th>src_img</th>\n",
       "      <th>art_auth</th>\n",
       "      <th>art_tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [art_content, art_content_html, art_extract_datetime, art_lang, art_title, art_url, src_name, src_type, src_url, src_img, art_auth, art_tag]\n",
       "Index: []"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df= pd.DataFrame(columns=['art_content','art_content_html','art_extract_datetime','art_lang','art_title','art_url','src_name','src_type','src_url','src_img','art_auth','art_tag']  )\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "In: url\n",
    "Out: a tuple of strings\n",
    "\"\"\"\n",
    "\n",
    "def get_all(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    art_content_html = soup.find(\"div\", {\"class\": \"post-detail-wrap\"})\n",
    "    if art_content_html is None:\n",
    "        art_content_html = no_data\n",
    "\n",
    "    art_content = str()\n",
    "    for paragraph in art_content_html:\n",
    "        art_paraph = paragraph.text.replace('\\xa0', '')\n",
    "        art_content += art_paraph\n",
    "    if art_content is None:\n",
    "        art_content = no_data    \n",
    "   \n",
    "    if soup.find(\"meta\", {\"property\": \"article:modified_time\"}) is not None:\n",
    "        art_extract_datetime = soup.find(\"meta\", {\"property\": \"article:modified_time\"})[\"content\"]\n",
    "    elif soup.find(\"meta\", {\"property\": \"article:published_time\"}) is not None:\n",
    "        art_extract_datetime = soup.find(\"meta\", {\"property\": \"article:published_time\"})[\"content\"]\n",
    "    else:\n",
    "        art_extract_datetime = datetime.datetime.now()\n",
    "    \n",
    "    if soup.find(\"meta\", {\"property\": \"og:locale\"}) is not None:\n",
    "        art_lang = soup.find(\"meta\", {\"property\": \"og:locale\"})[\"content\"]\n",
    "    else:\n",
    "        art_lang = no_data\n",
    "    \n",
    "    if soup.find(\"meta\", {\"property\": \"og:title\"}) is not None:\n",
    "        art_title = soup.find(\"meta\", {\"property\": \"og:title\"})[\"content\"]\n",
    "    else:\n",
    "        art_title = no_data\n",
    "        \n",
    "    if soup.find(\"meta\", {\"property\": \"og:url\"}) is not None:\n",
    "        art_url = soup.find(\"meta\", {\"property\": \"og:url\"})[\"content\"]\n",
    "    else:\n",
    "        art_url = no_data\n",
    "    \n",
    "    if soup.find(\"meta\", {\"property\": \"og:site_name\"}) is not None:\n",
    "        src_name = soup.find(\"meta\", {\"property\": \"og:site_name\"})[\"content\"]\n",
    "    else:\n",
    "        src_name = no_data\n",
    "    \n",
    "    src_type = \"xpath_source\" #default value  \n",
    "    \n",
    "    if soup.find(\"div\", {\"class\": \"cscra-brand dhve-mobile-logo dhve-retina-logo hav-normal-logo\"}).find(\"a\") is not None:\n",
    "        src_url = soup.find(\"div\", {\"class\": \"cscra-brand dhve-mobile-logo dhve-retina-logo hav-normal-logo\"}).find(\"a\")[\"href\"]\n",
    "    else:\n",
    "        src_url = no_data\n",
    "    \n",
    "    if soup.find(\"meta\", {\"property\": \"og:image\"}) is not None:\n",
    "        src_img = soup.find(\"meta\", {\"property\": \"og:image\"})[\"content\"]\n",
    "    else:\n",
    "        src_img = no_data\n",
    "        \n",
    "    if soup.find(\"meta\", {\"name\": \"author\"}) is not None:   \n",
    "        art_auth = soup.find(\"meta\", {\"name\": \"author\"})[\"content\"]\n",
    "    else:\n",
    "        art_auth = no_data\n",
    "\n",
    "    if soup.find_all(\"a\", {\"rel\": \"tag\"}) is not None:\n",
    "        art_tags = soup.find_all(\"a\", {\"rel\": \"tag\"})\n",
    "        art_tag = []\n",
    "        for tag in art_tags:\n",
    "            art_tag.append(str(tag.text))\n",
    "    else:\n",
    "        art_tag = no_data\n",
    "    \n",
    "    \n",
    "    return art_content, art_content_html, art_extract_datetime, art_lang, art_title, art_url,\\\n",
    "        src_name, src_type, src_url, src_img, art_auth, art_tag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'art_content' [0],'art_content_html'[1],'art_extract_datetime'[2],'art_lang'[3],\n",
    "    'art_title'[4],'art_url'[5],'src_name'[6],'src_type'[7],'src_url'[8],'src_img'[9],'art_auth'[10],'art_tag'[11]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "In: url\n",
    "Out: url of the previous article\n",
    "\"\"\"\n",
    "\n",
    "def get_prev(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    prev_soup = soup.find(\"div\", {\"class\": \"col-sm-6 col-xs-6\"}).find(\"a\")\n",
    "    if prev_soup is not None:\n",
    "        prev_url = prev_soup[\"href\"]\n",
    "        return prev_url\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "\"\"\"\n",
    "In: url\n",
    "Out: url of the following article\n",
    "\"\"\"\n",
    "    \n",
    "def get_next(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    next_soup = soup.find(\"div\", {\"class\": \"col-sm-6 col-xs-6 text-right\"}).find(\"a\")\n",
    "    if next_soup is not None:\n",
    "        next_url = next_soup[\"href\"]\n",
    "        return next_url\n",
    "    else:\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "In: url \n",
    "Out: \n",
    "\"\"\"\n",
    "\n",
    "def art_date(url):\n",
    "    url = get_prev(url)\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    if soup.find(\"meta\", {\"property\": \"article:modified_time\"}) is not None:\n",
    "        art_extract_datetime = soup.find(\"meta\", {\"property\": \"article:modified_time\"})[\"content\"]\n",
    "    elif soup.find(\"meta\", {\"property\": \"article:published_time\"}) is not None:\n",
    "        art_extract_datetime = soup.find(\"meta\", {\"property\": \"article:published_time\"})[\"content\"]\n",
    "\n",
    "    date_article = datetime.datetime.strptime(art_extract_datetime, '%Y-%m-%dT%H:%M:%S+00:00')\n",
    "    year = int(date_article.strftime('%Y'))\n",
    "    return year    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "In: url, year after which the articles have to be published\n",
    "Out: list of preceding articles\n",
    "\"\"\"\n",
    "\n",
    "def all_before(url, year):\n",
    "    before = []\n",
    "    while get_prev(url) is not None:\n",
    "        url = get_prev(url)\n",
    "        published = art_date(url)\n",
    "        if published > year:\n",
    "            before.append(url)\n",
    "                \n",
    "    return list(set(before))\n",
    "\n",
    "\"\"\"\n",
    "In: url, year after which the articles have to be published\n",
    "Out: list of following articles\n",
    "\"\"\"\n",
    "\n",
    "def all_after(url, year):\n",
    "    after = []\n",
    "    while get_next(url) is not None:\n",
    "        url = get_next(url)\n",
    "        published = art_date(url)\n",
    "        if published > year:\n",
    "            after.append(url)\n",
    "    return list(set(after))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "In: url, year after which the articles have to be published\n",
    "Out: list of all articles\n",
    "\"\"\"\n",
    "\n",
    "def all_in(url, year):\n",
    "    all_be = all_before(url, year)\n",
    "    all_af = all_after(url, year)\n",
    "    url_date = art_date(url)\n",
    "    if url_date > year:\n",
    "        return list(set(all_be + [url] + all_af))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "In: list of urls\n",
    "Out: dataframe with all scraped articles\n",
    "\"\"\"\n",
    "\n",
    "def scrap_all(list_urls):\n",
    "    i = 1\n",
    "    df= pd.DataFrame(columns=['art_content','art_content_html','art_extract_datetime',\\\n",
    "                              'art_lang','art_title','art_url','src_name','src_type','src_url',\\\n",
    "                              'src_img','art_auth','art_tag'])\n",
    "    for url in list_urls:\n",
    "        df.loc[i] = get_all(url)\n",
    "        i+=1\n",
    "    return df  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "In: url of a random article\n",
    "Out: dataframe with all scraped articles\n",
    "\"\"\"\n",
    "\n",
    "def do_it_all(url):\n",
    "    list_urls = all_in(url)\n",
    "    scrap_all(list_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "do_it_all(url)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
