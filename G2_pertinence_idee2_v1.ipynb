{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled22_3_1 (5).ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9qz37Qvo6sYO",
        "outputId": "6efa4b7d-4127-42aa-cdf1-b32d39880760"
      },
      "source": [
        "!pip install deplacy\r\n",
        "!python -m spacy download fr_core_news_sm"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting deplacy\n",
            "  Downloading https://files.pythonhosted.org/packages/ff/8a/d5417f6f4c6e8d8da5a1e31d3874e3c37ff06362657b8b185405148b6566/deplacy-1.8.8-py3-none-any.whl\n",
            "Installing collected packages: deplacy\n",
            "Successfully installed deplacy-1.8.8\n",
            "Collecting fr_core_news_sm==2.2.5\n",
            "\u001b[?25l  Downloading https://github.com/explosion/spacy-models/releases/download/fr_core_news_sm-2.2.5/fr_core_news_sm-2.2.5.tar.gz (14.7MB)\n",
            "\u001b[K     |████████████████████████████████| 14.7MB 3.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.6/dist-packages (from fr_core_news_sm==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->fr_core_news_sm==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->fr_core_news_sm==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->fr_core_news_sm==2.2.5) (4.41.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->fr_core_news_sm==2.2.5) (1.19.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->fr_core_news_sm==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->fr_core_news_sm==2.2.5) (3.0.5)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->fr_core_news_sm==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->fr_core_news_sm==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->fr_core_news_sm==2.2.5) (2.0.5)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->fr_core_news_sm==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->fr_core_news_sm==2.2.5) (51.1.1)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->fr_core_news_sm==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->fr_core_news_sm==2.2.5) (0.8.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->fr_core_news_sm==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->fr_core_news_sm==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->fr_core_news_sm==2.2.5) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->fr_core_news_sm==2.2.5) (2.10)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->fr_core_news_sm==2.2.5) (3.3.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->fr_core_news_sm==2.2.5) (3.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->fr_core_news_sm==2.2.5) (3.7.4.3)\n",
            "Building wheels for collected packages: fr-core-news-sm\n",
            "  Building wheel for fr-core-news-sm (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fr-core-news-sm: filename=fr_core_news_sm-2.2.5-cp36-none-any.whl size=14727027 sha256=6f4e257934a3b6e979c0fcaf2ae5a172b4a3e8ce0ec68257ccc11e2011ceedc3\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-vli8gjn8/wheels/46/1b/e6/29b020e3f9420a24c3f463343afe5136aaaf955dbc9e46dfc5\n",
            "Successfully built fr-core-news-sm\n",
            "Installing collected packages: fr-core-news-sm\n",
            "Successfully installed fr-core-news-sm-2.2.5\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('fr_core_news_sm')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CU2Am6Wy6uTH"
      },
      "source": [
        "import pkg_resources,imp\r\n",
        "imp.reload(pkg_resources)\r\n",
        "import spacy\r\n",
        "import re\r\n",
        "import numpy as np\r\n",
        "nlp=spacy.load(\"fr_core_news_sm\")"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BUjat0Z56uV3",
        "outputId": "43d9f354-7d5a-4c18-da46-dd56c083d03f"
      },
      "source": [
        "import nltk\r\n",
        "nltk.download('stopwords')\r\n",
        "stopword=nltk.corpus.stopwords.words('french')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "33fBa7tm6uZL",
        "outputId": "3a851b3b-c895-4684-e771-b4ccd393e633"
      },
      "source": [
        "nltk.download('wordnet')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WQcPkaGU6ujm"
      },
      "source": [
        "from nltk.stem.snowball import FrenchStemmer\r\n",
        "stemmer = FrenchStemmer()"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_4pgmAXi635F"
      },
      "source": [
        "wn=nltk.WordNetLemmatizer()\r\n",
        "def tokenize(text):\r\n",
        "    tokens = re.split('\\W+', text) #W+ means a word character or - can go there\r\n",
        "    return tokens\r\n",
        "def remove_stopwords(tokenzed_list):\r\n",
        "    text=[word for word in tokenzed_list if word not in stopword]\r\n",
        "    return text\r\n",
        "def lemmatizing(tokenized_text):\r\n",
        "    texte=[]\r\n",
        "    for elem in tokenized_text:\r\n",
        "      texte.append(stemmer.stem(elem))\r\n",
        "    return texte\r\n",
        "def return_mean_embedding(sentence):\r\n",
        "    # Tokeniser la phrase\r\n",
        "    doc = nlp(sentence)\r\n",
        "    # Retourner la moyenne des vecteurs pour chaque phrase\r\n",
        "    return np.mean([(X.vector) for X in doc], axis=0)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0LLdsXJi638K"
      },
      "source": [
        "def return_word_embedding(sentence):\r\n",
        "    # Tokeniser la phrase\r\n",
        "    doc = nlp(sentence)\r\n",
        "    # Retourner le vecteur lié à chaque token\r\n",
        "    return [(X.vector) for X in doc]\r\n",
        "def return_token(sentence):\r\n",
        "    # Tokeniser la phrase\r\n",
        "    doc = nlp(sentence)\r\n",
        "    # Retourner le texte de chaque token\r\n",
        "    return [X.text for X in doc]"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DkCZL8bvqUnx"
      },
      "source": [
        "import pandas as pd\r\n",
        "import numpy as np"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TC0def4mqbbW",
        "outputId": "426934a2-d517-4288-9ee8-883354797673"
      },
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x8fxJrNhq8NQ"
      },
      "source": [
        "df_test = pd.read_csv('/content/drive/My Drive/df_test.csv',delimiter=';')"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uRm4yZ-h-zvI"
      },
      "source": [
        "df_test['query'][0]='(machine learning AND test) OR (cat AND du futur) OR (adorer AND dog) OR (du futur AND machine learning)'\r\n",
        "df_test['resume'][0]='adorer le machine learning et du futur'"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SoDID6iH-aq9"
      },
      "source": [
        "df_test['title'] = [x.lower() for x in df_test.title]"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QeGsTDNb_pOG"
      },
      "source": [
        "for i in range (len(df_test)):\r\n",
        "  df_test['query'][i] = df_test['query'][i].lower()"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Txve4Eko_Ba4",
        "outputId": "918f6e0e-c9e8-496b-9759-ec46cd076535"
      },
      "source": [
        "import nltk\r\n",
        "nltk.download('stopwords')\r\n",
        "import pandas as pd\r\n",
        "import pickle\r\n",
        "import time\r\n",
        "import nltk\r\n",
        "from nltk.corpus import stopwords\r\n",
        "stop_words = set(stopwords.words('french'))\r\n",
        "from nltk.stem import LancasterStemmer\r\n",
        "import numpy as np\r\n",
        "import time"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9C0PpMMH-bkL",
        "outputId": "1e97adb5-a53e-4db5-8f13-a63e3ad71388"
      },
      "source": [
        "from nltk.stem import SnowballStemmer\r\n",
        "def cleandesc(desc):\r\n",
        "    sent = desc\r\n",
        "    sent = \"\".join([x.lower() if x.isalpha()  else \" \" for x in sent])\r\n",
        "    Porter=SnowballStemmer('french')\r\n",
        "    sent = \" \".join([Porter.stem(x) if x.lower() not in stop_words  else \"\" for x in sent.split()])\r\n",
        "    sent = \" \".join(sent.split())\r\n",
        "    return sent\r\n",
        "start_time=time.time()\r\n",
        "df_test['title']= [cleandesc(x.title) for x in df_test.itertuples()]\r\n",
        "df_test['resume']= [cleandesc(x.resume) for x in df_test.itertuples()]\r\n",
        "df_test['query']= [cleandesc(x.query) for x in df_test.itertuples()]\r\n",
        "end_time=time.time()\r\n",
        "print(\"total time : {} mn\".format((end_time-start_time)/60))"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total time : 9.655157725016275e-05 mn\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hpY_JTYdtmms"
      },
      "source": [
        "df_test['cat_pertinence']='Inconnu'"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WSWQV4yLojlc"
      },
      "source": [
        "pertinence = []\r\n",
        "pertinence_j = []\r\n",
        "def mots_commun2(df) :\r\n",
        "  for j in range (len(df)): # pour chaque ligne du df,\r\n",
        "    innov=0\r\n",
        "    gest=0\r\n",
        "    separator_or = list(df['query'][j].split(' or ')) #on stocke tous les couple de la requete dans une liste separator_or\r\n",
        "    list_test = []\r\n",
        "    for k in range (len(separator_or)): #pour chacun de ces couples,\r\n",
        "      nb_find = 0\r\n",
        "      separator_and = list(separator_or[k].split(' and ')) #on stocke tous les mots du couple de la requete dans une liste separator_and\r\n",
        "      nb_present = 0\r\n",
        "      for i in range (len(separator_and)): #pour tous les mots de la requete,\r\n",
        "        #print(separator_and[i])\r\n",
        "        if (df['title'][j].find(separator_and[i]) != -1) : #on le cherche dans le titre, et si c'est présent...\r\n",
        "          if (i==0):\r\n",
        "            innov+=1 #si la position est 0, c'est de l'innovation : on incrémente\r\n",
        "          else :\r\n",
        "            gest+=1 #si la position est 1, c'est de la gestion : on incrémente\r\n",
        "          nb_present = nb_present + 1 #on incrémente le nombre de mots de la requete trouvé \r\n",
        "          list_test.append(separator_and[i]+\";\") # et on stock le mot trouvé dans le titre dans une liste\r\n",
        "        if (df['resume'][j].find(separator_and[i]) != -1) and (df['resume'][j] not in list_test) : #ensuite, on le cherche dans le resumé, et si c'est présent...\r\n",
        "          if (i==0):\r\n",
        "            innov+=1 \r\n",
        "          else :\r\n",
        "            gest+=1\r\n",
        "          nb_present = nb_present + 1 #on incrémente le nombre de mots trouvés\r\n",
        "          list_test.append(separator_and[i]+\";\") #... on le met dans une liste\r\n",
        "          list_test = list(set(list_test))\r\n",
        "      str = ' '.join(list_test) #...\r\n",
        "      df['mots'][j]=str[:-1] #...on l'ajoute\r\n",
        "      #print('le df pour ',k)\r\n",
        "      #print(df['mots'])\r\n",
        "      if (k > 0) :\r\n",
        "        nb_find = len(df_test['mots'][j].split(';'))\r\n",
        "        print('on a trouve ',nb_find)\r\n",
        "        print('pour le ',k)\r\n",
        "        nb = (nb_find / (len(separator_and)+len(separator_or)))*100\r\n",
        "        pertinence.append(nb)\r\n",
        "        pertinence_j.append(j)\r\n",
        "        if (pertinence[k]<pertinence[k-1]): #on cherche à savoir pour ce couple, si le score précédent était meilleur ou pas. Si oui, on prend celui d'avant car c'est un OR.\r\n",
        "          pertinence[k]=pertinence[k-1]\r\n",
        "          nb = pertinence[k]\r\n",
        "      else :\r\n",
        "        nb_find = len(df_test['mots'][j].split(';'))\r\n",
        "        nb = (nb_find / len(separator_and))*100\r\n",
        "        pertinence.append(nb)\r\n",
        "        pertinence_j.append(j)\r\n",
        "      if (nb>=100) :\r\n",
        "        df['cat_pertinence'][j] = 'I&G' #si on a 100%, on a les 2\r\n",
        "      elif (nb>=50 and nb<100) :\r\n",
        "        df['cat_pertinence'][j] = 'I&G mais pas du meme couple' #à préciser si c'est l'un ou l'autre\r\n",
        "      elif (nb<50 and nb>0 and innov>gest):\r\n",
        "        df['cat_pertinence'][j] = 'I'\r\n",
        "      elif (nb<50 and nb>0 and gest>innov):\r\n",
        "        df['cat_pertinence'][j] = 'G'\r\n",
        "      else :\r\n",
        "        df['cat_pertinence'][j] = 'Aucun'\r\n",
        "      #print(nb,'% des mots de la requête apparaissent dans le titre ou dans le résumé.')\r\n",
        "      #print('innov ', innov)\r\n",
        "      #print('gestion ', gest)\r\n",
        "  pertinence_best = pd.DataFrame(columns=['ligne','score'])\r\n",
        "  pertinence_best['ligne'] = pertinence_j\r\n",
        "  pertinence_best['score'] = pertinence\r\n",
        "  print(pertinence_best)"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M9d9GTV6ovdj",
        "outputId": "4050a0d8-97ab-4588-8cf1-ef57737d56dd"
      },
      "source": [
        "mots_commun2(df_test)"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "on a trouve  2\n",
            "pour le  1\n",
            "on a trouve  3\n",
            "pour le  2\n",
            "on a trouve  3\n",
            "pour le  3\n",
            "   ligne  score\n",
            "0      0   50.0\n",
            "1      0   50.0\n",
            "2      0   50.0\n",
            "3      0   50.0\n",
            "4      1  100.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hjOqmBSZCCMK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "outputId": "b22007e1-9bac-4848-c766-6888f03f93a0"
      },
      "source": [
        "df_test"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>url</th>\n",
              "      <th>query</th>\n",
              "      <th>title</th>\n",
              "      <th>resume</th>\n",
              "      <th>mots</th>\n",
              "      <th>cat_pertinence</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>https://graylinegroup.com/machine-learning-and...</td>\n",
              "      <td>machin learning and test or cat and futur or a...</td>\n",
              "      <td>machin learning and the new innov paradigm</td>\n",
              "      <td>ador machin learning futur</td>\n",
              "      <td>futur; machin learning; ador</td>\n",
              "      <td>I&amp;G mais pas du meme couple</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>https://graylinegroup.com/machine-learning-and...</td>\n",
              "      <td>machin learning and innov</td>\n",
              "      <td>machin learning and the new innov paradigm</td>\n",
              "      <td>machin learning is very innov it</td>\n",
              "      <td>innov; machin learning</td>\n",
              "      <td>I&amp;G</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                 url  ...               cat_pertinence\n",
              "0  https://graylinegroup.com/machine-learning-and...  ...  I&G mais pas du meme couple\n",
              "1  https://graylinegroup.com/machine-learning-and...  ...                          I&G\n",
              "\n",
              "[2 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    }
  ]
}