{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Created on Tuesday 12th January 2021 \n",
    "\n",
    "# **POC mesures de pertinence**\n",
    "**Group 2 - Recherche de nouvelles sources**  \n",
    "*Projet Inter-Promo 2021 de la formation SID, Université Paul Sabatier, Toulouse*\n",
    "\n",
    "@authors : Corentin Prat-Marca, Nicolas Enjalbert Courrech, Sonia Bezombe, Flavien Caminade, Audrey Degaugue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un point clef de la veille technologique automatisée est de définir des nouvelles sources répondant au thème de cette veille. \n",
    "Dans le cadre de ce projet, l'identification des nouvelles sources pertinentes s'est faite en plusieurs étapes. La première est de récupérer un nombre important d'articles. Pour cela, une solution utilisant un moteur de recherche et un ensemble d'équation. Ces points sont détaillés dans le POC ou rapport dédié. Le résultat de cette étape-ci est un ensemble d'articles répondant plus ou moins aux requêtes.  \n",
    "L'étape suivante permet de mesurer la pertinence d'un article et par agrégation d'un site entier. Le but de cette étape est de définir si un site est qualifié de pertinent et le positionner dans la liste des sites à surveiller (ici en les scrappant) tous les jours.\n",
    "Ce document décrit des mesures qui ont été pensées durant le projet. \n",
    "\n",
    "Ces mesures sont calculées à partir d'un data frame obtenu après les résultats de moteur de recherche contenant :  \n",
    "  * art_url : l'url de l'article\n",
    "  * src_name : le nom du site\n",
    "  * src_url : l'url du site (page d'accueil)\n",
    "  * query : mots clefs utilisés pour effectuer la recherche\n",
    "  * title : titre proposé par le moteur de recherche\n",
    "  * snippet : résumé proposé par le moteur de recherche\n",
    "  * rank : position de l'article dans le résultat du moteur de recherche\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mesure de popularité"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La première mesure mise en place est très naïve : elle mesure la popularité d'un site, c'est-à-dire, le nombre de fois que le site est appartu à partir d'un ensemble de requêtes. \n",
    "Un simple *count* répond à cette question. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def countSite(df : pd.DataFrame):\n",
    "  # Count the number of URLs crawled per site\n",
    "  return df.groupby('src_name').count().reset_index()\n",
    "\n",
    "def popularite(df : pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Calcule the popularity score per site\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : DataFrame\n",
    "        Contains : art_url, src_name, src_url.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    DataFrame\n",
    "        Contains : src_name, popularity score.\n",
    "\n",
    "    \"\"\"\n",
    "    df1 = countSite(df)\n",
    "    df1[\"popularity\"] = [x/np.max(df1.art_url) for x in df1.art_url]\n",
    "    df_final = df1[['src_name','popularity']]\n",
    "    return df_final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cette mesure ne répond pas complètement à la question de pertinence. Les moteurs de recherche retournent souvent en priorité une liste de sites populaires, car très utilisés par les utilisateurs des moteurs de recherche, comme des réseaux sociaux, dictionnaires, traducteurs ou encyclopédies. Ces sites nous ont été retournés lors de notre première mise en place de récupération des résultats de moteurs de recherche. \n",
    "\n",
    "Cet ensemble de sites ne répondent pas à notre recherche d'articles innovants et très actuels. De plus, il ne prend pas en compte de site moins populaire mais contenant des articles très pertinents. Cette solution naïve n'est donc pas judicieuse. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Mesure du rang\n",
    "\n",
    "Cette mesure s'obtient en se basant sur l'ordonnancement des articles par le moteur de recherche. Intuitivement, un article étant classé en première position répond mieux à la requête de recherche qu'un article étant placé en dernière position. \n",
    "\n",
    "La fonction _score_rank_ permet de mettre en valeur le rang de position en attribuant un score élevé à l'article étant au premier rang et en agrégeant par la moyenne des scores par sites. \n",
    "\n",
    "$scoreArticle_i = 1 - \\frac{rang_i}{card(R)} $  avec R l'ensemble des résultats de la requête donnant l'article i. \n",
    "\n",
    "La fonction d'agrégation est la moyenne de ces scores groupés par sites $\\frac{1}{card(c)} \\sum_{c \\in C} scoreArticle_c $\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_rank(df : pd.DataFrame) :\n",
    "    \"\"\" Documentation\n",
    "\n",
    "    Calculate the score rank per site based on the position of the item in the\n",
    "    crawl \n",
    "\n",
    "    Parameters :\n",
    "        df : dataframe processed\n",
    "    Out :\n",
    "        df_result_rank : dataframe containing the score rank per site\n",
    "    \"\"\"\n",
    "\n",
    "    for i in range(df.shape[0]):\n",
    "        df['score_rank'] = 1/((df['position']/df[df['query'] == df['query'].iloc[i]].shape[0])+1) # the rank score is calculated based on the position of the item in the crawl \n",
    "        df_result_rank = df.groupby('src_name')['score_rank'].mean() # we do a groupby to get the average of this score per site,\n",
    "    return (df_result_rank)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ce score est aussi biaisé par les sites populaires qui sont placés souvent en première position. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mesure du contenu de la requête dans le titre et le résumé\n",
    "\n",
    "Cette mesure vise à savoir si les mots de la requête se retrouvent en leur totalité ou non dans le titre et le résumé proposés par le moteur de recherche. \n",
    "Cette mesure propose un score compris entre 0 et 1 pour chaque site."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def common_query_words(df : pd.DataFrame) :\n",
    "    \"\"\" Documentation\n",
    "\n",
    "    Calculate the relevance of the queries\n",
    "\n",
    "    Parameters :\n",
    "        df : dataframe processed\n",
    "    Out :\n",
    "        df : dataframe containing relevance score and categorie\n",
    "    \"\"\"\n",
    "\n",
    "  df = transform_data(df)\n",
    "  df['cat_pertinence'] = ''\n",
    "  df['common_words'] = ''\n",
    "  df['relevance_query']=0.00\n",
    "  relevance_query : list = [] #list that will store the relevance score,\n",
    "    #pertinence_j = [] #list that will store the number of the line concerned\n",
    "  df_relevance = pd.DataFrame(columns=['nb_row','score']).set_index('nb_row') #creation of a df allowing to have the score for each line,\n",
    "  for j in range (len(df)): # for each line of the df,\n",
    "    innov : int = 0 #initialization of a variable to count the number of words in the innovation lexicon \n",
    "    gest : int = 0 #...the same for the management lexicon\n",
    "    separator_or : list = list(df['query'][j].split(' or ')) #we store all pairs of the request in a separator_or list \n",
    "    relevance_listing : list = [] #list to store the relevance scores for each of the couples\n",
    "    list_find : list = [] #list to store words in common for each line,\n",
    "    for k in range (len(separator_or)): #for each of these couples,\n",
    "      nb_present : int = 0 #count the number of words in common\n",
    "      separator_and : list = list(separator_or[k].split(' and ')) #we store all the words of the couple of the query in a separator_and list \n",
    "      for i in range (len(separator_and)): #for all the words in the query,\n",
    "        if (df['title'][j].find(separator_and[i]) != -1) : #we look for it in the title, and if it's there... \n",
    "          if (i == 0): #if it's the first member of the couple, we've found a word of innovation \n",
    "            innov += 1\n",
    "          else : #If not, a word of management \n",
    "            gest += 1\n",
    "          nb_present = nb_present + 1 #increments the number of words in the query found.  \n",
    "          list_find.append(separator_and[i]+\";\")\n",
    "        if (df['resume'][j].find(separator_and[i]) != -1) : #then we look for it in the summary, and if it's there... \n",
    "          if (i == 0):\n",
    "            innov += 1\n",
    "          else :\n",
    "            gest += 1\n",
    "          nb_present += 1\n",
    "          list_find.append(separator_and[i] +\";\") #... we put it in a list\n",
    "          list_find = list(set(list_find))\n",
    "      relevance_query : float = (nb_present / len(separator_and))*100 #we calculate the relevance score nb of words found / nb of total words in the query\n",
    "      relevance_listing.append(relevance_query)\n",
    "      str = ' '.join(list_find) #...\n",
    "    df['common_words'][j]=str[:-1] #...we add the words found in the df\n",
    "    df_relevance = df_relevance.append({'nb_row': j}, ignore_index=True)\n",
    "    df_relevance['score'][j] = max(relevance_listing) #for each line, we take the best relevance of a couple,\n",
    "    df['relevance_query'][j] = df_relevance['score'][j]\n",
    "    nb = df_relevance['score'][j]\n",
    "    #and we categorize according to the score obtained :\n",
    "    if (nb>=100) : #all the words of at least one couple are found,\n",
    "      df['cat_pertinence'][j] = 'I&G'\n",
    "    else :\n",
    "      if (innov >= 1 and gest >= 1) : #words of innovation and gestion are found but not in the same couple,\n",
    "        df['cat_pertinence'][j] = 'I&G but not from the same couple'\n",
    "      elif (innov >= 1 and gest == 0) : #at least one word of innovation is found, but no gestion,\n",
    "        df['cat_pertinence'][j] = 'I'\n",
    "      elif (innov == 0 and gest >= 1): #at least one word of management is found but no innovation, \n",
    "        df['cat_pertinence'][j] = 'G'\n",
    "      else :\n",
    "        df['cat_pertinence'][j] = 'None' #no word is found \n",
    "  return(df)\n",
    "\n",
    "def result_score_def (df : pd.DataFrame):\n",
    "    \"\"\" Documentation\n",
    "\n",
    "    Calculate the relevance of a query per site\n",
    "\n",
    "    Parameters :\n",
    "        df : dataframe processed\n",
    "    Out :\n",
    "        df_result_score : dataframe containing the relevance of a query per\n",
    "        site\n",
    "    \"\"\"\n",
    "\n",
    "    df_relevance_query = common_query_words(df) # record our relevance scores obtained using common words\n",
    "    df_relevance_query['src_name'] = ''\n",
    "    for i in range (len(df_relevance_query)) :\n",
    "        df_relevance_query['src_name'][i] = site_name(df_relevance_query['art_url'][i]) # add the name of the site in the dataframe according to the url \n",
    "    df_result_score = df_relevance_query.groupby('src_name')['relevance_query'].mean() # we make a groupby to have the average score according to the request per site \n",
    "    return (df_result_score) # return the new dataframe  \n",
    "\n",
    "def relevance_query (df : pd.DataFrame) :\n",
    "    \"\"\" Documentation\n",
    "\n",
    "    Calculate the standardized relevance of a query per site\n",
    "\n",
    "    Parameters :\n",
    "        df : dataframe processed\n",
    "    Out :\n",
    "        df_result_score : dataframe containing the relevance of a query per\n",
    "        site\n",
    "    \"\"\"\n",
    "\n",
    "    df_result_score = result_score_def(df) # we record our relevance scores grouped by site, \n",
    "    df_result_score = df_result_score.reset_index()\n",
    "    df_result_score['relevance_query'] = df_result_score['relevance_query']/(max(df_result_score['relevance_query'])) # they are divided by the maximum to get a relevance score between 0 and 1,\n",
    "    return (df_result_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cette mesure prend en compte les mots de la requête étant dans le titre et le résumé. Dans notre cas, le moteur de recherche Google, choisi pour ce projet, utilise automatiquement aussi des synonymes des mots contenus dans la requête textuelle. Cette approche prend en compte des formes réduites des mots pouvant ainsi prendre en compte toutes les formes de notre recherche textuelle. \n",
    "\n",
    "Néanmoins, un synonyme ne répond pas à ces formes réduites puisque différents par leur racine. Cette mesure ne prend donc pas en compte un potentiel article très pertinent mais ne répondant qu'à des synonymes de la requête textuelle utilisée. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mesure : présence des mots du lexique dans l'article.\n",
    "\n",
    "Une première approche pour répondre à la problématique des synonymes est d'utiliser l'ensemble des mots clefs des lexiques proposés en entrée des requêtes textuelles. \n",
    "\n",
    "Les fonctions suivantes permettent de calculer ce score-ci.\n",
    "\n",
    "On cherche à attribuer un score à des articles scrappés en regardant combien on trouve de mots de chaque lexique. On choisit sur quelles parties on se base (nom de l'article, résumé, contenu, etc), et on calcule un score dépendant du nombre de mots des lexiques (plus il y a de mots mieux c'est, si un article comporte 0 mots d'un lexique on pénalise) de la différence des nombres de mots de chaque lexique (on veut à peu près autant de mots de chaque lexique), et de la taille de l'article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ubl2-Z5haKdH",
    "outputId": "b295acd6-69aa-4838-cbe0-1e6e89c48286"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/cmisid/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk   #the importations for cleandesc\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import LancasterStemmer\n",
    "from nltk.stem import SnowballStemmer \n",
    "nltk.download('stopwords')\n",
    "stop_words=nltk.corpus.stopwords.words('french') #we're working on french articles\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "def cleandesc(desc : str): #function to remove stopwords and suffixes\n",
    "    \"\"\"Documentation\n",
    "    Function took from @SoniaBezombes\n",
    "    \"\"\"\n",
    "    sent = desc\n",
    "    sent = \"\".join([x.lower() if x.isalpha()  else \" \" for x in sent])\n",
    "    Porter=SnowballStemmer('french')\n",
    "    sent = \" \".join([Porter.stem(x) if x.lower() not in stop_words  else \"\" for x in sent.split()])\n",
    "    sent = \" \".join(sent.split())\n",
    "    return sent\n",
    "\n",
    "def calcul_score(texte : str,lexique : list): \n",
    "    #function to count the number of words of a list in a text\n",
    "    \"\"\"Documentation\n",
    "    Parameters:\n",
    "        texte : the text we're working on\n",
    "        lexique : the list of words we're looking for\n",
    "    Out :\n",
    "        somme : the number of lexique's words we found in texte\n",
    "    \"\"\"\n",
    "    somme : int = 0\n",
    "    for mot in lexique:\n",
    "        somme += texte.count(' '+mot+' ')\n",
    "    return somme\n",
    "\n",
    "\n",
    "#before using pertinence 4, we have to clean the columns we're gonna work on like that :\n",
    "#df_scrapped_articles['column_name']= [cleandesc(x.column_name) for x in df_scrapped_articles.itertuples()]\n",
    "#cleaning art_content all the time, just once\n",
    "#df_scrapped_articles['art_content']= [cleandesc(x.art_content) for x in df_work.itertuples()]\n",
    "\n",
    "def pertinence_4(df : pd.DataFrame , list_columns : list, \n",
    "                 lexiqueI : list, lexiqueG : list):\n",
    "    \"\"\"Documentation\n",
    "    Parameters:\n",
    "        df : a data frame of scrapped articles. It must contains \n",
    "        a \"art_content\" columns to determine its size\n",
    "        \n",
    "        list_columns : the list of columns we want to count \n",
    "        the words of the two lexiques on. \n",
    "        \n",
    "        lexiqueI : the innovation lexique, it has to be a list\n",
    "        lexiqueG : the gestion lexique, it has to be a list\n",
    "    Out :\n",
    "        df : the same data but with the scores and the step to have it\n",
    "    \"\"\"\n",
    "    list_nom_colonne_I : list = [] #the list of the names of the innovation's score columns \n",
    "    list_nom_colonne_G : list = [] #the list of the names of the gestion's score columns \n",
    "    cleanedG : list = [cleandesc(mot) for mot in lexiqueG] #cleaning the gestion's lexique\n",
    "    cleanedI : list = [cleandesc(mot) for mot in lexiqueI] #cleaning the gestion's lexique\n",
    "\n",
    "    for columns in list_columns:\n",
    "\n",
    "        nomI : str ='score_innovation_'+columns #the name of the new columns : score_innovation_column_name or score_gestion_column_name\n",
    "        nomG : str ='score_gestion_'+columns\n",
    "        df[nomI]=[calcul_score(df.loc[i][columns], cleanedI) for i in range(len(df))] #creating the new columns using calcul_score, counting the number of words\n",
    "        df[nomG]=[calcul_score(df.loc[i][columns], cleanedG) for i in range(len(df))]\n",
    "        df['score_'+columns]=[df.loc[i][nomI]+df.loc[i][nomG] for i in range(len(df))] #new column, wich is the sum of gestion and innovation scores\n",
    "        list_nom_colonne_I.append(nomI)\n",
    "        list_nom_colonne_G.append(nomG)\n",
    "    \n",
    "    #length of the cleaned content of the article\n",
    "    df['taille_content'] = [len(df['art_content'][i]) for i in range(len(df))] \n",
    "    #sum of all the innovation's scores\n",
    "    df['score_I'] = df[list_nom_colonne_I].sum(axis = 1) \n",
    "    #sum of all the gestion's scores\n",
    "    df['score_G'] = df[list_nom_colonne_G].sum(axis = 1) \n",
    "    #absolute value of the difference of the number of gestion's words and innovation's words\n",
    "    df['score_abs'] = abs(df['score_I']-df['score_G'])\n",
    "    #finding if there is 0 gestion's words or 0 innovation's words, \n",
    "    #to penalise the articles which don't speak of innovation or gestion\n",
    "    df['detct_0'] = [0 if (x.score_I == 0) or (x.score_G == 0) else 1 for x in df.itertuples()] \n",
    "    \n",
    "    df['score_total'] = [(1.09*(x.score_I+x.score_G)-abs(x.score_I-x.score_G))*100/x.taille_content \n",
    "                         if x.detct_0 == 0 \n",
    "                         else (1.09*(x.score_I+x.score_G)-abs(x.score_I-x.score_G))*1000/\\\n",
    "                         x.taille_content \n",
    "                         for x in df.itertuples()]\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XR9SbuETaJ5z"
   },
   "source": [
    "Obtenant ainsi un score par article, la fonction d'agrégation utilisée est la moyenne des scores par sites webs.\n",
    "Pour cela on utilise une fonction qui, à partir d'un url d'article, trouve l'url du site source. Du coup, on se retrouve avec linkedin.com =/= fr.linkedin, mais 2 sites différents avec le même nom (site.fr, www.site.com, etc) sont bien différenciés. Cela nous permet aussi de faire la différence entre la version française et anglaise d'un site."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def url_site(url_art : str):\n",
    "    \"\"\"Documentation\n",
    "    @Flavien Caminade et Corentin Prat-Marca\n",
    "    Parameters:\n",
    "        url_art : the url of an article\n",
    "    Out :\n",
    "        url_site : the url of a site obtained with the article's url\n",
    "    \"\"\"\n",
    "    site : list  =url_art.split(\"://\")\n",
    "    #keep everything on the right side of \"https://\"\"\n",
    "    if site[0]==\"https\" or site[0]==\"http\":\n",
    "        url_site : str = site[1]\n",
    "    else:\n",
    "        url_site : str = site[0]\n",
    "    #delete everything after the first \"/\"\n",
    "    tab : list = url_site.split(\"/\")\n",
    "    url_site = tab[0]\n",
    "    return url_site\n",
    "\n",
    "def pertinence_4_source(df_pertinence_4 : pd.DataFrame):\n",
    "    \"\"\"Documentation\n",
    "    Parameters:\n",
    "        df_pertinence_4 : a data frame of scrapped articles with their score\n",
    "    Out :\n",
    "        df : a data frame with the score for each sites\n",
    "    \"\"\"\n",
    "    dico : dict ={} #dictionnary : keys=sources url, values=sum of the scores of each articles\n",
    "    for articles in df_calcul_global.itertuples():\n",
    "\n",
    "        if url_site(articles.art_url) in dico: #if the source il already in the dictionnary\n",
    "            dico[url_site(articles.art_url)][0] += (articles.score_total)**2 #using square so higher score have more values\n",
    "            dico[url_site(articles.art_url)][1] += 1 #increasing the number of articles for the source\n",
    "        else :\n",
    "            dico[url_site(articles.art_url)] = [(articles.score_total)**2, 1]\n",
    "\n",
    "    df : pd.DataFrame = pd.DataFrame.from_dict(dico, orient = 'index', columns = ['score_total', 'nb_articles'])\n",
    "    df['score_moyen'] = df.apply(lambda row: row.score_total / row.nb_articles, axis = 1) #mean of the scores of the artciles for each sources\n",
    "    return df\n",
    "#pertinence_4_source(df_pertinence_4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cette mesure apporte une meilleure pertinence si elle est utilisée sur le contenu de l'article. Or cette solution fait appel à l'obligation de scrapper les articles pour récupérer le contenu. \n",
    "Or, en suivant les problématiques des scrapeurs (cf documents spécifiques) on se rend compte que le scrapeur générique est nécessaire pour récupérer le contenu des articles. Cette hypothèse forte rend l'application de cette mesure difficile à mettre en place. \n",
    "\n",
    "Néanmoins, en partant du principe que le scrapeur générique est fonctionnel et performant, cette mesure de pertinence permet de prendre en compte les synonymes soulevés dans la mesure précédente. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1MV02FFUaLhU"
   },
   "source": [
    "## Mesure : utilisation des résultats de modélisation en classification (résultat du groupe 5)\n",
    "\n",
    "Une autre solution pour prendre en compte les synonymes et bien définir le sujet des articles est d'utiliser les résultats du groupe 5 du projet. Ce groupe a pour objectif de faire une classification et définir si un article parle d'innovation, de gestion ou d'un autre thème. Deux de leur résultat sont de donner la probabilité qu'un article parle d'innovation et une probabilité qu'un article parle de gestion. \n",
    "\n",
    "Le data frame d'entrée de ces fonctions sont : \n",
    "  * art_url : l'url de l'article\n",
    "  * src_name : le nom du site web\n",
    "  * src_url : l'url du site web correspondant à sa page d'accueil\n",
    "  * probaInnovation : la probabilité que l'article parle d'innovation $[0;1]$\n",
    "  * probaGestion : la probabilité que l'article parle de gamme de gestion $[0;1]$\n",
    "  \n",
    "  \n",
    "Pour calculer la probabilité conjointe, nous avons fait plusieurs mesures d'agrégation des probabilité. Le but est de prendre en compte qu'un article parle à la fois d'innovation et de gestion. Ainsi un article qui parle beaucoup de gestion mais pas d'innovation devra être moins bien classé qu'un article parlant moyennement d'innovation et de gestion (ici proba = 0.5 pour gestion et innovation). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p8HthiBDaL7v"
   },
   "source": [
    "Cette fonction permet de créer un score de pertinence à partir d'un data frame d'articles scrappés dont on a les probabilités d'apparitions d'un mot de gestion ou d'innovation (travail du groupe 5).\n",
    "Elle permet d'évaluer les articles pour avoir une idée de la pertinence. Les scores vont de 0 à 1, et prennent en compte la somme des probabilités (plus on a de mots des lexiques, mieux c'est) et la différence des probabilités (les sites avec un nombre à peu près égal de mots de chaque lexique sont plus intéressants).\n",
    "\n",
    "La première famille de fonction est :  \n",
    "\n",
    "$$ score_i = \\frac{\\lambda*(P_i^I + P_i^G)-\\left|{P_i^I-P_i^G}\\right|}{2*\\lambda}, \\forall i \\in A, \\lambda \\in \\{10, 5, 2\\}$$\n",
    "\n",
    "avec A l'ensemble des articles et $P_i^I$ et $P_i^G$ respectivement les probabilités d'Innovation et de Gestion pour l'article $i$\n",
    "\n",
    "La deuxième mesure prise en compte est une mesure se rapprochant de la F1_mesure utilisée pour agréger la précision et le rappel. Cette mesure permet de mettre en avant des probabilités équilibrées. \n",
    "\n",
    "$$ F1\\_score_i = \\frac{2*(P_i^I*P_i^G)}{P_i^I+P_i^G}, \\forall i \\in A$$ avec A l'ensemble des articles et $P_i^I$ et $P_i^G$ respectivement les probabilités d'Innovation et de Gestion pour l'article $i$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "4iI0al5XWAz_"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>probaInnovation</th>\n",
       "      <th>probaGestion</th>\n",
       "      <th>pertinence_10</th>\n",
       "      <th>pertinence_5</th>\n",
       "      <th>pertinence_2</th>\n",
       "      <th>pertinence_F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.99</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.9900</td>\n",
       "      <td>0.990</td>\n",
       "      <td>0.9900</td>\n",
       "      <td>0.990000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.80</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.8855</td>\n",
       "      <td>0.876</td>\n",
       "      <td>0.8475</td>\n",
       "      <td>0.884916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.99</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.8855</td>\n",
       "      <td>0.876</td>\n",
       "      <td>0.8475</td>\n",
       "      <td>0.884916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.80</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.8000</td>\n",
       "      <td>0.800</td>\n",
       "      <td>0.8000</td>\n",
       "      <td>0.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.99</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.7205</td>\n",
       "      <td>0.696</td>\n",
       "      <td>0.6225</td>\n",
       "      <td>0.664430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.50</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.7205</td>\n",
       "      <td>0.696</td>\n",
       "      <td>0.6225</td>\n",
       "      <td>0.664430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.80</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.6350</td>\n",
       "      <td>0.620</td>\n",
       "      <td>0.5750</td>\n",
       "      <td>0.615385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.50</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.6350</td>\n",
       "      <td>0.620</td>\n",
       "      <td>0.5750</td>\n",
       "      <td>0.615385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.20</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.5555</td>\n",
       "      <td>0.516</td>\n",
       "      <td>0.3975</td>\n",
       "      <td>0.332773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.99</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.5555</td>\n",
       "      <td>0.516</td>\n",
       "      <td>0.3975</td>\n",
       "      <td>0.332773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.20</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.4700</td>\n",
       "      <td>0.440</td>\n",
       "      <td>0.3500</td>\n",
       "      <td>0.320000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.80</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.4700</td>\n",
       "      <td>0.440</td>\n",
       "      <td>0.3500</td>\n",
       "      <td>0.320000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.20</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.3350</td>\n",
       "      <td>0.320</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.285714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.50</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.3350</td>\n",
       "      <td>0.320</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.285714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.20</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.2000</td>\n",
       "      <td>0.200</td>\n",
       "      <td>0.2000</td>\n",
       "      <td>0.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.01</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.4510</td>\n",
       "      <td>0.402</td>\n",
       "      <td>0.2550</td>\n",
       "      <td>0.019800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.99</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.4510</td>\n",
       "      <td>0.402</td>\n",
       "      <td>0.2550</td>\n",
       "      <td>0.019800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.01</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.3655</td>\n",
       "      <td>0.326</td>\n",
       "      <td>0.2075</td>\n",
       "      <td>0.019753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.80</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.3655</td>\n",
       "      <td>0.326</td>\n",
       "      <td>0.2075</td>\n",
       "      <td>0.019753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.01</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.2305</td>\n",
       "      <td>0.206</td>\n",
       "      <td>0.1325</td>\n",
       "      <td>0.019608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.50</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.2305</td>\n",
       "      <td>0.206</td>\n",
       "      <td>0.1325</td>\n",
       "      <td>0.019608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.01</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.0955</td>\n",
       "      <td>0.086</td>\n",
       "      <td>0.0575</td>\n",
       "      <td>0.019048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.20</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.0955</td>\n",
       "      <td>0.086</td>\n",
       "      <td>0.0575</td>\n",
       "      <td>0.019048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.010000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    probaInnovation  probaGestion  pertinence_10  pertinence_5  pertinence_2  \\\n",
       "0              0.99          0.99         0.9900         0.990        0.9900   \n",
       "1              0.80          0.99         0.8855         0.876        0.8475   \n",
       "5              0.99          0.80         0.8855         0.876        0.8475   \n",
       "6              0.80          0.80         0.8000         0.800        0.8000   \n",
       "10             0.99          0.50         0.7205         0.696        0.6225   \n",
       "2              0.50          0.99         0.7205         0.696        0.6225   \n",
       "11             0.80          0.50         0.6350         0.620        0.5750   \n",
       "7              0.50          0.80         0.6350         0.620        0.5750   \n",
       "12             0.50          0.50         0.5000         0.500        0.5000   \n",
       "3              0.20          0.99         0.5555         0.516        0.3975   \n",
       "15             0.99          0.20         0.5555         0.516        0.3975   \n",
       "8              0.20          0.80         0.4700         0.440        0.3500   \n",
       "16             0.80          0.20         0.4700         0.440        0.3500   \n",
       "13             0.20          0.50         0.3350         0.320        0.2750   \n",
       "17             0.50          0.20         0.3350         0.320        0.2750   \n",
       "18             0.20          0.20         0.2000         0.200        0.2000   \n",
       "4              0.01          0.99         0.4510         0.402        0.2550   \n",
       "20             0.99          0.01         0.4510         0.402        0.2550   \n",
       "9              0.01          0.80         0.3655         0.326        0.2075   \n",
       "21             0.80          0.01         0.3655         0.326        0.2075   \n",
       "14             0.01          0.50         0.2305         0.206        0.1325   \n",
       "22             0.50          0.01         0.2305         0.206        0.1325   \n",
       "19             0.01          0.20         0.0955         0.086        0.0575   \n",
       "23             0.20          0.01         0.0955         0.086        0.0575   \n",
       "24             0.01          0.01         0.0100         0.010        0.0100   \n",
       "\n",
       "    pertinence_F1  \n",
       "0        0.990000  \n",
       "1        0.884916  \n",
       "5        0.884916  \n",
       "6        0.800000  \n",
       "10       0.664430  \n",
       "2        0.664430  \n",
       "11       0.615385  \n",
       "7        0.615385  \n",
       "12       0.500000  \n",
       "3        0.332773  \n",
       "15       0.332773  \n",
       "8        0.320000  \n",
       "16       0.320000  \n",
       "13       0.285714  \n",
       "17       0.285714  \n",
       "18       0.200000  \n",
       "4        0.019800  \n",
       "20       0.019800  \n",
       "9        0.019753  \n",
       "21       0.019753  \n",
       "14       0.019608  \n",
       "22       0.019608  \n",
       "19       0.019048  \n",
       "23       0.019048  \n",
       "24       0.010000  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "def pertinence_6(df : pd.DataFrame): #df avec proba innovation et proba gestion\n",
    "    \"\"\"Documentation\n",
    "    Parameters:\n",
    "        df : a data frame of scrapped articles with the probability for each to contain a \"gestion\" word or an \"innovation\" word (group 5 work)\n",
    "    Out :\n",
    "        df : the same data frame with 4 more columns each containing a score based on the probability\n",
    "    \"\"\"\n",
    "    df['pertinence_10'] = df.apply(lambda row: (10*(row.probaInnovation + row.probaGestion) - abs(row.probaInnovation-row.probaGestion))/20, axis = 1) #(10*(motsGestion+motsInnovation)-abs(motsGestion-motsInnovation)) / 20 (pour un score entre 0 et 1)\n",
    "    df['pertinence_5'] = df.apply(lambda row: (5*(row.probaInnovation + row.probaGestion) - abs(row.probaInnovation-row.probaGestion))/10, axis = 1) #(5*(motsGestion+motsInnovation)-abs(motsGestion-motsInnovation)) / 10 (pour un score entre 0 et 1)\n",
    "    df['pertinence_2'] = df.apply(lambda row: (2*(row.probaInnovation + row.probaGestion) - abs(row.probaInnovation-row.probaGestion))/4, axis = 1) #(2*(motsGestion+motsInnovation)-abs(motsGestion-motsInnovation)) / 4 (pour un score entre 0 et 1)\n",
    "    df['pertinence_F1'] = df.apply(lambda row: 2*(row.probaInnovation * row.probaGestion) / (row.probaInnovation+row.probaGestion), axis = 1) #F1 score adapté\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "df = pd.DataFrame({'probaInnovation': [0.99,0.8,0.5, 0.2, 0.01]*5, 'probaGestion': [0.99]*5+[0.8]*5+[0.5]*5+[0.2]*5+[0.01]*5})\n",
    "df1 = pertinence_6(df)\n",
    "df1.sort_values(by='pertinence_F1', axis=0, ascending=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les résultats précédents montrent bien la pertinence de la fonction proche du F1_score. Cette mesure est donc celle à utiliser pour garder des articles qui parlent conjointement d'innovation et de gestion. \n",
    "La famille des mesures permet, quand $\\lambda$ augmentent, de mettre en avant des articles ayant un fort score en innovation et un faible score en gestion (ou inversement). \n",
    "\n",
    "Les deux méthodes peuvent être utilisées dans notre application. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "fnPnsjY7Wbpo"
   },
   "outputs": [],
   "source": [
    "def pertinence_6_source(df_pertinence_6 : pd.DataFrame):\n",
    "    \"\"\"Documentation\n",
    "    Parameters: \n",
    "        df_pertinence_6 : a data frame of scrapped articles with their score, obtained with the function pertinence_6\n",
    "    Out :\n",
    "        df : a data frame of the means of each score for each source\n",
    "    \"\"\"\n",
    "    dico_pertinence : dict = {} #key : a source, values : a list [score_10,score_5,score_2,score_F1,number of articles]\n",
    "    for articles in df_pertinence_6.itertuples():\n",
    "        #if the source arlready is in the dictionnary\n",
    "        if url_site(articles.art_url) in dico_pertinence: \n",
    "            #using square function so that articles with a higher score are more important\n",
    "            dico_pertinence[url_site(articles.art_url)][0] += articles.pertinence_10**2\n",
    "            dico_pertinence[url_site(articles.art_url)][1] += articles.pertinence_5**2\n",
    "            dico_pertinence[url_site(articles.art_url)][2] += articles.pertinence_2**2\n",
    "            dico_pertinence[url_site(articles.art_url)][3] += articles.pertinence_F1**2\n",
    "            #increasing the number or articles\n",
    "            dico_pertinence[url_site(articles.art_url)][4] += 1 \n",
    "        else :\n",
    "            #adding a new source\n",
    "            dico_pertinence[url_site(articles.art_url)]=\n",
    "            [articles.pertinence_10**2,articles.pertinence_5**2,articles.pertinence_2**2,articles.pertinence_F1**2,1]\n",
    "\n",
    "    df : pd.DataFrame = pd.DataFrame.from_dict(dico_pertinence, orient = 'index', columns = ['pertinence_10','pertinence_5','pertinence_2','pertinence_F1', 'nb_articles']) #turning the dictionnary into a data frame\n",
    "    df['pertinence_10_moyenne'] = df.apply(lambda row: row.pertinence_10 / row.nb_articles, axis = 1)\n",
    "    df['pertinence_5_moyenne'] = df.apply(lambda row: row.pertinence_5 / row.nb_articles, axis = 1)\n",
    "    df['pertinence_2_moyenne'] = df.apply(lambda row: row.pertinence_2 / row.nb_articles, axis = 1)\n",
    "    df['pertinence_F1_moyenne'] = df.apply(lambda row: row.pertinence_F1 / row.nb_articles, axis = 1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M--lIncOZs27"
   },
   "source": [
    "La mesure d'agrégation ici est la moyenne par site web. L'utilisation de la fonction carré permet de mettre en avant les articles avec une forte pertinence. \n",
    "\n",
    "Cette mesure bien que très pertinentes dépend de deux hypothèses fortes : \n",
    "  * le scrapeur générique fonctionne et est performant. \n",
    "  * les modèles de classification sont justes.\n",
    "\n",
    "Cette méthode nécessite donc de scraper les articles visés (avec le scrapeur générique). Ils vont par la suite passer le traitement classique de stockage en Base de Données, faire des embedding pour pouvoir donner une probabilité que l'article parle d'innovation et de gestion. \n",
    "Cette méthode peut donc être coûteuse mais permet de donner un bon score de pertinence. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nLeRTUP4oEvU"
   },
   "source": [
    "## Conclusion\n",
    "\n",
    "Les mesures présentées ci-dessus ne sont pas toutes pertinentes indépendamment les unes des autres, mais peuvent être combinées en les agrégeant (par sites) en faisant par exemple une moyenne pondérée afin de mettre en avant les mesures les plus justes (comme la dernière présentée). \n",
    "Dans le cadre de l'automatisation de ce processus seulement les trois premières mesures ont été utilisées.\n",
    "\n",
    "La liste des mesures n'est pas exhaustive et d'autres peuvent être mises en place.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fusion_2(df : pd.DataFrame) :\n",
    "    \"\"\" Documentation\n",
    "\n",
    "    Calculate the relevance for each site crawled\n",
    "\n",
    "    Parameters :\n",
    "        df : dataframe containing the results crawled without the sites we \n",
    "             don't want\n",
    "    Out :\n",
    "        fusion_result : dataframe containing the score for each site crawled\n",
    "    \"\"\"\n",
    "\n",
    "    df_prepare = prepareDF(df)\n",
    "    fusion = pd.merge(popularite(df_prepare), relevance_query(df_prepare), how=\"right\", left_on=\"src_name\", right_on=\"src_name\") #the new relevance score is merged with those obtained previously, \n",
    "    df_result_rank = score_rank(df_prepare)\n",
    "    df_result_rank = df_result_rank.reset_index()\n",
    "    fusion_result = pd.merge(fusion, df_result_rank, how=\"right\", left_on=\"src_name\", right_on=\"src_name\")\n",
    "    fusion_result['score_mean'] = 0.0\n",
    "    for i in range (len(fusion_result)):\n",
    "        fusion_result['score_mean'][i] = fusion_result['popularity'][i]*0.2 + fusion_result['relevance_query'][i]*0.4 + fusion_result['score_rank'][i]*0.4 #These scores are weighted by coefficients applied after reflection and an average relevance score is calculated,\n",
    "    return(fusion_result)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Untitled7.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
