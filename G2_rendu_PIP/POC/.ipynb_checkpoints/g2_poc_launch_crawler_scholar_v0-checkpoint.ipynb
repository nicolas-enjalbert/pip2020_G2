{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5_Wd1pn6rNJv"
   },
   "source": [
    "Created on Thursday 14th January 2021  \n",
    "\n",
    "# **POC Recherche de nouvelles sources pertinentes en utilisant le moteur de recherche Google Scholar**\n",
    "**Group 2 - Recherche de nouvelles sources**  \n",
    "*Projet Inter-Promo 2021 de la formation SID, UniversitÃ© Paul Sabatier, Toulouse*\n",
    "\n",
    "@authors : Flavien Caminade, Audrey Degaugue, Nicolas Enjalbert Courrech"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tOKUtNrYrrvf"
   },
   "source": [
    "**POC : crawler sur google scholar**\n",
    "\n",
    "Cette preuve de concept est l'application directe de la solution d'identification des nouvelles sources pertinentes. Notre solution principale est d'utiliser le moteur de recherche google et de faire des requÃªtes textuelles. \n",
    "\n",
    "Nous avons voulu adapter cette solution au moteur de recherche Google Scholar. Ce moteur de recherche retourne des articles scientifiques de recherches. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6sXkggsPrGyc"
   },
   "outputs": [],
   "source": [
    "########## Module import ##########\n",
    "\n",
    "# Files\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# Maths\n",
    "import random\n",
    "\n",
    "# Extraction\n",
    "import re\n",
    "\n",
    "# Scraping\n",
    "import scrapy\n",
    "from requests import get\n",
    "\n",
    "# Parsing\n",
    "from urllib.parse import urlencode\n",
    "\n",
    "# Format\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "import time\n",
    "\n",
    "# Parameters to change \n",
    "from Parameters import *\n",
    "\n",
    "    \n",
    "########## Folders ##########\n",
    "\n",
    "# General directory\n",
    "global path_general\n",
    "path_general = 'C:/Users/flavien/Documents/SID/L3Sid/pip/'\n",
    "# Directory to store crawled URLs\n",
    "global path_links_crawler\n",
    "path_links_crawler = path_general\n",
    "# Directory to store partial backups\n",
    "global path_backup\n",
    "path_backup = path_general\n",
    "# Directory to load files :\n",
    "# Lexique_Gammes_Gestion, Lexique_Innovation, listCouple\n",
    "global path_files\n",
    "path_files = path_general\n",
    "\n",
    "########## Parameters ##########\n",
    "\n",
    "# API Key (created on Scraper API)\n",
    "API_KEY = open(path_files+'API_key.txt', 'r').read()\n",
    "\n",
    "\n",
    "def create_google_url(query, nb_results):\n",
    "    \"\"\"\n",
    "    Allows you to create a Google URL from a keyword\n",
    "\n",
    "    Parameter :\n",
    "        query : keyword to enter in the search bar\n",
    "\n",
    "    Out :\n",
    "        google_url : google URL created from the keyword\n",
    "    \"\"\"\n",
    "    # num = number of results to be scraped\n",
    "    google_dict = {'q': query, 'num': nb_results, }\n",
    "    google_url = 'http://scholar.google.com/scholar?' + urlencode(google_dict)\n",
    "    return google_url\n",
    "\n",
    "\n",
    "def combAND(couple):\n",
    "    \"\"\"Documentation\n",
    "    Parameters:\n",
    "        couple: a list of 2 Strings\n",
    "    Out :\n",
    "        list : a combination of the 2 members of a couple with AND between them\n",
    "    \"\"\"\n",
    "\n",
    "    return str(couple[0])+' '+'AND'+' '+str(couple[1])\n",
    "\n",
    "\n",
    "def listToAND(listCouple):\n",
    "    \"\"\"Documentation\n",
    "    Parameters:\n",
    "        listCouple: a list of couple\n",
    "\n",
    "    Out :\n",
    "        list : a list of the combination of the couple of listCouple\n",
    "    \"\"\"\n",
    "\n",
    "    # We use combAND\n",
    "    return [combAND(i) for i in listCouple]\n",
    "\n",
    "\n",
    "def combOR(tuple):\n",
    "    \"\"\"Documentation\n",
    "\n",
    "    Parameters:\n",
    "        tuple: a list of String\n",
    "\n",
    "    Out :\n",
    "        final : a combination of the members of the tuple with OR between them\n",
    "                and framed with ()\n",
    "    \"\"\"\n",
    "\n",
    "    # First step : initialisation of final\n",
    "    final = '('+str(tuple[0])+')'\n",
    "    # Second step : adding the rest of the tuple\n",
    "    for i in range(1, len(tuple)):\n",
    "        final = final+'|'+'('+tuple[i]+')'\n",
    "    return final\n",
    "\n",
    "\n",
    "# Applying the previous function to a list of tuple\n",
    "def listToOR(listTuple):\n",
    "    \"\"\"Documentation\n",
    "    Parameters:\n",
    "        listTuple: a list of tuples\n",
    "\n",
    "    Out :\n",
    "        list : a list of the combination of the tuple of listTuple\n",
    "    \"\"\"\n",
    "\n",
    "    # We use combOR\n",
    "    return [combOR(i) for i in listTuple]\n",
    "\n",
    "\n",
    "# Making a list of random tuples\n",
    "# We have to limit the number of request, by default 1000, and we make couples\n",
    "def listComb(listAND, numbT=2, iteration=int(1000)):\n",
    "    \"\"\"Documentation\n",
    "\n",
    "    Parameters:\n",
    "        listAND: a list Strings with AND\n",
    "        numbT : the length of the tuple we want to create\n",
    "        iteration : the maximum number of combination we want to create\n",
    "\n",
    "    Out :\n",
    "        finalList : a list of the combination of the tuple of listTuple\n",
    "    \"\"\"\n",
    "\n",
    "    finalList = []\n",
    "    i = 0\n",
    "    # Step 1 : we loop until we have enough tuples or the list is empty\n",
    "    while ((len(listAND) >= numbT) and (i < iteration)):\n",
    "        i += 1\n",
    "        # Step 2 : at each loop, we take some random elements of listAND and\n",
    "        # create a tuple with them\n",
    "        listRand = random.sample(listAND, numbT)\n",
    "        # Step 3 : we remove the elements from listAND\n",
    "        for j in listRand:\n",
    "            listAND.remove(j)\n",
    "        # Step 4 :we add the tuple we created to our finalList\n",
    "        finalList.append(listRand)\n",
    "    return finalList\n",
    "\n",
    "\n",
    "def link_filter_date(link, date_filter):\n",
    "    \"\"\"Documentation\n",
    "    Function that allows to add a \"limit date\" parameter in the link\n",
    "\n",
    "    Parameters:\n",
    "        link : URL to which the date filter should be added\n",
    "        date_limit : Limit date\n",
    "\n",
    "    Out :\n",
    "        link_new : URL with a date filter\n",
    "    \"\"\"\n",
    "\n",
    "    date_limit = datetime.strptime(date_filter, '%Y-%m-%d')\n",
    "    # Crawling 1 day before last crawling date\n",
    "    days_to_substract = timedelta(days=1)\n",
    "    \n",
    "    # Limit date\n",
    "    date_limit = date_limit - days_to_substract\n",
    "    jour = str(date_limit.day)\n",
    "    mois = str(date_limit.month)\n",
    "    annee = str(date_limit.year)\n",
    "\n",
    "    link_new = link+\"&source=lnt&tbs=cdr%3A1%2Ccd_min%3A\"+mois+\"%2F\"+jour+\"%2F\"+annee+\"%2Ccd_max%3A&tbm=\"\n",
    "    return link_new\n",
    "\n",
    "\n",
    "def get_url(url, date_filter):\n",
    "    \"\"\"\n",
    "    Creation of the URL that will allow the legal scraping of Google results\n",
    "    (use of the API key). This URL is equivalent to a Google search.\n",
    "\n",
    "    Parameter :\n",
    "        url : google URL created from the keyword\n",
    "\n",
    "    Out :\n",
    "        proxy_url : URLs built using the API\n",
    "    \"\"\"\n",
    "\n",
    "    payload = {'api_key': API_KEY,\n",
    "               'url': url,\n",
    "               'autoparse': 'true',\n",
    "               'lr': 'fr'#,\n",
    "               # Depersonalisation of results\n",
    "#               'pws': 0\n",
    "    }\n",
    "    proxy_url = 'http://api.scraperapi.com/?' + urlencode(payload)\n",
    "    date_url = link_filter_date(proxy_url, date_filter)+ \"&scisbd=1\"\n",
    "    return date_url\n",
    "\n",
    "\n",
    "class GoogleSpider(scrapy.Spider):\n",
    "    \"\"\"\n",
    "    This class lists functions for scraping Google results from a list of keywords\n",
    "    \"\"\"\n",
    "\n",
    "    # GoogleSpider class name\n",
    "    name = 'google'\n",
    "    # Name of the site to be scraped\n",
    "    allowed_domains = ['scholar.google.com']\n",
    "    # Settings\n",
    "    custom_settings = {\n",
    "                        # Criticality level at which the log is displayed\n",
    "                        'LOG_LEVEL': 'INFO',\n",
    "                        # Maximum number of simultaneous requests\n",
    "                        # 'CONCURRENT_REQUESTS_PER_DOMAIN': 1,\n",
    "                        # 'CONCURRENT_REQUESTS': 2,\n",
    "                        # 'CONCURRENT_ITEMS': 200,\n",
    "                        # Maximum number of retries to be made if the query fails\n",
    "                        'RETRY_TIMES': 0}\n",
    "\n",
    "    def start_requests(self, listCouple, length, requestNumber, date_filter, nb_results):\n",
    "        # Initialisation of DataFrame\n",
    "        df = pd.DataFrame(columns=['URL', 'Query'])\n",
    "        # Format changeover\n",
    "        lWork = listToAND(listCouple)\n",
    "        # Selection of queries\n",
    "        lWork = listComb(lWork, numbT=length, iteration=requestNumber)\n",
    "        # We change the format of Keywords\n",
    "        lWork = listToOR(lWork)\n",
    "\n",
    "        lURL = []\n",
    "        # We loop the keywords to generate the queries\n",
    "        print(lWork)\n",
    "        for query in lWork:\n",
    "            url = create_google_url(query, nb_results)\n",
    "            lURL.append(str(scrapy.Request(get_url(url, date_filter),\n",
    "                                           meta={'pos': 0}))[5:-1])\n",
    "        # Column generation\n",
    "        df['Query'] = lWork\n",
    "        df['URL'] = lURL\n",
    "\n",
    "        yield df\n",
    "\n",
    "\n",
    "# Launch crawling\n",
    "def Launch_Crawler():\n",
    "\n",
    "    ########## Load files ##########\n",
    "\n",
    "    # Last crawling date\n",
    "    p_date = open(path_files+'date_last_crawling.txt', 'r').read()\n",
    "    \n",
    "    #Lexique_Gammes_Gestion = open(path_files+'Lexique_Gammes_Gestion.txt', 'r').read()\n",
    "    #Lexique_Innovation = open(path_files+'Lexique_Innovation.txt', 'r').read()\n",
    "    \n",
    "    # List of keyword pairs\n",
    "    df = pd.read_json(path_files+'listCouple.json', orient='index')\n",
    "    df_t = df.T\n",
    "    listCouple = df_t.values.tolist()\n",
    "    \n",
    "    # Test on a few couples\n",
    "    p_listCouple = listCouple[2:4]\n",
    "    \n",
    "    \n",
    "    ########## Start building URLs ##########\n",
    "    \n",
    "    df_result = list(GoogleSpider().start_requests(p_listCouple, p_length, p_requestNumber, p_date, p_nb_results))[0]\n",
    "    \n",
    "    ########## Crawling of Google results ##########\n",
    "    \n",
    "    # Crawling start time\n",
    "    print(\"Crawling start time : \"+datetime.now().strftime(\"%H:%M:%S\"))\n",
    "    \n",
    "    list_source = []\n",
    "    i = 0\n",
    "    \n",
    "    for index, row in df_result.iterrows():\n",
    "        link = row['URL']\n",
    "        query = row['Query']\n",
    "        # 1 minute break to avoid API overloading\n",
    "        time.sleep(60)\n",
    "        # URL scraping\n",
    "        response = get(link)\n",
    "    \n",
    "        # Test if the request was successful\n",
    "        if response.status_code == 200:\n",
    "            # Addition of the scraped google results and the corresponding query\n",
    "            list_source.append([response.text, query])\n",
    "    \n",
    "            i += 1\n",
    "            # Saving the results every 20 queries\n",
    "            if (i % 20 == 0):\n",
    "                with open(path_backup+'etape'+str(i)+'.json', 'w') as jsonfile:\n",
    "                    json.dump(list_source, jsonfile)\n",
    "\n",
    "\n",
    "    # Crawling end time\n",
    "    print(\"Crawling end time : \"+datetime.now().strftime(\"%H:%M:%S\"))\n",
    "    date_crawling = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "    \n",
    "    \n",
    "    ########## Processing of results ##########\n",
    "    \n",
    "    for source in list_source:\n",
    "        \n",
    "        source=source.replace(\"<b>\", \"\")\n",
    "        source=source.replace(\"</b>\", \"\")\n",
    "                \n",
    "        list_texte=source.split(\"</a></h3><div class=\\\"gs_a\\\">\")\n",
    "        for texte in list_texte:\n",
    "            \n",
    "            list_texte2=texte.split(\"<div class=\\\"gs_rs\\\">\")\n",
    "            \n",
    "            for texte2 in list_texte2:\n",
    "                \n",
    "                list_texte3=texte2.split(\">\")\n",
    "                l= len(list_texte3)-1\n",
    "                print('titre :',list_texte3[l])\n",
    "                titre=list_texte3[l]\n",
    "                print(\"\\n \\n\")\n",
    "                \n",
    "                \n",
    "                list_texte4=texte2.split(\"<\")\n",
    "                print('resume :',list_texte4[0])\n",
    "                resume=list_texte4[0]\n",
    "                print(\"\\n \\n\")\n",
    "                \n",
    "                \n",
    "        print(\"\\n \\n\")\n",
    "        print(len(list_texte))\n",
    "        \n",
    "\n",
    "    # Storing the data in JSON format\n",
    "    df_sources.to_json(path_links_crawler+'liens_crawler.json', orient='split')\n",
    "    \n",
    "\n",
    "    # Storing the crawling date\n",
    "    with open(path_files+'date_last_crawling.txt', 'w') as file:\n",
    "        file.write(date_crawling)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "g2_launch_crawler_scholar.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
