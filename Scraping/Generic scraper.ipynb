{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import datetime\n",
    "from textblob import TextBlob\n",
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_date(soup):\n",
    "    \"\"\" \n",
    "        Extracting the date of publishing / updating of the article\n",
    "    \"\"\"\n",
    "    # Getting date of a video published on the article because it's the only date i found that can fit\n",
    "    if soup.find('meta', property='article:modified_time'):\n",
    "        date = soup.find('meta', property='article:modified_time').get('content')\n",
    "        art_published_datetime = datetime.datetime.strptime(date, '%Y-%m-%dT%H:%M:%S%z').date()\n",
    "    else:\n",
    "        art_published_datetime = datetime.date.today()\n",
    "    return art_published_datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lang(soup):\n",
    "    \n",
    "    \"\"\" \n",
    "        Extracting the language the article is written in\n",
    "    \"\"\"\n",
    "    \n",
    "    if soup.find('meta', property='og:locale'):\n",
    "        art_lang = soup.find('meta', property='og:locale').get('content')\n",
    "    elif soup.find('html').get('lang'):\n",
    "        art_lang = soup.find('html').get('lang')\n",
    "    else:\n",
    "        art_lang = 'fr-FR'\n",
    "    return art_lang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_title(soup):\n",
    "    \"\"\" \n",
    "        Extracting the title of the article\n",
    "    \"\"\"\n",
    "    if soup.find('meta', property = \"og:title\"):\n",
    "        art_title = soup.find('meta', property = \"og:title\").get('content')\n",
    "    elif soup.find('title'):\n",
    "        art_title = soup.find('title').get_text()\n",
    "    else :\n",
    "        art_title = \"no_data\"\n",
    "        #à voir avec des expressions regulières pour recup dans le lien du site la partie qui nous interesse\n",
    "        #for val in re.finditer(\"/+(\\w)+/+(\\w)+\", url): \n",
    "            #return val.group(0)\n",
    "    return art_title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_url(soup, url):\n",
    "    \"\"\" \n",
    "        Extracting url of the article\n",
    "    \"\"\"\n",
    "    if soup.find('meta', property = \"og:url\"):\n",
    "        art_url = soup.find('meta', property = \"og:url\").get('content')\n",
    "    elif soup.find('link', rel = 'canonical'):\n",
    "        art_url = soup.find('link', rel = 'canonical').get('href')\n",
    "    else :  #Si jamais on trouve pas, 2e paramètre url et on l'assigne (LUL)\n",
    "        art_url = url\n",
    "    return art_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_source_url(soup):\n",
    "    \"\"\" \n",
    "        Article's source url\n",
    "    \"\"\"\n",
    "    for val in re.finditer(\"(\\w)+://[^/]+/\", url):\n",
    "        return val.group(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_source_name(soup):\n",
    "    \"\"\" \n",
    "        Extracting article's source name\n",
    "    \"\"\"\n",
    "    src_name = getSourceUrl(soup).replace('https://www.','').replace('.fr/','')\n",
    "    \n",
    "    return src_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_source_type(soup):\n",
    "    \"\"\" \n",
    "        Extracting article's source type\n",
    "    \"\"\"\n",
    "    src_type = \"xpath_source\"\n",
    "    \n",
    "    return src_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_img(soup):\n",
    "    \"\"\" \n",
    "        Extracting article's image\n",
    "    \"\"\"\n",
    "    if soup.find('meta', property = \"og:image\"):\n",
    "        src_img = soup.find('meta', property = \"og:image\").get('content')\n",
    "    else :\n",
    "        src_img = \"no_data\"\n",
    "    return src_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_art_auth(soup):\n",
    "    \"\"\" \n",
    "        Extracting article's author\n",
    "    \"\"\"\n",
    "    if soup.find('meta', attrs={'name':\"twitter:data1\"}) :\n",
    "        art_auth = soup.find('meta', attrs={'name':\"twitter:data1\"}).get('content')\n",
    "    elif soup.find('div', class_ = \"td-post-author-name\") :\n",
    "        art_auth = find('div', class_ = \"td-post-author-name\").text\n",
    "    elif soup.find(\"a\", {\"class\": \"nomAuteur\"}):\n",
    "        art_auth = soup.find(\"a\", {\"class\": \"nomAuteur\"}).get('content')\n",
    "    else :\n",
    "        art_auth = soup.find_all(string = re.compile(\"écrit par (\\w)+ (\\w)+\"))\n",
    "        #'no_data'\n",
    "    \n",
    "    return art_auth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tag(soup):\n",
    "    ##keywords or tags\n",
    "    \"\"\"\n",
    "        Extracting article's tags if they exist\n",
    "    \"\"\"\n",
    "    if soup.find('meta', attrs={'name':\"keywords\"}):\n",
    "        art_tag = soup.find('meta', attrs={'name':\"keywords\"}).get('content')\n",
    "    else:\n",
    "        art_tag = soup.find_all(string=['tag', 'tags', 'keywords']) ##marche pas trop trop\n",
    "    return art_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_content(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    list_balises = ['h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'b', 'strong', 'i', 'em',\n",
    "                    'pre', 'mark', 'small', 'del', 's', 'ins', 'u', 'sub', 'sup', 'dfn', 'p', 'span']\n",
    "    if soup.find('article') is not None:\n",
    "        content_html = soup.find('article')\n",
    "        ### add if soup.find('article').find_all(list_balises) is not None:\n",
    "        content = soup.find('article').find_all(list_balises, string = True)\n",
    "        content = ' '.join(tag.text for tag in content).replace('\\xa0', '').strip()\n",
    "### Second way : get only the text in the div tag(s) (else member)\n",
    "#         list_p = soup.find('article').find_all('p', string = True)\n",
    "#         list_parent_p = [p.parent for p in list_p]\n",
    "#         if len(set(list_parent_p)) == 1:\n",
    "#             content_html = list_parent_p[0]\n",
    "#             content = ' '.join(\n",
    "#                 tag.text for tag in content_html.children if tag.name in list_balises).replace('\\xa0', '').strip()\n",
    "#         else:\n",
    "#             content_html = soup.find('article')\n",
    "#             content = [el.get_text() for el in list_parent_p]\n",
    "#             content = ' '.join(content).replace('\\xa0', '').strip()\n",
    "    else:\n",
    "        list_div = list()\n",
    "        for el in soup.find_all('div'):\n",
    "            if el.find_all('p', recursive = False):\n",
    "                list_div.append(el)\n",
    "        index_max = np.argmax([len(block.find_all('p')) for block in list_div])\n",
    "        content_html = list_div[index_max]\n",
    "        content = ' '.join(\n",
    "            tag.text for tag in content_html.children if tag.name in list_balises).replace('\\xa0', '').strip()\n",
    "    return content_html, content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "URLs = ['https://www.fnccr.asso.fr/article/big-data-territorial-publication-de-letude-de-la-fnccr/',\n",
    "        'https://www.banquedesterritoires.fr/big-data-territorial-la-gestion-des-donnees-un-enjeu-davenir-pour-les-collectivites', \n",
    "        'https://www.theinnovation.eu/comment-tuer-linnovation-avec-lanalyse-financiere/45',\n",
    "        'https://www.lemondeinformatique.fr/actualites/lire-les-salaries-et-les-dirigeants-percoivent-differement-la-transformation-digitale-81352.html',\n",
    "        'https://www.cnil.fr/fr/les-collectivites-territoriales-et-lopen-data-concilier-ouverture-des-donnees-et-protection-des',\n",
    "        'https://www.inserm.fr/actualites-et-evenements/actualites/ondes-electromagnetiques-faut-il-craindre-5g',\n",
    "        'https://www.parlonsrh.com/raisons-utiliser-lintelligence-artificielle-dans-gestion-gpec/',\n",
    "        'https://www.myrhline.com/actualite-rh/de-la-gpec-et-au-workforce-planning-les-5-evolutions-a-connaitre.html',\n",
    "        'https://grh-multi.net/fr/2016/05/compte-rendu-de-levenement-big-data-gpec/',\n",
    "        'https://changethework.com/gestion-paie-innovations/',\n",
    "        'https://changethework.com/chatbot-rh-recrutement/',\n",
    "        'http://sabbar.fr/management/le-management-strategique-et-le-management-operationnel/#:~:text=Le%20management%20op%C3%A9rationnel%20correspond%20aux,pour%20atteindre%20les%20objectifs%20fix%C3%A9s.',\n",
    "        # 'http://uis.unesco.org/fr/topic/donnees-sur-linnovation',\n",
    "        'http://www.linternaute.com/ville/classement/villes/population',\n",
    "        'http://www.territorial.fr/PAR_TPL_IDENTIFIANT/722/TPL_CODE/TPL_OUVR_NUM_FICHE/PAG_TITLE/La+gestion+en+AP-CP+et+la+programmation+des+investissements/53-dossiers-d-expert.htm',\n",
    "        'https://apnews.com/',\n",
    "        'https://blockchainfrance.net/decouvrir-la-blockchain/c-est-quoi-la-blockchain/',\n",
    "        'https://citoyen-ne-s-de-marseille.fr/cest-quoi-les-autorisations-de-programmes/',\n",
    "        'https://dataanalyticspost.com/Lexique/algorithmes-genetiques/']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = URLs[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.banquedesterritoires.fr/big-data-territorial-la-gestion-des-donnees-un-enjeu-davenir-pour-les-collectivites\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Données Publiques -  Publié le Cohésion des territoires,\\u2002 Fonction publique,\\u2002 Organisation territoriale, élus et institutions Launch Print Window Share this Page \\n      Plusieurs études récentes viennent éclairer les enjeux de la gouvernance des données à l\\'échelle locale. La dernière en date émane de la FNCCR, en partenariat avec la Caisse des Dépôts. Si la loi pour une République numérique a permis de mettre le sujet des données à l\\'agenda des collectivités, un long chemin reste à faire pour tirer parti du big data dans le cadre de l\\'action publique et pour coordonner les initiatives des différents acteurs publics et privés.\\n     La Fédération nationale des collectivités concédantes et régies (FNCCR) a rendu publique le 17 novembre une étude, co-pilotée par la Caisse des Dépôts, se penchant sur l\\'enjeu émergent du \"big data territorial\". Les activités des entités publiques et de leurs opérateurs génèrent en effet de plus en plus de données, qui pourraient être d\\'une grande aide pour mieux piloter les activités des collectivités et améliorer leur évaluation. Les territoires s\\'intéressent également de plus en plus à l\\'internet des objets et la mise en place de capteurs génère de nouvelles quantités de données qu\\'il est nécessaire de gérer, sécuriser et intégrer dans la gestion des services publics. Dans les métropoles, les données toujours mieux prises en compte L\\'étudeconfirme que les grandes métropoles françaises démontrent un dynamisme particulier dans la prise en compte des nouveaux enjeux des données. Lyon et Paris ont modifié leur organisation interne pour instituer la fonction transversale d\\'administrateur général des données, afin de décloisonner la gestion des systèmes d\\'information et de promouvoir l\\'usage des data sciences à l\\'échelle des collectivités tout entières. Sur ce point encore, Paris, Lyon, mais encore Nice, font office de pionniers en mettant en place des expérimentations tirant bénéfice des données. La métropole de Nice utilise par exemple le big data pour améliorer la gestion des risques d\\'inondation auxquels elle est exposée. Ces nouveaux usages et ces nouvelles organisations vont de pair avec une gestion plus unifiée des données gérées par les collectivités. Bordeaux met en place une \"stratégie smart data\" permettant la collecte de l\\'ensemble de ses données métier, leur homogénéisation, et leur stockage dans un unique entrepôt de données. Cette gestion centralisée permet un meilleur monitoring, un accès facilité aux données pour les agents publics et une gestion de l\\'open data également plus fluide et cohérente. De nombreux obstacles restent à franchir En dehors des grandes métropoles, la gestion des données est encore un thème en défrichage. Pour atteindre une masse critique, les acteurs publics d\\'un territoire ne peuvent se passer d\\'une mutualisation de la prise en charge de leurs données : or, l\\'intégration des systèmes d\\'information à l\\'échelle du bloc local communes-EPCI est parfois encore embryonnaire. Nombreux sont les enjeux face auxquels les collectivités peuvent se trouver démunies : gouvernance, acculturation des agents publics aux nouveaux usages de la donnée, retours d\\'expérience limités... Les données générées par les délégataires de services publics sont également parfois difficiles à récupérer et un effort juridique doit être fourni sur ce point pour clarifier la propriété des autorités concédantes sur les informations ayant trait à l\\'exploitation d\\'un service public. L\\'échelon national peut jouer un rôle L\\'étudede la FNCCR ouvre des pistes d\\'amélioration pour accompagner les collectivités dans la révolution des données. La notion de service public de la donnée, inaugurée par la loi pour une République numérique, augure de bons signaux pour améliorer l\\'intéropérabilité et les standardisation des données locales. Pour compléter ce dispositif, le document appelle à la création de \"missions locales de service public de la donnée\" à même d\\'harmoniser, au niveau régional, les démarches des collectivités pour la gestion des données. En somme, tout ce qui peut renforcer la coopération et la mutualisation est bienvenu, dans un contexte où les acteurs locaux ont besoin d\\'un apport d\\'expertise pour franchir le pas du big data. A ce titre, les structures de mutualisation informatique (syndicats mixtes, GIP, etc.), qui maillent certains territoires et procurent leur assistance aux petites collectivités, pourraient également revêtir un rôle déterminant dans l\\'apprivoisement du sujet des données par l\\'ensemble des territoires. Pierre-Marie Langlois / EVS \\n Plusieurs rapports récents sur les données territoriales\\n \\n P.-M.L.'"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(url)\n",
    "get_content(url)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<div class=\"skip-links position-fixed\">\n",
      " <ul class=\"list-inline m-0 p-0\">\n",
      "  <li class=\"list-inline-item\">\n",
      "   <a class=\"skip-links__link visually-hidden focusable\" data-a11y-dialog-show=\"modal-dialog-access-config\" data-target=\"#a11yModal\" data-toggle=\"modal\" href=\"#modal_access\" role=\"button\">\n",
      "    Open accessibility window\n",
      "   </a>\n",
      "  </li>\n",
      "  <li class=\"list-inline-item\">\n",
      "   <a class=\"skip-links__link visually-hidden focusable\" href=\"#block-navigationprincipale\">\n",
      "    Aller au menu principal\n",
      "   </a>\n",
      "  </li>\n",
      "  <li class=\"list-inline-item\">\n",
      "   <a class=\"skip-links__link visually-hidden focusable\" href=\"#main-content\">\n",
      "    Aller au contenu principal\n",
      "   </a>\n",
      "  </li>\n",
      " </ul>\n",
      "</div>\n",
      "\n",
      "\n",
      "\n",
      "<ul class=\"list-inline m-0 p-0\">\n",
      "<li class=\"list-inline-item\">\n",
      "<a class=\"skip-links__link visually-hidden focusable\" data-a11y-dialog-show=\"modal-dialog-access-config\" data-target=\"#a11yModal\" data-toggle=\"modal\" href=\"#modal_access\" role=\"button\"> Open accessibility window </a>\n",
      "</li>\n",
      "<li class=\"list-inline-item\">\n",
      "<a class=\"skip-links__link visually-hidden focusable\" href=\"#block-navigationprincipale\"> Aller au menu principal </a>\n",
      "</li>\n",
      "<li class=\"list-inline-item\">\n",
      "<a class=\"skip-links__link visually-hidden focusable\" href=\"#main-content\"> Aller au contenu principal </a>\n",
      "</li>\n",
      "</ul>\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "<ul class=\"list-inline m-0 p-0\">\n",
      "<li class=\"list-inline-item\">\n",
      "<a class=\"skip-links__link visually-hidden focusable\" data-a11y-dialog-show=\"modal-dialog-access-config\" data-target=\"#a11yModal\" data-toggle=\"modal\" href=\"#modal_access\" role=\"button\"> Open accessibility window </a>\n",
      "</li>\n",
      "<li class=\"list-inline-item\">\n",
      "<a class=\"skip-links__link visually-hidden focusable\" href=\"#block-navigationprincipale\"> Aller au menu principal </a>\n",
      "</li>\n",
      "<li class=\"list-inline-item\">\n",
      "<a class=\"skip-links__link visually-hidden focusable\" href=\"#main-content\"> Aller au contenu principal </a>\n",
      "</li>\n",
      "</ul>\n",
      "\n",
      "\n",
      "<li class=\"list-inline-item\">\n",
      "<a class=\"skip-links__link visually-hidden focusable\" data-a11y-dialog-show=\"modal-dialog-access-config\" data-target=\"#a11yModal\" data-toggle=\"modal\" href=\"#modal_access\" role=\"button\"> Open accessibility window </a>\n",
      "</li>\n",
      "\n",
      "\n",
      "<a class=\"skip-links__link visually-hidden focusable\" data-a11y-dialog-show=\"modal-dialog-access-config\" data-target=\"#a11yModal\" data-toggle=\"modal\" href=\"#modal_access\" role=\"button\"> Open accessibility window </a>\n",
      " Open accessibility window \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "<li class=\"list-inline-item\">\n",
      "<a class=\"skip-links__link visually-hidden focusable\" href=\"#block-navigationprincipale\"> Aller au menu principal </a>\n",
      "</li>\n",
      "\n",
      "\n",
      "<a class=\"skip-links__link visually-hidden focusable\" href=\"#block-navigationprincipale\"> Aller au menu principal </a>\n",
      " Aller au menu principal \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "<li class=\"list-inline-item\">\n",
      "<a class=\"skip-links__link visually-hidden focusable\" href=\"#main-content\"> Aller au contenu principal </a>\n",
      "</li>\n",
      "\n",
      "\n",
      "<a class=\"skip-links__link visually-hidden focusable\" href=\"#main-content\"> Aller au contenu principal </a>\n",
      " Aller au contenu principal \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "all_div = soup.find_all('div')\n",
    "print(all_div[0].prettify())\n",
    "for i in all_div[0].children:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "De la GPEC et au Workforce Planning : Les 5 évolutions à connaître\n",
      "Une philosophie nouvelle\n",
      "Une philosophie nouvelle\n",
      "L’accélération de la transformation des métiers, couplée à un environnement économique incertain, conduit à réinventer l’approche GPEC pour qu’elle réponde à des questions opérationnelles d’anticipation des besoins en compétences et d’évolution des effectifs. Il s’agit de fournir aux équipes RH des outils de pilotage identiques à ceux déployés dans le reste de l’entreprise afin qu’elles puissent modéliser rapidement les impacts RH des décisions stratégiques ou des ruptures d’activité comme celles que nous avons vécus avec la crise sanitaire. L’exercice d’anticipation est fait régulièrement et donne lieu à des plans d’actions opérationnels suivis par les RRH et les managers grâce au outils digitaux.\n",
      "La fin des « référentiels statiques »\n",
      "La fin des « référentiels statiques »\n",
      "Pour que cela fonctionne, les entreprises ne peuvent plus passer 2 à 3 ans à construire et déployer leur référentiel métiers et compétences. Elles doivent gagner en agilité en s’appuyant sur des bases de données externes nourries par une approche marché, enrichies par l’intelligence artificielle. Ces bases de données permettent de se concentrer sur la création d’une vision commune de ce que seront les métiers et compétences dans son entreprise demain. En parallèle, les collaborateurs déclarent leurs compétences de manière dynamique. Ils mettent ainsi à jour la cartographie de l’existant au fil de l’eau.\n",
      "L’exploitation des historiques\n",
      "L’exploitation des historiques\n",
      "Les projections GPEC sont en général réalisées sur la base d’un tableau excel avec l’effectif actuel, l’effectif cible, la prévision des entrées/sorties renseignés par le métier. Les historiques de données sont peu ou pas du tout exploités. La mise en place des Data Lake permet aujourd’hui aux entreprises d’exploiter les données d’historique pour construire des prévisions plus fiables et se donner les moyens de mieux anticiper les évolutions.\n",
      "La prise en compte de la réalité des individus\n",
      "La prise en compte de la réalité des individus\n",
      "Les exercices d’analyse de l’existant et de vieillissement des compétences étaient souvent faits à dire d’expert. Les cartographie de compétences aujourd’hui déployées dans de nombreuses entreprises permettent de mieux connaître l’état des lieux des compétences et la réalité des métiers. L’analyse de l’existant et sa projection sont désormais plus riches et plus fiables. Les systèmes permettent aux RH et Managers de faire le lien avec l’évolution des métiers et compétences pour déployer des plans d’actions individuels adaptés (Formation / Mobilité / …).\n",
      "L’apport des algorithmes \n",
      "L’apport des algorithmes \n",
      "La brutalité de la crise du COVID nous a montré les limites des approches traditionnelles. Nombreux sont ceux qui ont mis des semaines à répondre à des questions aussi simples que : « Que se passe-t-il si on gèle les recrutements pendant 6 mois ? », « Quelles sont les compétences clefs à maintenir sur site ? »,… Les solutions actuelles proposent des algorithmes de prévisions qui permettent de répondre à ces questions plus rapidement. Progressivement, les équipes peuvent se concentrer sur l’analyse et les plans d’actions. Les solutions les plus évoluées proposent des datavisualisations et des alertes permettant d’exploiter pleinement l’apport du Big Data et de l’IA.\n",
      " \n",
      " \n",
      "Carole MENGUY\n",
      "Carole MENGUY\n",
      " \n",
      " \n",
      "Articles RH relatifs \n",
      " \n",
      "Comment va évoluer le digital workspace ?\n",
      "Lier management innovant et RH  avec l’Odyssée Managériale\n",
      "[Actualités RH 2021] Ce qui change en Janvier\n",
      "L’écologie d’aller au travail avec  le Forfait mobilités durables\n",
      "Comment maintenir la culture d’entreprise en télétravail ?\n",
      "Emploi des personnes en situation de handicap : 6 nouvelles mesures annoncées\n",
      "à propos\n",
      "Dossiers RH\n",
      "Dossier transformation digitale RH\n",
      " \n",
      "Livres blancs et guides RH\n",
      "Suivez-nous\n",
      "Newsletter RH\n",
      " \n",
      "Le média des professionnels des Ressources Humaines. “Toute l’actualité RH et les tendances des ressources humaines d’aujourd’hui et de demain”\n",
      "© DESIGNRH 2019 – Tous droits réservés\n",
      "© DESIGNRH 2019 – Tous droits réservés\n",
      "Mentions légales\n",
      "Comment va évoluer le digital workspace ?\n"
     ]
    }
   ],
   "source": [
    "for el in soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'b', 'strong', 'i', 'em', 'pre', 'mark', 'small', 'del', 's', 'ins', 'u', 'sub', 'sup', 'dfn', 'p'], string = True):\n",
    "    print(el.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Historiquement, la GPEC est un exercice mené dans les entreprises pour répondre à des impératifs légaux de dialogue social. L’exercice, réalisé en moyenne tous les 3 ans, est souvent perçu comme théorique, chronophage et fastidieux.  Pourtant, depuis quelques mois, le terme de Workforce Planning revient sur le devant de la scène pour matérialiser la nouvelle dynamique de la GPEC. Voici les 5 évolutions à maîtriser pour comprendre l’évolution en cours :Une philosophie nouvelleL’accélération de la transformation des métiers, couplée à un environnement économique incertain, conduit à réinventer l’approche GPEC pour qu’elle réponde à des questions opérationnelles d’anticipation des besoins en compétences et d’évolution des effectifs. Il s’agit de fournir aux équipes RH des outils de pilotage identiques à ceux déployés dans le reste de l’entreprise afin qu’elles puissent modéliser rapidement les impacts RH des décisions stratégiques ou des ruptures d’activité comme celles que nous avons vécus avec la crise sanitaire. L’exercice d’anticipation est fait régulièrement et donne lieu à des plans d’actions opérationnels suivis par les RRH et les managers grâce au outils digitaux.La fin des « référentiels statiques »Pour que cela fonctionne, les entreprises ne peuvent plus passer 2 à 3 ans à construire et déployer leur référentiel métiers et compétences. Elles doivent gagner en agilité en s’appuyant sur des bases de données externes nourries par une approche marché, enrichies par l’intelligence artificielle. Ces bases de données permettent de se concentrer sur la création d’une vision commune de ce que seront les métiers et compétences dans son entreprise demain. En parallèle, les collaborateurs déclarent leurs compétences de manière dynamique. Ils mettent ainsi à jour la cartographie de l’existant au fil de l’eau.L’exploitation des historiquesLes projections GPEC sont en général réalisées sur la base d’un tableau excel avec l’effectif actuel, l’effectif cible, la prévision des entrées/sorties renseignés par le métier. Les historiques de données sont peu ou pas du tout exploités. La mise en place des Data Lake permet aujourd’hui aux entreprises d’exploiter les données d’historique pour construire des prévisions plus fiables et se donner les moyens de mieux anticiper les évolutions.La prise en compte de la réalité des individusLes exercices d’analyse de l’existant et de vieillissement des compétences étaient souvent faits à dire d’expert. Les cartographie de compétences aujourd’hui déployées dans de nombreuses entreprises permettent de mieux connaître l’état des lieux des compétences et la réalité des métiers. L’analyse de l’existant et sa projection sont désormais plus riches et plus fiables. Les systèmes permettent aux RH et Managers de faire le lien avec l’évolution des métiers et compétences pour déployer des plans d’actions individuels adaptés (Formation / Mobilité / …).L’apport des algorithmes La brutalité de la crise du COVID nous a montré les limites des approches traditionnelles. Nombreux sont ceux qui ont mis des semaines à répondre à des questions aussi simples que : « Que se passe-t-il si on gèle les recrutements pendant 6 mois ? », « Quelles sont les compétences clefs à maintenir sur site ? »,… Les solutions actuelles proposent des algorithmes de prévisions qui permettent de répondre à ces questions plus rapidement. Progressivement, les équipes peuvent se concentrer sur l’analyse et les plans d’actions. Les solutions les plus évoluées proposent des datavisualisations et des alertes permettant d’exploiter pleinement l’apport du Big Data et de l’IA.Vous l’aurez compris la GPEC, comme l’ensemble des pratiques RH, se transforme sous l’effet du digital et de l’IA. C’est un peu comme passer de la photo à la vidéo. N’hésitez pas à contacter les équipes de WiserSKILLS si vous souhaitez en savoir plus. Carole MENGUY Articles RH relatifs Programme : Semaine de la transformation digitale RH Janvier 2021 Sur quoi repose la croissance massive du marché des logiciels RH ? Les logiciels RH – ou SIRH (Système d’Information des Ressources…PartagezTweetezPartagez0 Partages      Carole MENGUYCompetenceGPECwiserskillsworkforce planning\n"
     ]
    }
   ],
   "source": [
    "from itertools import groupby\n",
    "\n",
    "l = [list(j) for i, j in groupby(parents)]\n",
    "idx = np.argmax(map(len, l))\n",
    "print(list(set(l[idx]))[0].text.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.myrhline.com/actualite-rh/de-la-gpec-et-au-workforce-planning-les-5-evolutions-a-connaitre.html\n",
      "[]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "attempt to get argmax of an empty sequence",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-37-fbddd65e94dd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0murl\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mURLs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m7\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mget_content\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-36-064ecf7e2b33>\u001b[0m in \u001b[0;36mget_content\u001b[1;34m(url)\u001b[0m\n\u001b[0;32m     25\u001b[0m                 \u001b[0mlist_div\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist_div\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m         \u001b[0mindex_max\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_all\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'p'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mblock\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlist_div\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m         \u001b[0mcontent_html\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist_div\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindex_max\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m         content = '\\n'.join(\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36margmax\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py\u001b[0m in \u001b[0;36margmax\u001b[1;34m(a, axis, out)\u001b[0m\n\u001b[0;32m   1184\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1185\u001b[0m     \"\"\"\n\u001b[1;32m-> 1186\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_wrapfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'argmax'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1187\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1188\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py\u001b[0m in \u001b[0;36m_wrapfunc\u001b[1;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[0;32m     56\u001b[0m     \u001b[0mbound\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mbound\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 58\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_wrapit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     59\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py\u001b[0m in \u001b[0;36m_wrapit\u001b[1;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[0;32m     45\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m         \u001b[0mwrap\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 47\u001b[1;33m     \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     48\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mwrap\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: attempt to get argmax of an empty sequence"
     ]
    }
   ],
   "source": [
    "\n",
    "print(url)\n",
    "get_content(url)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
