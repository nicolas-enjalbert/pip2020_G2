{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Created on Monday 04 January 2021  \n",
    "\n",
    "# **POC Crawler API Google quotidien**\n",
    "**Group 2 - Recherche de nouvelles sources**  \n",
    "*Projet Inter-Promo 2021 de la formation SID, Université Paul Sabatier, Toulouse*\n",
    "\n",
    "@authors : Michaël Corbeau, Marianne Manson, Louis Marquez, Nicolas Enjalbert Courrech"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'objectif de notre groupe était d'identifier de nouvelles sources d'information pertinentes et de les scraper. Nous avons développé des crawlers quotidiens spécifiques aux différents sites trouvés par le crawler de nouvelles sources. Avec l'objectif de chercher des articles sur des sources nouvellemment définies comme pertinentes, la solution de faire des crawler spécifiques n'est pas la plus optimale car elle demande une intervention humaine et d'ingéniérie pour faire ces crawler spécifiques. Nous vous présentons ici une preuve de concept d'un crawler quotidien générique basé sur une API Google visant à remplacer la totalité ou au moins une grande partie des crawlers quotidiens spécifiques. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous avons choisi une API google car elle nous permet de récupérer les résultats des recherches, ce qui s'avère assez compliqué sans API (mesures de protection prises par google pour éviter l'utilisation abusive de leur moteur de recherche). Ce crawler permet d'automatiser le processus de recherche de nouveaux articles quotidiens à partir d'une liste de sites présélectionnés et définis comme pertinents dans le cadre de la veille technologique. \n",
    "\n",
    "Ces sites-ci ont vocation à être consulté quotidiennement afin de connaître les derniers articles publiés. De manière naïve, l'ensemble des requêtes faite au moteur de recherche aurait le format suivant \"site:site-i.com mots-clef_j\". En combinant toutes les possibilités, nous obtenons un ensemble d'équation de taille $ K*P$ avec $K$ le nombre de sites et $P$ le nombre de mots clefs. Dans notre cas, nous avons définis dans un premier temps 30 sites à consultés quotidiennements et plus de 15000 mots clefs (résultat du produit cartésien de deux listes de mots clefs répondant par assemblage à la thématique de recherche). Dans ce cas précis cela retourne plus de 450 000 équations de recherche différentes. Ce nombre de requête est bien trop important pour être traité quotidiennement. Ce nombre aura tendance à augmenter au fur et à mesure que le nombre de sites pertinents augmentent.\n",
    "\n",
    "En faisant quelques essaies, nous nous sommes rendus compte que les sites visés retourne en moyenne pas plus de 10 articles par jour. L'intérêt est de réaliser des requêtes par site et non plus par mots clés, ce qui réduit drastiquement le nombre de requêtes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comme il est recommandé de ne pas effectuer plus d'une requête par minute avec la verison gratuite de l'API, nous avons optimisé le traitement en couplant le crawling et le scraping. Chaque crawl de site est directement suivi du scraping des articles correspondants pendant la durée d'attente d'une minute avant le prochain crawl."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimation des coûts de l'utilisation de l'API scraperAPI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- gratuit : 1000 requêtes / 5 requêtes concurrentes\n",
    "- 29\\\\$ par mois = 250 000 requêtes / 10 requêtes concurrentes\n",
    "- 99\\\\$ par mois = 1 000 000 requêtes / 25 requêtes concurrentes\n",
    "- 249\\\\$ par mois = 3 000 000 requêtes / 50 requêtes concurrentes\n",
    "- sur demande = requêtes illimités / requêtes concurrentes illimitées\n",
    "\n",
    "Plus d'informations sur https://www.scraperapi.com/pricing\n",
    "\n",
    "Pour la preuve de concept, nous utilisons 26 sites et 1 requête par page de résultats retournée. Comme les recherches sont sur les dernières 24h, il y a le plus souvent 1 seule page de résultats, 3 au maximum.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import json\n",
    "from urllib.parse import urlencode\n",
    "from requests import get\n",
    "import pandas as pd\n",
    "from BigScraper import BigScraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# API Key (created on Scraper API)\n",
    "API_KEY = '2daa0fbbc103c5172e706fcbc5845747'\n",
    "#'fff2df1787bf81bfe98277920f9a9fcb'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_google_url(url_site):\n",
    "    \"\"\"\n",
    "    Create the url for the google search of the new ressources \n",
    "    posted by the website during the last 24 hours\n",
    "    \n",
    "    Parameters:\n",
    "        url_site : string of the website url\n",
    "    \n",
    "    Out:\n",
    "        google : url of the search\n",
    "    \n",
    "    \"\"\"\n",
    "    google = \"https://www.google.com/search?hl=fr\"\n",
    "    #search on a specific site\n",
    "    google += \"&q=site%3A\" + url_site\n",
    "    #use of the google inner syntax to restrict the request to the last 24 hours\n",
    "    google += \"&as_qdr=d\"\n",
    "    return google"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_api_url(url):\n",
    "    \"\"\" \n",
    "    Creation of the URL that will allow the legal scraping of Google results (use of the API key). \n",
    "    This URL is equivalent to a Google search.\n",
    "\n",
    "    Parameter :\n",
    "        url : google URL created from the url website (create_google_url)\n",
    "    \n",
    "    Out :\n",
    "        proxy_url : URLs built using the API\n",
    "    \"\"\"\n",
    "\n",
    "    payload = {'api_key': API_KEY, 'url': url, 'autoparse': 'true', 'country_code': 'fr'}\n",
    "    proxy_url = 'http://api.scraperapi.com/?' + urlencode(payload)\n",
    "    return proxy_url\n",
    "\n",
    "def scraping(url):     \n",
    "    \"\"\" \n",
    "    Scraping with scraperapi of the different pages of the google search\n",
    "\n",
    "    Parameter :\n",
    "        url : URL built using the API\n",
    "    \n",
    "    Out :\n",
    "        list_src : list of scraping results for the different pages\n",
    "    \"\"\"\n",
    "    headers = {'Accept': 'application/json'}\n",
    "    response = get(url, headers = headers)\n",
    "    source = response.text\n",
    "    list_src = [source]\n",
    "\n",
    "    dic = json.loads(source)\n",
    "    next_page = dic['pagination']['nextPageUrl']\n",
    "\n",
    "    if next_page is not None:\n",
    "        time.sleep(60)\n",
    "        list_src.extend(scraping(get_api_url(next_page)))\n",
    "\n",
    "    \n",
    "    return list_src\n",
    "\n",
    "def get_links(result_scrap):\n",
    "    \"\"\" \n",
    "    Retrieval of the links from the scraping results of the google search\n",
    "\n",
    "    Parameter :\n",
    "        result_scrap : list of scraping results\n",
    "    \n",
    "    Out :\n",
    "        list_links : URL list\n",
    "    \"\"\"\n",
    "    list_links = []\n",
    "    for page in result_scrap:\n",
    "        try:\n",
    "            dico = json.loads(page)\n",
    "            result = dico['organic_results']\n",
    "\n",
    "            for i in range(len(result)):\n",
    "                list_links.append(result[i]['link'])\n",
    "        except: \n",
    "            list_links = []\n",
    "    return list_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_google_links_list(url):\n",
    "    \"\"\" \n",
    "    Given a preselectionned website, returns a list with all the links which \n",
    "    will be used to scrap the corresponding articles\n",
    "\n",
    "    Parameter :\n",
    "        url : url retrieved by the global crawler \n",
    "    \n",
    "    Out :\n",
    "        list_links : articles links list\n",
    "    \"\"\"\n",
    "\n",
    "    return get_links(scraping(get_api_url(create_google_url(url))))\n",
    "\n",
    "def get_dataframe(liste, df):\n",
    "    \"\"\" \n",
    "    From a list of articles to scrap and a predefined dataframe, \n",
    "    returns a dataframe with all the articles scraped\n",
    "\n",
    "    Parameter :\n",
    "        liste : articles links list\n",
    "        df : an empty df with the named columns\n",
    "    \n",
    "    Out :\n",
    "        df : a df filled with the scraped articles\n",
    "    \"\"\"\n",
    "    \n",
    "    i = 0\n",
    "    delay = 0\n",
    "    bs = BigScraper()\n",
    "    start_time = time.time() \n",
    "    for link in liste:\n",
    "        try: \n",
    "            df.loc[i] = bs.scrap(link)\n",
    "            print(link + \" has been successfully added to the dataframe\")\n",
    "            i += 1\n",
    "        except:\n",
    "            print(link + \" is not a valid article\")\n",
    "    interval = int(time.time() - start_time)\n",
    "\n",
    "    #it's recommended to execute 1 request only per minute in the free API version\n",
    "    if interval < 60:\n",
    "        delay = 60 - interval\n",
    "        print(str(delay) + \" s before the next request\")\n",
    "        time.sleep(delay)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "list of preselectionned urls from the global scraper\n",
    "'''\n",
    "\n",
    "liste_urls = ['https://www.zdnet.fr',\n",
    "              'https://www.cnil.fr',\n",
    "              'https://www.fnccr.asso.fr',\n",
    "              'https://grh-multi.net/fr',\n",
    "              'https://www.theinnovation.eu',\n",
    "              'https://www.inserm.fr',\n",
    "              'https://www.parlonsrh.com',\n",
    "              'https://www.myrhline.com',\n",
    "              'https://changethework.com',\n",
    "              'https://blockchainfrance.net',\n",
    "              'https://citoyen-ne-s-de-marseille.fr',\n",
    "              'https://hellofuture.orange.com',\n",
    "              'https://www.data.gouv.fr',\n",
    "              'https://www.digitalrecruiters.com',\n",
    "              'https://www.erudit.org/fr',\n",
    "              'https://www.lebigdata.fr',\n",
    "              'https://www.linternaute.fr',\n",
    "              'https://www.riskinsight-wavestone.com',\n",
    "              'https://search.sap.com',\n",
    "              'https://www.cadre-dirigeant-magazine.com',\n",
    "              'https://www.journaldunet.com',\n",
    "              'https://www.usine-digitale.fr',\n",
    "              'https://www.usinenouvelle.com',\n",
    "              'https://www.lemondeinformatique.fr',\n",
    "              'https://www.silicon.fr',\n",
    "              'https://www.lemonde.fr']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_articles_to_df(liste_urls):\n",
    "    \"\"\" \n",
    "    From a list of websites to scrap and a predefined dataframe, \n",
    "    returns a dataframe with all the websites scraped\n",
    "\n",
    "    Parameter :\n",
    "        liste_urls : websites links list\n",
    "        an empty df with the named columns\n",
    "    \n",
    "    Out :\n",
    "        df : a df filled with the scraped websites\n",
    "    \"\"\"\n",
    "    \n",
    "    df_final = pd.DataFrame(columns=['art_content','art_content_html','art_published_datetime','art_lang','art_title','art_url','src_name','src_type','src_url','art_img','art_auth','art_tag']  )\n",
    "\n",
    "    for website in liste_urls:\n",
    "        print(website)\n",
    "        test = get_google_links_list(website)\n",
    "        print(\"total results : \"+str(len(test)))\n",
    "        df = pd.DataFrame(columns=['art_content','art_content_html','art_published_datetime','art_lang','art_title','art_url','src_name','src_type','src_url','art_img','art_auth','art_tag']  )\n",
    "\n",
    "        df = get_dataframe(test, df)\n",
    "        df_final = df_final.append(df)\n",
    "    \n",
    "    return df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.zdnet.fr\n",
      "total results : 10\n",
      "https://www.zdnet.fr/actualites/adieu-token-place-au-cyberjeton-39916309.htm has been successfully added to the dataframe\n",
      "https://www.zdnet.fr/actualites/secnumcloud-tout-comprendre-en-cinq-points-39916267.htm has been successfully added to the dataframe\n",
      "https://www.zdnet.fr/actualites/le-bitcoin-rebondit-christine-lagarde-tres-mefiante-39916263.htm has been successfully added to the dataframe\n",
      "https://www.zdnet.fr/actualites/xiaomi-rejoint-la-liste-noire-de-l-administration-trump-39916283.htm has been successfully added to the dataframe\n",
      "https://www.zdnet.fr/actualites/ledger-la-faille-de-juillet-etait-plus-serieuse-qu-annoncee-39916301.htm has been successfully added to the dataframe\n",
      "https://www.zdnet.fr/actualites/testeurs-pros-asus-business-une-solution-de-mobilite-qui-tient-ses-promesses-39915699.htm has been successfully added to the dataframe\n",
      "https://www.zdnet.fr/actualites/google-a-elabore-un-nouveau-programme-de-formation-sur-le-cloud-computing-39916205.htm has been successfully added to the dataframe\n",
      "https://www.zdnet.fr/actualites/microsoft-oracle-et-salesforce-rejoignent-un-projet-de-vaccination-pour-la-covid-19-39916303.htm has been successfully added to the dataframe\n",
      "https://www.zdnet.fr/actualites/l-espoir-de-porter-linux-sur-les-mac-m1-d-apple-est-il-realiste-39916297.htm has been successfully added to the dataframe\n",
      "https://www.zdnet.fr/actualites/le-nouveau-pdg-d-intel-pat-gelsinger-est-confronte-a-des-defis-de-taille-39916281.htm has been successfully added to the dataframe\n",
      "56 s before the next request\n",
      "https://www.cnil.fr\n",
      "total results : 5\n",
      "https://www.cnil.fr/fr/les-fichiers-utilises-en-communication-politique has been successfully added to the dataframe\n",
      "https://www.cnil.fr/fr/evenement/seance-pleniere-102 has been successfully added to the dataframe\n",
      "https://www.cnil.fr/fr/type-doffre-demploi/emplois is not a valid article\n",
      "https://www.cnil.fr/fr/evenement/seance-pleniere-105 has been successfully added to the dataframe\n",
      "https://www.cnil.fr/fr/marche-public/prestations-de-formation-en-anglais-des-agents-de-la-cnil-0 has been successfully added to the dataframe\n",
      "47 s before the next request\n",
      "https://www.fnccr.asso.fr\n",
      "total results : 2\n",
      "https://www.fnccr.asso.fr/article/base-de-redaction-cahier-des-charges/ is not a valid article\n",
      "https://www.fnccr.asso.fr/article/note-decret-preservation-ressources-en-eau/ is not a valid article\n",
      "58 s before the next request\n",
      "https://grh-multi.net/fr\n",
      "total results : 0\n",
      "60 s before the next request\n",
      "https://www.theinnovation.eu\n",
      "total results : 0\n",
      "60 s before the next request\n",
      "https://www.inserm.fr\n",
      "total results : 1\n",
      "https://www.inserm.fr/actualites-et-evenements/actualites/alcool-et-ecstasy-cocktail-explosif has been successfully added to the dataframe\n",
      "60 s before the next request\n",
      "https://www.parlonsrh.com\n",
      "total results : 2\n",
      "https://www.parlonsrh.com/blog/ has been successfully added to the dataframe\n",
      "https://www.parlonsrh.com/revue-du-web-329-couvre-feu-protocole-sanitaire-et-arret-de-travail-2/ has been successfully added to the dataframe\n",
      "56 s before the next request\n",
      "https://www.myrhline.com\n",
      "total results : 9\n",
      "https://www.myrhline.com/actualite-rh/tag/myprotime has been successfully added to the dataframe\n",
      "https://www.myrhline.com/actualite-rh/tag/elodie-loing has been successfully added to the dataframe\n",
      "https://www.myrhline.com/actualite-rh/tag/karine-merle has been successfully added to the dataframe\n",
      "https://www.myrhline.com/actualite-rh/tag/veronique-soussan has been successfully added to the dataframe\n",
      "https://www.myrhline.com/actualite-rh/tag/mobilite-professionnelles has been successfully added to the dataframe\n",
      "https://www.myrhline.com/actualite-rh/tag/david-boulestin has been successfully added to the dataframe\n",
      "https://www.myrhline.com/actualite-rh/tag/julie-coudry has been successfully added to the dataframe\n",
      "https://www.myrhline.com/actualite-rh/tag/relaxation-au-travail has been successfully added to the dataframe\n",
      "https://www.myrhline.com/actualite-rh/theme/type-article/page/37?filter_by=random_posts has been successfully added to the dataframe\n",
      "52 s before the next request\n",
      "https://changethework.com\n",
      "total results : 0\n",
      "60 s before the next request\n",
      "https://blockchainfrance.net\n",
      "total results : 0\n",
      "60 s before the next request\n",
      "https://citoyen-ne-s-de-marseille.fr\n",
      "total results : 1\n",
      "https://citoyen-ne-s-de-marseille.fr/wp-files/eg-transparency/VILLE/RECUEILS%20ADMINISTRATIFS/raa_20140315_431.pdf is not a valid article\n",
      "57 s before the next request\n",
      "https://hellofuture.orange.com\n",
      "total results : 0\n",
      "60 s before the next request\n"
     ]
    }
   ],
   "source": [
    "df = all_articles_to_df(liste_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans notre exemple, avec 26 sites, le processus total dure environ 45 minutes et nous retourne un dataframe avec 233 articles scrapés. Sur les 26 sites, une partie ont retourné un résultat nul (pas d'articles dans les dernières 24 heures) cf. pistes d'amélioration. Notre solution fonctionne avec la version gratuite de l'API google. \n",
    "\n",
    "Cette solution apporte certains avantage non résolu par des crawlers spécifiques. Ce déroulement fonctionne effectivement de la même façon pour tous les sites référencés par google et il n'est donc pas nécessaire d'avoir une intervention humaine lorsque de nouveaux sites web sont définis comme pertinents. De plus, le moteur de recherche Google permet de faire une sélection par date lors de cette recherche ce qui permet d'éviter de faire la sélection en post-scrapping. \n",
    "\n",
    "Néanmoins, implémenter de cette façon, la recherche ne prend pas en compte la sélection de l'article de façon thématique. L'article sera quand même défini comme répondant à la thématique innovation et gamme de gestion après le traitement du groupe de classification (groupe 5) mais une partie des articles retournés ne parleront peut-être pas de la thématique voulue. Une solution est de faire une recherche de mots clefs directement sur le contenu de l'article une fois qu'il est scrapé. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pistes d'amélioration :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- vérifier si une page contient la balise \"meta content = article\" afin de détecter si la page à scraper est un article. Cela ne marche pas dans tous les cas, mais cela permettrait d'éviter l'appel du BigScraper pour traiter des pages non pertinentes => gain de temps, augmentation de la fiabilité\n",
    "- conserver les liens d'articles rejetés pour vérifier qu'ils ne sont effectivement pas scrapables\n",
    "- associer une requête à un mot-clé d'un lexique pour augmenter la pertinence des résultats des requêtes google\n",
    "- calculer les fréquences de mises à jour des sites. Beaucoup de sites ne sont pas mis à jour quotidiennement et peuvent donc être scrapés à des intervalles de temps plus espacés ce qui permettrait de diminuer le nombre de requêtes quotidiennes ainsi que le temps de traitement\n",
    "- pour les sites qui sont souvent mis à jour avec de nombreuses pages, affiner l'url source par dossiers et sous-dossiers afin de réduire le temps de traitement\n",
    "- la durée de traitement peut être réduite en utilisant des requêtes concurrentes\n",
    "- on peut augmenter la fiabilité en effectuant des recherches par mot-clé une fois que les articles ont été scrapés"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code pour détecter si la page à scraper est un article :\n",
    "(Cette fonction n'est pour le moment pas implémentée dans le scraping des derniers articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_articles(url_list):\n",
    "    \"\"\" \n",
    "    Sorting of the articles in the url list (given that the article as the tag \"<meta property=\"og:type\" content=\"article\">\")\n",
    "\n",
    "    Parameter :\n",
    "        url_list : URLs of the result of the google search\n",
    "    \n",
    "    Out :\n",
    "        list_links : list with only the links of pages identified as articles\n",
    "    \"\"\"\n",
    "    list_links = []\n",
    "    for i in range(len(url_list)):\n",
    "            url = url_list[i]\n",
    "            req = get(url)\n",
    "            html_soup = BeautifulSoup(req.text, 'html.parser')\n",
    "            meta = html_soup.find('meta',{'property':'og:type'})\n",
    "            if meta is not None:\n",
    "                og_type = meta['content']\n",
    "                if og_type == \"article\":\n",
    "                    list_links.append(url)\n",
    "    return list_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
