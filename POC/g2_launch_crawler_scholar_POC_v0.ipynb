{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "g2_launch_crawler_scholar.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5_Wd1pn6rNJv"
      },
      "source": [
        "# -*- coding: utf-8 -*-\r\n",
        "\r\n",
        "Created on Mon Jan 11 11:39:57 2021\r\n",
        "\r\n",
        "@author: flavien\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tOKUtNrYrrvf"
      },
      "source": [
        "**POC : crawler sur google scholar**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6sXkggsPrGyc"
      },
      "source": [
        "########## Module import ##########\r\n",
        "\r\n",
        "# Files\r\n",
        "import json\r\n",
        "import pandas as pd\r\n",
        "\r\n",
        "# Maths\r\n",
        "import random\r\n",
        "\r\n",
        "# Extraction\r\n",
        "import re\r\n",
        "\r\n",
        "# Scraping\r\n",
        "import scrapy\r\n",
        "from requests import get\r\n",
        "\r\n",
        "# Parsing\r\n",
        "from urllib.parse import urlencode\r\n",
        "\r\n",
        "# Format\r\n",
        "from datetime import datetime\r\n",
        "from datetime import timedelta\r\n",
        "import time\r\n",
        "\r\n",
        "# Parameters to change \r\n",
        "from Parameters import *\r\n",
        "\r\n",
        "    \r\n",
        "########## Folders ##########\r\n",
        "\r\n",
        "# General directory\r\n",
        "global path_general\r\n",
        "path_general = 'C:/Users/flavien/Documents/SID/L3Sid/pip/'\r\n",
        "# Directory to store crawled URLs\r\n",
        "global path_links_crawler\r\n",
        "path_links_crawler = path_general\r\n",
        "# Directory to store partial backups\r\n",
        "global path_backup\r\n",
        "path_backup = path_general\r\n",
        "# Directory to load files :\r\n",
        "# Lexique_Gammes_Gestion, Lexique_Innovation, listCouple\r\n",
        "global path_files\r\n",
        "path_files = path_general\r\n",
        "\r\n",
        "########## Parameters ##########\r\n",
        "\r\n",
        "# API Key (created on Scraper API)\r\n",
        "API_KEY = open(path_files+'API_key.txt', 'r').read()\r\n",
        "\r\n",
        "\r\n",
        "def create_google_url(query, nb_results):\r\n",
        "    \"\"\"\r\n",
        "    Allows you to create a Google URL from a keyword\r\n",
        "\r\n",
        "    Parameter :\r\n",
        "        query : keyword to enter in the search bar\r\n",
        "\r\n",
        "    Out :\r\n",
        "        google_url : google URL created from the keyword\r\n",
        "    \"\"\"\r\n",
        "    # num = number of results to be scraped\r\n",
        "    google_dict = {'q': query, 'num': nb_results, }\r\n",
        "    google_url = 'http://scholar.google.com/scholar?' + urlencode(google_dict)\r\n",
        "    return google_url\r\n",
        "\r\n",
        "\r\n",
        "def combAND(couple):\r\n",
        "    \"\"\"Documentation\r\n",
        "    Parameters:\r\n",
        "        couple: a list of 2 Strings\r\n",
        "    Out :\r\n",
        "        list : a combination of the 2 members of a couple with AND between them\r\n",
        "    \"\"\"\r\n",
        "\r\n",
        "    return str(couple[0])+' '+'AND'+' '+str(couple[1])\r\n",
        "\r\n",
        "\r\n",
        "def listToAND(listCouple):\r\n",
        "    \"\"\"Documentation\r\n",
        "    Parameters:\r\n",
        "        listCouple: a list of couple\r\n",
        "\r\n",
        "    Out :\r\n",
        "        list : a list of the combination of the couple of listCouple\r\n",
        "    \"\"\"\r\n",
        "\r\n",
        "    # We use combAND\r\n",
        "    return [combAND(i) for i in listCouple]\r\n",
        "\r\n",
        "\r\n",
        "def combOR(tuple):\r\n",
        "    \"\"\"Documentation\r\n",
        "\r\n",
        "    Parameters:\r\n",
        "        tuple: a list of String\r\n",
        "\r\n",
        "    Out :\r\n",
        "        final : a combination of the members of the tuple with OR between them\r\n",
        "                and framed with ()\r\n",
        "    \"\"\"\r\n",
        "\r\n",
        "    # First step : initialisation of final\r\n",
        "    final = '('+str(tuple[0])+')'\r\n",
        "    # Second step : adding the rest of the tuple\r\n",
        "    for i in range(1, len(tuple)):\r\n",
        "        final = final+'|'+'('+tuple[i]+')'\r\n",
        "    return final\r\n",
        "\r\n",
        "\r\n",
        "# Applying the previous function to a list of tuple\r\n",
        "def listToOR(listTuple):\r\n",
        "    \"\"\"Documentation\r\n",
        "    Parameters:\r\n",
        "        listTuple: a list of tuples\r\n",
        "\r\n",
        "    Out :\r\n",
        "        list : a list of the combination of the tuple of listTuple\r\n",
        "    \"\"\"\r\n",
        "\r\n",
        "    # We use combOR\r\n",
        "    return [combOR(i) for i in listTuple]\r\n",
        "\r\n",
        "\r\n",
        "# Making a list of random tuples\r\n",
        "# We have to limit the number of request, by default 1000, and we make couples\r\n",
        "def listComb(listAND, numbT=2, iteration=int(1000)):\r\n",
        "    \"\"\"Documentation\r\n",
        "\r\n",
        "    Parameters:\r\n",
        "        listAND: a list Strings with AND\r\n",
        "        numbT : the length of the tuple we want to create\r\n",
        "        iteration : the maximum number of combination we want to create\r\n",
        "\r\n",
        "    Out :\r\n",
        "        finalList : a list of the combination of the tuple of listTuple\r\n",
        "    \"\"\"\r\n",
        "\r\n",
        "    finalList = []\r\n",
        "    i = 0\r\n",
        "    # Step 1 : we loop until we have enough tuples or the list is empty\r\n",
        "    while ((len(listAND) >= numbT) and (i < iteration)):\r\n",
        "        i += 1\r\n",
        "        # Step 2 : at each loop, we take some random elements of listAND and\r\n",
        "        # create a tuple with them\r\n",
        "        listRand = random.sample(listAND, numbT)\r\n",
        "        # Step 3 : we remove the elements from listAND\r\n",
        "        for j in listRand:\r\n",
        "            listAND.remove(j)\r\n",
        "        # Step 4 :we add the tuple we created to our finalList\r\n",
        "        finalList.append(listRand)\r\n",
        "    return finalList\r\n",
        "\r\n",
        "\r\n",
        "def link_filter_date(link, date_filter):\r\n",
        "    \"\"\"Documentation\r\n",
        "    Function that allows to add a \"limit date\" parameter in the link\r\n",
        "\r\n",
        "    Parameters:\r\n",
        "        link : URL to which the date filter should be added\r\n",
        "        date_limit : Limit date\r\n",
        "\r\n",
        "    Out :\r\n",
        "        link_new : URL with a date filter\r\n",
        "    \"\"\"\r\n",
        "\r\n",
        "    date_limit = datetime.strptime(date_filter, '%Y-%m-%d')\r\n",
        "    # Crawling 1 day before last crawling date\r\n",
        "    days_to_substract = timedelta(days=1)\r\n",
        "    \r\n",
        "    # Limit date\r\n",
        "    date_limit = date_limit - days_to_substract\r\n",
        "    jour = str(date_limit.day)\r\n",
        "    mois = str(date_limit.month)\r\n",
        "    annee = str(date_limit.year)\r\n",
        "\r\n",
        "    link_new = link+\"&source=lnt&tbs=cdr%3A1%2Ccd_min%3A\"+mois+\"%2F\"+jour+\"%2F\"+annee+\"%2Ccd_max%3A&tbm=\"\r\n",
        "    return link_new\r\n",
        "\r\n",
        "\r\n",
        "def get_url(url, date_filter):\r\n",
        "    \"\"\"\r\n",
        "    Creation of the URL that will allow the legal scraping of Google results\r\n",
        "    (use of the API key). This URL is equivalent to a Google search.\r\n",
        "\r\n",
        "    Parameter :\r\n",
        "        url : google URL created from the keyword\r\n",
        "\r\n",
        "    Out :\r\n",
        "        proxy_url : URLs built using the API\r\n",
        "    \"\"\"\r\n",
        "\r\n",
        "    payload = {'api_key': API_KEY,\r\n",
        "               'url': url,\r\n",
        "               'autoparse': 'true',\r\n",
        "               'lr': 'fr'#,\r\n",
        "               # Depersonalisation of results\r\n",
        "#               'pws': 0\r\n",
        "    }\r\n",
        "    proxy_url = 'http://api.scraperapi.com/?' + urlencode(payload)\r\n",
        "    date_url = link_filter_date(proxy_url, date_filter)+ \"&scisbd=1\"\r\n",
        "    return date_url\r\n",
        "\r\n",
        "\r\n",
        "class GoogleSpider(scrapy.Spider):\r\n",
        "    \"\"\"\r\n",
        "    This class lists functions for scraping Google results from a list of keywords\r\n",
        "    \"\"\"\r\n",
        "\r\n",
        "    # GoogleSpider class name\r\n",
        "    name = 'google'\r\n",
        "    # Name of the site to be scraped\r\n",
        "    allowed_domains = ['scholar.google.com']\r\n",
        "    # Settings\r\n",
        "    custom_settings = {\r\n",
        "                        # Criticality level at which the log is displayed\r\n",
        "                        'LOG_LEVEL': 'INFO',\r\n",
        "                        # Maximum number of simultaneous requests\r\n",
        "                        # 'CONCURRENT_REQUESTS_PER_DOMAIN': 1,\r\n",
        "                        # 'CONCURRENT_REQUESTS': 2,\r\n",
        "                        # 'CONCURRENT_ITEMS': 200,\r\n",
        "                        # Maximum number of retries to be made if the query fails\r\n",
        "                        'RETRY_TIMES': 0}\r\n",
        "\r\n",
        "    def start_requests(self, listCouple, length, requestNumber, date_filter, nb_results):\r\n",
        "        # Initialisation of DataFrame\r\n",
        "        df = pd.DataFrame(columns=['URL', 'Query'])\r\n",
        "        # Format changeover\r\n",
        "        lWork = listToAND(listCouple)\r\n",
        "        # Selection of queries\r\n",
        "        lWork = listComb(lWork, numbT=length, iteration=requestNumber)\r\n",
        "        # We change the format of Keywords\r\n",
        "        lWork = listToOR(lWork)\r\n",
        "\r\n",
        "        lURL = []\r\n",
        "        # We loop the keywords to generate the queries\r\n",
        "        print(lWork)\r\n",
        "        for query in lWork:\r\n",
        "            url = create_google_url(query, nb_results)\r\n",
        "            lURL.append(str(scrapy.Request(get_url(url, date_filter),\r\n",
        "                                           meta={'pos': 0}))[5:-1])\r\n",
        "        # Column generation\r\n",
        "        df['Query'] = lWork\r\n",
        "        df['URL'] = lURL\r\n",
        "\r\n",
        "        yield df\r\n",
        "\r\n",
        "\r\n",
        "# Launch crawling\r\n",
        "def Launch_Crawler():\r\n",
        "\r\n",
        "    ########## Load files ##########\r\n",
        "\r\n",
        "    # Last crawling date\r\n",
        "    p_date = open(path_files+'date_last_crawling.txt', 'r').read()\r\n",
        "    \r\n",
        "    #Lexique_Gammes_Gestion = open(path_files+'Lexique_Gammes_Gestion.txt', 'r').read()\r\n",
        "    #Lexique_Innovation = open(path_files+'Lexique_Innovation.txt', 'r').read()\r\n",
        "    \r\n",
        "    # List of keyword pairs\r\n",
        "    df = pd.read_json(path_files+'listCouple.json', orient='index')\r\n",
        "    df_t = df.T\r\n",
        "    listCouple = df_t.values.tolist()\r\n",
        "    \r\n",
        "    # Test on a few couples\r\n",
        "    p_listCouple = listCouple[2:4]\r\n",
        "    \r\n",
        "    \r\n",
        "    ########## Start building URLs ##########\r\n",
        "    \r\n",
        "    df_result = list(GoogleSpider().start_requests(p_listCouple, p_length, p_requestNumber, p_date, p_nb_results))[0]\r\n",
        "    \r\n",
        "    ########## Crawling of Google results ##########\r\n",
        "    \r\n",
        "    # Crawling start time\r\n",
        "    print(\"Crawling start time : \"+datetime.now().strftime(\"%H:%M:%S\"))\r\n",
        "    \r\n",
        "    list_source = []\r\n",
        "    i = 0\r\n",
        "    \r\n",
        "    for index, row in df_result.iterrows():\r\n",
        "        link = row['URL']\r\n",
        "        query = row['Query']\r\n",
        "        # 1 minute break to avoid API overloading\r\n",
        "        time.sleep(60)\r\n",
        "        # URL scraping\r\n",
        "        response = get(link)\r\n",
        "    \r\n",
        "        # Test if the request was successful\r\n",
        "        if response.status_code == 200:\r\n",
        "            # Addition of the scraped google results and the corresponding query\r\n",
        "            list_source.append([response.text, query])\r\n",
        "    \r\n",
        "            i += 1\r\n",
        "            # Saving the results every 20 queries\r\n",
        "            if (i % 20 == 0):\r\n",
        "                with open(path_backup+'etape'+str(i)+'.json', 'w') as jsonfile:\r\n",
        "                    json.dump(list_source, jsonfile)\r\n",
        "\r\n",
        "\r\n",
        "    # Crawling end time\r\n",
        "    print(\"Crawling end time : \"+datetime.now().strftime(\"%H:%M:%S\"))\r\n",
        "    date_crawling = datetime.now().strftime(\"%Y-%m-%d\")\r\n",
        "    \r\n",
        "    \r\n",
        "    ########## Processing of results ##########\r\n",
        "    \r\n",
        "    for source in list_source:\r\n",
        "        \r\n",
        "        source=source.replace(\"<b>\", \"\")\r\n",
        "        source=source.replace(\"</b>\", \"\")\r\n",
        "                \r\n",
        "        list_texte=source.split(\"</a></h3><div class=\\\"gs_a\\\">\")\r\n",
        "        for texte in list_texte:\r\n",
        "            \r\n",
        "            list_texte2=texte.split(\"<div class=\\\"gs_rs\\\">\")\r\n",
        "            \r\n",
        "            for texte2 in list_texte2:\r\n",
        "                \r\n",
        "                list_texte3=texte2.split(\">\")\r\n",
        "                l= len(list_texte3)-1\r\n",
        "                print('titre :',list_texte3[l])\r\n",
        "                titre=list_texte3[l]\r\n",
        "                print(\"\\n \\n\")\r\n",
        "                \r\n",
        "                \r\n",
        "                list_texte4=texte2.split(\"<\")\r\n",
        "                print('resume :',list_texte4[0])\r\n",
        "                resume=list_texte4[0]\r\n",
        "                print(\"\\n \\n\")\r\n",
        "                \r\n",
        "                \r\n",
        "        print(\"\\n \\n\")\r\n",
        "        print(len(list_texte))\r\n",
        "        \r\n",
        "\r\n",
        "    # Storing the data in JSON format\r\n",
        "    df_sources.to_json(path_links_crawler+'liens_crawler.json', orient='split')\r\n",
        "    \r\n",
        "\r\n",
        "    # Storing the crawling date\r\n",
        "    with open(path_files+'date_last_crawling.txt', 'w') as file:\r\n",
        "        file.write(date_crawling)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}