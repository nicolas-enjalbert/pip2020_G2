{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled23.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ugdyGjObLj1g"
      },
      "source": [
        "#**STATISTICAL PART**\r\n",
        "\r\n",
        "Created on Monday 04 January 2021  \r\n",
        "\r\n",
        "**Group 5 - Identification of new sources**  \r\n",
        "\r\n",
        "@authors : C.P.M, Y.S., S.B. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fhKR7skPLqgI"
      },
      "source": [
        "##**1/ Import of library**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Rzygq9PHt-v",
        "outputId": "1e9879f2-6dca-46fb-b123-616e9f245cb6"
      },
      "source": [
        "!pip install scrapy\r\n",
        "import pandas as pd\r\n",
        "import numpy as np\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import seaborn as sns\r\n",
        "from google.colab import files\r\n",
        "from wordcloud import WordCloud\r\n",
        "from pandas import DataFrame\r\n",
        "#import warnings\r\n",
        "#warnings.filterwarnings('ignore')\r\n",
        "%matplotlib inline\r\n",
        "import pkg_resources,imp\r\n",
        "imp.reload(pkg_resources)\r\n",
        "import spacy\r\n",
        "import re\r\n",
        "import nltk\r\n",
        "nltk.download('stopwords')\r\n",
        "nltk.download('wordnet')\r\n",
        "from nltk.stem.snowball import FrenchStemmer\r\n",
        "stemmer = FrenchStemmer()\r\n",
        "import pickle\r\n",
        "import time\r\n",
        "from nltk.corpus import stopwords\r\n",
        "stop_words = set(stopwords.words('french'))\r\n",
        "from nltk.stem import LancasterStemmer\r\n",
        "from nltk.stem import SnowballStemmer\r\n",
        "import json\r\n",
        "import random\r\n",
        "import scrapy\r\n",
        "from scrapy import Selector\r\n",
        "from requests import get\r\n",
        "from urllib.parse import urlencode\r\n",
        "from urllib.parse import urlparse\r\n",
        "from datetime import datetime\r\n",
        "from datetime import date\r\n",
        "from datetime import timedelta"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting scrapy\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3a/16/3c7c37caf25f91aa21db194655515718c2a15f704f9f5c59a194f5c83db0/Scrapy-2.4.1-py2.py3-none-any.whl (239kB)\n",
            "\r\u001b[K     |█▍                              | 10kB 23.7MB/s eta 0:00:01\r\u001b[K     |██▊                             | 20kB 30.2MB/s eta 0:00:01\r\u001b[K     |████                            | 30kB 13.6MB/s eta 0:00:01\r\u001b[K     |█████▌                          | 40kB 10.0MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 51kB 6.2MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 61kB 6.6MB/s eta 0:00:01\r\u001b[K     |█████████▋                      | 71kB 7.1MB/s eta 0:00:01\r\u001b[K     |███████████                     | 81kB 7.3MB/s eta 0:00:01\r\u001b[K     |████████████▎                   | 92kB 7.2MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 102kB 7.5MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 112kB 7.5MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 122kB 7.5MB/s eta 0:00:01\r\u001b[K     |█████████████████▊              | 133kB 7.5MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 143kB 7.5MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 153kB 7.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 163kB 7.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████▎        | 174kB 7.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████▋       | 184kB 7.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 194kB 7.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▍    | 204kB 7.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▊   | 215kB 7.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 225kB 7.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 235kB 7.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 245kB 7.5MB/s \n",
            "\u001b[?25hCollecting service-identity>=16.0.0\n",
            "  Downloading https://files.pythonhosted.org/packages/e9/7c/2195b890023e098f9618d43ebc337d83c8b38d414326685339eb024db2f6/service_identity-18.1.0-py2.py3-none-any.whl\n",
            "Collecting protego>=0.1.15\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/db/6e/bf6d5e4d7cf233b785719aaec2c38f027b9c2ed980a0015ec1a1cced4893/Protego-0.1.16.tar.gz (3.2MB)\n",
            "\u001b[K     |████████████████████████████████| 3.2MB 11.4MB/s \n",
            "\u001b[?25hCollecting parsel>=1.5.0\n",
            "  Downloading https://files.pythonhosted.org/packages/23/1e/9b39d64cbab79d4362cdd7be7f5e9623d45c4a53b3f7522cd8210df52d8e/parsel-1.6.0-py2.py3-none-any.whl\n",
            "Collecting Twisted>=17.9.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b7/04/1a664c9e5ec0224a1c1a154ddecaa4dc7b8967521bba225efcc41a03d5f3/Twisted-20.3.0-cp36-cp36m-manylinux1_x86_64.whl (3.1MB)\n",
            "\u001b[K     |████████████████████████████████| 3.1MB 45.6MB/s \n",
            "\u001b[?25hCollecting cssselect>=0.9.1\n",
            "  Downloading https://files.pythonhosted.org/packages/3b/d4/3b5c17f00cce85b9a1e6f91096e1cc8e8ede2e1be8e96b87ce1ed09e92c5/cssselect-1.1.0-py2.py3-none-any.whl\n",
            "Collecting pyOpenSSL>=16.2.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b2/5e/06351ede29fd4899782ad335c2e02f1f862a887c20a3541f17c3fa1a3525/pyOpenSSL-20.0.1-py2.py3-none-any.whl (54kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 4.6MB/s \n",
            "\u001b[?25hCollecting itemadapter>=0.1.0\n",
            "  Downloading https://files.pythonhosted.org/packages/88/83/ab33780fd93278e699561d61862d27343c95d3fe0a0081acd73e8e26a649/itemadapter-0.2.0-py3-none-any.whl\n",
            "Collecting w3lib>=1.17.0\n",
            "  Downloading https://files.pythonhosted.org/packages/a3/59/b6b14521090e7f42669cafdb84b0ab89301a42f1f1a82fcf5856661ea3a7/w3lib-1.22.0-py2.py3-none-any.whl\n",
            "Collecting cryptography>=2.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c9/de/7054df0620b5411ba45480f0261e1fb66a53f3db31b28e3aa52c026e72d9/cryptography-3.3.1-cp36-abi3-manylinux2010_x86_64.whl (2.6MB)\n",
            "\u001b[K     |████████████████████████████████| 2.6MB 26.1MB/s \n",
            "\u001b[?25hCollecting zope.interface>=4.1.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/82/b0/da8afd9b3bd50c7665ecdac062f182982af1173c9081f9af7261091c5588/zope.interface-5.2.0-cp36-cp36m-manylinux2010_x86_64.whl (236kB)\n",
            "\u001b[K     |████████████████████████████████| 245kB 49.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: lxml>=3.5.0; platform_python_implementation == \"CPython\" in /usr/local/lib/python3.6/dist-packages (from scrapy) (4.2.6)\n",
            "Collecting queuelib>=1.4.2\n",
            "  Downloading https://files.pythonhosted.org/packages/4c/85/ae64e9145f39dd6d14f8af3fa809a270ef3729f3b90b3c0cf5aa242ab0d4/queuelib-1.5.0-py2.py3-none-any.whl\n",
            "Collecting itemloaders>=1.0.1\n",
            "  Downloading https://files.pythonhosted.org/packages/b3/2b/eb2ddf7becf834679273a6f79ffdc6fbedf07c5272e2eddf412582143c0e/itemloaders-1.0.4-py3-none-any.whl\n",
            "Collecting PyDispatcher>=2.0.5\n",
            "  Downloading https://files.pythonhosted.org/packages/cd/37/39aca520918ce1935bea9c356bcbb7ed7e52ad4e31bff9b943dfc8e7115b/PyDispatcher-2.0.5.tar.gz\n",
            "Requirement already satisfied: pyasn1 in /usr/local/lib/python3.6/dist-packages (from service-identity>=16.0.0->scrapy) (0.4.8)\n",
            "Requirement already satisfied: attrs>=16.0.0 in /usr/local/lib/python3.6/dist-packages (from service-identity>=16.0.0->scrapy) (20.3.0)\n",
            "Requirement already satisfied: pyasn1-modules in /usr/local/lib/python3.6/dist-packages (from service-identity>=16.0.0->scrapy) (0.2.8)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from protego>=0.1.15->scrapy) (1.15.0)\n",
            "Collecting Automat>=0.3.0\n",
            "  Downloading https://files.pythonhosted.org/packages/dd/83/5f6f3c1a562674d65efc320257bdc0873ec53147835aeef7762fe7585273/Automat-20.2.0-py2.py3-none-any.whl\n",
            "Collecting incremental>=16.10.1\n",
            "  Downloading https://files.pythonhosted.org/packages/f5/1d/c98a587dc06e107115cf4a58b49de20b19222c83d75335a192052af4c4b7/incremental-17.5.0-py2.py3-none-any.whl\n",
            "Collecting PyHamcrest!=1.10.0,>=1.9.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/40/16/e54cc65891f01cb62893540f44ffd3e8dab0a22443e1b438f1a9f5574bee/PyHamcrest-2.0.2-py3-none-any.whl (52kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 8.9MB/s \n",
            "\u001b[?25hCollecting constantly>=15.1\n",
            "  Downloading https://files.pythonhosted.org/packages/b9/65/48c1909d0c0aeae6c10213340ce682db01b48ea900a7d9fce7a7910ff318/constantly-15.1.0-py2.py3-none-any.whl\n",
            "Collecting hyperlink>=17.1.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6e/aa/8caf6a0a3e62863cbb9dab27135660acba46903b703e224f14f447e57934/hyperlink-21.0.0-py2.py3-none-any.whl (74kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 12.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.6/dist-packages (from cryptography>=2.0->scrapy) (1.14.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from zope.interface>=4.1.3->scrapy) (51.1.1)\n",
            "Collecting jmespath>=0.9.5\n",
            "  Downloading https://files.pythonhosted.org/packages/07/cb/5f001272b6faeb23c1c9e0acc04d48eaaf5c862c17709d20e3469c6e0139/jmespath-0.10.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: idna>=2.5 in /usr/local/lib/python3.6/dist-packages (from hyperlink>=17.1.1->Twisted>=17.9.0->scrapy) (2.10)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.6/dist-packages (from cffi>=1.12->cryptography>=2.0->scrapy) (2.20)\n",
            "Building wheels for collected packages: protego, PyDispatcher\n",
            "  Building wheel for protego (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for protego: filename=Protego-0.1.16-cp36-none-any.whl size=7766 sha256=b099a103388f113b70e8fe0e8f718ba090c61fd139415ce88f887242ac34c9df\n",
            "  Stored in directory: /root/.cache/pip/wheels/51/01/d1/4a2286a976dccd025ba679acacfe37320540df0f2283ecab12\n",
            "  Building wheel for PyDispatcher (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for PyDispatcher: filename=PyDispatcher-2.0.5-cp36-none-any.whl size=11516 sha256=ce65582389b2aa6617570eb9716e0eef5c1946952f6d6fe2e8210240ec86c4ee\n",
            "  Stored in directory: /root/.cache/pip/wheels/88/99/96/cfef6665f9cb1522ee6757ae5955feedf2fe25f1737f91fa7f\n",
            "Successfully built protego PyDispatcher\n",
            "Installing collected packages: cryptography, service-identity, protego, w3lib, cssselect, parsel, Automat, incremental, PyHamcrest, zope.interface, constantly, hyperlink, Twisted, pyOpenSSL, itemadapter, queuelib, jmespath, itemloaders, PyDispatcher, scrapy\n",
            "Successfully installed Automat-20.2.0 PyDispatcher-2.0.5 PyHamcrest-2.0.2 Twisted-20.3.0 constantly-15.1.0 cryptography-3.3.1 cssselect-1.1.0 hyperlink-21.0.0 incremental-17.5.0 itemadapter-0.2.0 itemloaders-1.0.4 jmespath-0.10.0 parsel-1.6.0 protego-0.1.16 pyOpenSSL-20.0.1 queuelib-1.5.0 scrapy-2.4.1 service-identity-18.1.0 w3lib-1.22.0 zope.interface-5.2.0\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4PcYrQZIL3w7"
      },
      "source": [
        "##**2/ File import**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HvWoeW8eL4hC",
        "outputId": "2106aa12-cd98-477a-8056-e024d7e9cbef"
      },
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8RHtdfetH3_H"
      },
      "source": [
        "data = pd.read_json('/content/drive/My Drive/df_sources.json')"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NAunNeq-IIxu"
      },
      "source": [
        "data.drop(['score'], axis='columns', inplace=True)\r\n",
        "data = data.rename(columns = {'URL': 'art_url', 'Query': 'query'}) "
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uBWKB7EkMLEx"
      },
      "source": [
        "##**3/ Some statistics on the names of sites ...**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5jBwXcFeMNoG"
      },
      "source": [
        "1 - Preparation of data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sD35y1OGMOTi"
      },
      "source": [
        "# Step 1 : get the domain (site name) of the site thanks to the url of the article  \r\n",
        "\r\n",
        "def site_name(url: str) -> str :\r\n",
        "    \"\"\"Documentation\r\n",
        "\r\n",
        "        Parameter:\r\n",
        "            url: complete url of an article\r\n",
        "\r\n",
        "        Out:\r\n",
        "            base_url: name of a website\r\n",
        "\r\n",
        "        \"\"\"\r\n",
        "    site = url.split(\"://\")\r\n",
        "    if site[0] == \"https\" or site[0] == \"http\":\r\n",
        "        name_site = site[1]\r\n",
        "    else:\r\n",
        "        name_site = site[0]\r\n",
        "    tab = name_site.split(\"/\")\r\n",
        "    name_site = tab[0]\r\n",
        "    TLD = [\"fr.\",\"www.\",\"www2.\",\".org\",\".fr\",\".eu\",\".net\",\".com\"]\r\n",
        "    for i in TLD:\r\n",
        "        name_site = name_site.replace(i, \"\")\r\n",
        "    return(name_site)\r\n",
        "\r\n",
        "def get_base_url(url: str) -> str:\r\n",
        "        \"\"\"Documentation\r\n",
        "\r\n",
        "        Parameter:\r\n",
        "            url: complete url of an article\r\n",
        "\r\n",
        "        Out:\r\n",
        "            base_url: base url of a website\r\n",
        "\r\n",
        "        \"\"\"\r\n",
        "        for val in re.finditer(r\"(\\w)+://[^/]+/\", url):\r\n",
        "            base_url = val.group(0)\r\n",
        "        return base_url"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zveH-FTVMy6B"
      },
      "source": [
        "def prepareDF(df): \r\n",
        "    df['src_name'] = [site_name(url) for url in df.art_url]\r\n",
        "    df[\"src_url\"] = [get_base_url(url) for url in df.art_url]\r\n",
        "    return df\r\n",
        "data = prepareDF(data)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6CWJ8DBXOB68"
      },
      "source": [
        "# Step 2 : put all the names of the sites in a list ...\r\n",
        "\r\n",
        "list_sites : list = []\r\n",
        "\r\n",
        "for i in range (len(data)):\r\n",
        "  name : str = site_name(str(data['art_url'][i]))\r\n",
        "  name : str = name.upper() #puts in uppercase \r\n",
        "  name : str = name.replace(\".\", \"_\")\r\n",
        "  name : str = name.replace(\" \",\"_\")\r\n",
        "  name : str = name.replace(\"-\",\"_\")\r\n",
        "  list_sites.append(name)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uNIBmjPgOJu1"
      },
      "source": [
        "# Step 3 : ... or in a dataframe, as needed\r\n",
        "\r\n",
        "df_sites = DataFrame(list_sites,columns=['src_name'])"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q3Qc9MouOSdt"
      },
      "source": [
        "# function that retrieves the url of the site thanks to the url of the article article\r\n",
        "\r\n",
        "def lienRacine(lienArticle) :\r\n",
        "  split = lienArticle.split('/')\r\n",
        "  return (split[0]+'//'+split[2])\r\n",
        "\r\n",
        "def splitPath(lien) :\r\n",
        "  listePath = lien.split('/')[3:]\r\n",
        "  for mot in range(len(listePath)) :\r\n",
        "    if listePath[mot] =='' :\r\n",
        "      listePath.remove('')\r\n",
        "  return listePath"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8bp3OvBjOUvn"
      },
      "source": [
        "## **4/ Relevance score :**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C6UnmopIQMsQ"
      },
      "source": [
        "#### Popularité ####\r\n",
        "def countSite(df):\r\n",
        "  return df.groupby('src_name').count().reset_index()\r\n",
        "\r\n",
        "def popularite(df):\r\n",
        "    \"\"\"\r\n",
        "    Parameters\r\n",
        "    ----------\r\n",
        "    df : DataFrame\r\n",
        "        Contains : art_url, src_name, src_url.\r\n",
        "\r\n",
        "    Returns\r\n",
        "    -------\r\n",
        "    DataFrame\r\n",
        "        Contains : src_name, popularity score.\r\n",
        "\r\n",
        "    \"\"\"\r\n",
        "    df1 = countSite(df)\r\n",
        "    df1[\"popularity\"] = [x/np.max(df1.art_url) for x in df1.art_url]\r\n",
        "    df_final = df1[['src_name','popularity']]\r\n",
        "    return df_final\r\n",
        "df_resultat = popularite(data) #the modified dataframe is saved "
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "usy2m9NuR3cA"
      },
      "source": [
        "**2 -** Estimation of the relevance by taking the **words in common between those of the query, and those present in the title and in the Google summary** of the article."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2supkBB9RaXm"
      },
      "source": [
        "# Step 1 : Create the cleanup to get the root, remove punctuation and empty words\r\n",
        "\r\n",
        "def cleandesc(desc):\r\n",
        "    sent = desc\r\n",
        "    sent = \"\".join([x.lower() if x.isalpha()  else \" \" for x in sent])\r\n",
        "    Porter = SnowballStemmer('french')\r\n",
        "    sent = \" \".join([Porter.stem(x) if x.lower() not in stop_words  else \"\" for x in sent.split()])\r\n",
        "    sent = \" \".join(sent.split())\r\n",
        "    return sent\r\n",
        "start_time = time.time()"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9RWXJvZcR8z0",
        "outputId": "f506fd08-ac7c-4598-cb1e-54e9b43e6d48"
      },
      "source": [
        "# Step 2 : Apply the cleaning function is applied \r\n",
        "\r\n",
        "data['title'] = [cleandesc(x.title) for x in data.itertuples()]\r\n",
        "data['resume'] = [cleandesc(x.resume) for x in data.itertuples()]\r\n",
        "data['query'] = [cleandesc(x.query) for x in data.itertuples()]\r\n",
        "end_time = time.time()\r\n",
        "print(\"total time : {} mn\".format((end_time-start_time)/60))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total time : 0.02756366729736328 mn\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N3PFzEGdSHT9"
      },
      "source": [
        "# Step 3 : to do the function that will determine the relevance of the article according to the request \r\n",
        "\r\n",
        "data['cat_pertinence'] = ''\r\n",
        "data['common_words'] = ''\r\n",
        "data['relevance_query']=0.00\r\n",
        "relevance_query : list = [] #list that will store the relevance score,\r\n",
        "#pertinence_j = [] #list that will store the number of the line concerned\r\n",
        "def common_query_words(df) :\r\n",
        "  df_relevance = pd.DataFrame(columns=['nb_row','score']).set_index('nb_row') #creation of a df allowing to have the score for each line,\r\n",
        "  for j in range (len(df)): # for each line of the df,\r\n",
        "    innov : int = 0 #initialization of a variable to count the number of words in the innovation lexicon \r\n",
        "    gest : int = 0 #...the same for the management lexicon\r\n",
        "    separator_or : list = list(df['query'][j].split(' or ')) #we store all pairs of the request in a separator_or list \r\n",
        "    relevance_listing : list = [] #list to store the relevance scores for each of the couples\r\n",
        "    list_find : list = [] #list to store words in common for each line,\r\n",
        "    for k in range (len(separator_or)): #for each of these couples,\r\n",
        "      nb_present : int = 0 #count the number of words in common\r\n",
        "      separator_and : list = list(separator_or[k].split(' and ')) #we store all the words of the couple of the query in a separator_and list \r\n",
        "      for i in range (len(separator_and)): #for all the words in the query,\r\n",
        "        if (df['title'][j].find(separator_and[i]) != -1) : #we look for it in the title, and if it's there... \r\n",
        "          if (i == 0): #if it's the first member of the couple, we've found a word of innovation \r\n",
        "            innov += 1\r\n",
        "          else : #If not, a word of management \r\n",
        "            gest += 1\r\n",
        "          nb_present = nb_present + 1 #increments the number of words in the query found.  \r\n",
        "          list_find.append(separator_and[i]+\";\")\r\n",
        "        if (df['resume'][j].find(separator_and[i]) != -1) : #then we look for it in the summary, and if it's there... \r\n",
        "          if (i == 0):\r\n",
        "            innov += 1\r\n",
        "          else :\r\n",
        "            gest += 1\r\n",
        "          nb_present += 1\r\n",
        "          list_find.append(separator_and[i]+\";\") #... we put it in a list\r\n",
        "          list_find = list(set(list_find))\r\n",
        "      relevance_query : float = (nb_present / len(separator_and))*100 #we calculate the relevance score nb of words found / nb of total words in the query\r\n",
        "      relevance_listing.append(relevance_query)\r\n",
        "      str = ' '.join(list_find) #...\r\n",
        "    df['common_words'][j]=str[:-1] #...we add the words found in the df\r\n",
        "    df_relevance = df_relevance.append({'nb_row': j}, ignore_index=True)\r\n",
        "    df_relevance['score'][j] = max(relevance_listing) #for each line, we take the best relevance of a couple,\r\n",
        "    df['relevance_query'][j] = df_relevance['score'][j]\r\n",
        "    nb = df_relevance['score'][j]\r\n",
        "    #and we categorize according to the score obtained :\r\n",
        "    if (nb>=100) : #all the words of at least one couple are found,\r\n",
        "      df['cat_pertinence'][j] = 'I&G'\r\n",
        "    else :\r\n",
        "      if (innov >= 1 and gest >= 1) : #words of innovation and gestion are found but not in the same couple,\r\n",
        "        df['cat_pertinence'][j] = 'I&G but not from the same couple'\r\n",
        "      elif (innov >= 1 and gest == 0) : #at least one word of innovation is found, but no gestion,\r\n",
        "        df['cat_pertinence'][j] = 'I'\r\n",
        "      elif (innov == 0 and gest >= 1): #at least one word of management is found but no innovation, \r\n",
        "        df['cat_pertinence'][j] = 'G'\r\n",
        "      else :\r\n",
        "        df['cat_pertinence'][j] = 'None' #no word is found \r\n",
        "  return(df)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4_H7eUs8XuKH",
        "outputId": "fb06aa26-b4a6-4b69-c71b-21740c2e3315"
      },
      "source": [
        "def result_score_def (df):\r\n",
        "  df_relevance_query = common_query_words(df)\r\n",
        "  df_relevance_query['src_name'] = ''\r\n",
        "  for i in range (len(df_relevance_query)) :\r\n",
        "    df_relevance_query['src_name'][i] = site_name(df_relevance_query['art_url'][i])\r\n",
        "  df_result_score = df_relevance_query.groupby('src_name')['relevance_query'].mean()\r\n",
        "  return (df_result_score)\r\n",
        "\r\n",
        "df_result_score = result_score_def(data)\r\n",
        "df_result_score = df_result_score.reset_index()"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:38: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:40: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:41: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:52: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:50: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:45: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:54: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  \"\"\"\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9XjTy2yRgNBo"
      },
      "source": [
        "df_result_score['relevance_query'] = df_result_score['relevance_query']/(max(df_result_score['relevance_query']))"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iA7MP-ugT0ac"
      },
      "source": [
        "fusion = pd.merge(df_resultat, df_result_score, how=\"right\", left_on=\"src_name\", right_on=\"src_name\")"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uYLKwGXfUEsh"
      },
      "source": [
        "**3 -** Estimation of relevance by the **position of the article in the crawl**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zuCvhC1JUn4Y"
      },
      "source": [
        "def score_rank(df) :\r\n",
        "  for i in range(df.shape[0]):\r\n",
        "    df['score_rank'] = 1 - df['position']/df[df['query'] == df['query'].iloc[i]].shape[0]\r\n",
        "    df_result_rank = df.groupby('src_name')['score_rank'].mean()\r\n",
        "    return (df_result_rank)\r\n",
        "df_result_rank = score_rank(data)\r\n",
        "df_result_rank = df_result_rank.reset_index()"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EztfjtNIWUdl"
      },
      "source": [
        "# All relevant measurements are then merged into a single dataframe.\r\n",
        "fusion_result = pd.merge(fusion, df_result_rank, how=\"right\", left_on=\"src_name\", right_on=\"src_name\")\r\n",
        "fusion_result['score_mean']=0.0"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HNdO0hSxfpHd",
        "outputId": "cc0692d8-e50e-4b26-ae5d-18c827e1abd5"
      },
      "source": [
        "for i in range (len(fusion_result)):\r\n",
        "  fusion_result['score_mean'][i] = fusion_result['popularity'][i]*0.2 + fusion_result['relevance_query'][i]*0.4 + fusion_result['score_rank'][i]*0.4"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 402
        },
        "id": "pqqPz92ugD0p",
        "outputId": "40c35477-b7d8-4c14-c43f-3a12c39e6311"
      },
      "source": [
        "fusion_result"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>src_name</th>\n",
              "      <th>popularity</th>\n",
              "      <th>relevance_query</th>\n",
              "      <th>score_rank</th>\n",
              "      <th>score_mean</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>actionfirst</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.373737</td>\n",
              "      <td>0.616162</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>actufinance</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.020202</td>\n",
              "      <td>0.074747</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>aijobs.tech</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.949495</td>\n",
              "      <td>0.846465</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>alliedmarketresearch</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.066667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>amazing.dev</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.676768</td>\n",
              "      <td>0.537374</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>186</th>\n",
              "      <td>whatis.techtarget</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.030303</td>\n",
              "      <td>0.278788</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>187</th>\n",
              "      <td>wired.jp</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.434343</td>\n",
              "      <td>0.440404</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>188</th>\n",
              "      <td>wisdom.nec</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.535354</td>\n",
              "      <td>0.280808</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>189</th>\n",
              "      <td>worldbrainmapping</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.616162</td>\n",
              "      <td>0.513131</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>190</th>\n",
              "      <td>youtube</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.232323</td>\n",
              "      <td>0.159596</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>191 rows × 5 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                 src_name  popularity  relevance_query  score_rank  score_mean\n",
              "0             actionfirst    0.333333              1.0    0.373737    0.616162\n",
              "1             actufinance    0.333333              0.0    0.020202    0.074747\n",
              "2             aijobs.tech    0.333333              1.0    0.949495    0.846465\n",
              "3    alliedmarketresearch    0.333333              0.0    0.000000    0.066667\n",
              "4             amazing.dev    0.333333              0.5    0.676768    0.537374\n",
              "..                    ...         ...              ...         ...         ...\n",
              "186     whatis.techtarget    0.333333              0.5    0.030303    0.278788\n",
              "187              wired.jp    0.333333              0.5    0.434343    0.440404\n",
              "188            wisdom.nec    0.333333              0.0    0.535354    0.280808\n",
              "189     worldbrainmapping    0.333333              0.5    0.616162    0.513131\n",
              "190               youtube    0.333333              0.0    0.232323    0.159596\n",
              "\n",
              "[191 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    }
  ]
}